\chapter{Project Plan}\label{ch:project-plan}
% Methodology and approach
% - Decompose the whole project into multiple
% components
% - Explain the functions of each component and
% how you would tackle each component
% • Thesis timeline – for next two terms (term for
% those doing both Thesis A and Thesis B in one
% term)
% • Justification of time allocation for each
% component
% • Required training and upskilling identified
Solutions to the gaps in the literature will be explored by integrating MLIR in PostgreSQL on a query 
execution plan level. This would replace the volcano model entirely with an executable version. 
It supports the core customers by being a useable database, provides a toolkit that can 
make developing a compiler easier, 
and extends an existing system rather than creating a new database. The compiler will
parse the output of the relational operators module inside of PostgreSQL and compile that into
code, so the optimiser is still used. 

While LingoDB has explored MLIR, they rearchitected the entire database design rather
than fitting it into an existing one. Otherwise, MLIR is new enough that it has not been widely explored
by the database community. There are other ideas that can sprout from this, such as using the V8 Engine 
from the LLVM IR that is produced, similar to Mutable, or additional lowerings for sub-operators similar to LingoDB.

\section{Methodology and Approach}

Integrating MLIR into PostgreSQL on a query expression level is a significant undertaking.
Non-functional requirements are 1) The database should still fully work (including obscure functions like \verb|WINDOW|)
and 2) It should (ideally) strictly improve query performance without tuning when it is downloaded. 
Causing some queries to be worse while making the median or average query better is not a
worthwhile trade off in this context due to wanting a positive out-of-the-box experience. 
However, ACID and muliprocessing will not be included as they are a significant amount of effort.
The idea is to show that core idea is possible without breaking or rearchitecting the entire database.

Supporting the TPC-H-like benchmarks would make it possible to compare it to the existing
literature. Mutable only runs benchmarks on a subset of TPC-H-like queries: 1, 3, 6, 12, and 14,
while LingoDB supports all of them. They provided the minimal operators they needed to run these, and these
projects can be used to find what is necessary for a running example. 

The non-functional requirements can be supported by having a validation module. It will inspect the
query and evaluate whether the system supports it and route it to the compiled method or the regular
execution engine.

Including features such as multithreading and ensuring ACID compliance could make the project too long.


\section{Thesis Timeline}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{figures/project-plan.png}
    \caption{Thesis timeline}
    \label{fig:project-plan}
\end{figure}

\section{Justification of Time Allocation for Each Component}

A specific goal is to have deliverables by the end of the first trimester. Being able to 
execute certain types of queries by then would show progress. The minimum for this is 
Query 6 in TPC-H, which requires \verb|SELECT|, \verb|sum|, \verb|WHERE|, 
logical operators, some maths, and dates. A risk to consider is that 
PostgreSQL is vast, and it will take a while to find where things are. 
For instance, the nodes in the plan which comes from the query plan might have multiple types of nodes
(sorts) which might mean the same thing. The project plan is based around SQL, not 
the specific relational algebra syntax PostgreSQL uses, so it might not completely align 
with the sections below. All the steps below have added "fat" in their predictions
so that there is space for blockers.

Setting up the project includes forking PostgreSQL, building from source, 
installing MLIR, investigating how the query tree would get parsed by MLIR, and
the compiler boilerplate that will be required to support the queries. This also includes
things like types - floats, integers, strings, and timestamps - and benchmarking. Two weeks
might be too short for this, but it is fine if it is not completely done, and is
done when necessary. Another stage here will be partially defining the context free grammars that
are needed.

Implementing \verb|SELECT| statements should be relatively straight forward after doing the project
setup, as it is finding the entry in the database files and loading it into tuples. This also includes \verb|as|
for renaming columns. It is possible for work on the operators to start within the same week if there is time.

Operators include addition, subtraction, division, multiplication, \verb|SUM|, \verb|AVG|,
and \verb|COUNT|. There are a number of them, and particularly the projection of taking existing
columns and making them into new columns will take time. 

Operators will be fairly tightly coupled to \verb|WHERE| statements which will include logical operators
and comparisons. Once this is done, query 6 in the TPC-H benchmarks should be fully supported and be
useable as a deliverable.

\verb|ORDER BY| requires implementing a sorting algorithm, and might be difficult due to the 
importance of managing buffers. Similarly, \verb|GROUP BY| needs to manage buffers and projecting the
selection can take significant time. Once these two are implemented, several TPC-H queries will be runnable.
It would be ideal if the first trimester can reach here, but that would require a low number of blockers.

\verb|JOINS| will be a large amount of work, particularly in that
some TPC-H queries have specific join statements like \verb|LEFT OUTER JOIN|. 
Once implicit \verb|JOIN|s are implemented a significant number of TPC-H queries will 
be runnable, and with nested queries nearly all of them will be. Nested queries 
might be simple in that the language specification could make it
easy to implement these, however, they will lead to the database needing 
to store intermediate tables. 

The validation module is also conceptually simple because it once it inspects whether the query is
runnable and can get a speed improvement, it will feed it to the execution engine. Another option 
which could be explored here is putting in nodes that measure the volcano executor,
and part way into execution interrupts it, measures whether it's worth compiling, and decides there.
This can be a complicated task. It is also not critical to the thesis for this to be included, and 
in the worst case scenario could be cut out of the plan.

At the point of starting the validation module, time should start being spent writing the thesis. 
This is an early start, so that there can be space for thinking about structuring the writing and 
receiving feedback. In the last two weeks, time will be spent only on writing the thesis.

\pagebreak

\section{Required Training}

The core tool being used is MLIR, which will need training to use. However, LingoDB has managed 
to be compatible with all 22 TPC-H queries with approximately 12,000 lines of code to parse and execute code. This 
excludes their lowering stage, and also provides examples of how to do this with MLIR.

Exploring PostgreSQL's codebase and understanding the operators the query plan has will
likely take a significant portion of time. LingoDB parsed the SQL expression directly, whereas
here we need to parse a query plan.

Another requirement is implementing benchmarking this against other reviewed databases. This will 
be explored directly in Chapter~\ref{ch:prelim}.

Other requirements are generic: programming familiarity, version control, databasing, compiler
architecture, experience with long term projects, documentation and testing.

