\chapter{Literature Review}\label{ch:lit-review}

\section{Summary}\label{sec:system-r}

In the following section a series of relevant databases and their architecture will
be explored. This consists of System R in Section~\ref{sec:system-r} and JAMDB in Section~\ref{sec:jamdb}
as early research papers for compiled queries. Following that, HyPer will be explored in
Section~\ref{sec:hyper} and Umbra in Section~\ref{sec:umbra} because they are functioning enterprise-level 
systems that are widely considered to have pioneered JIT, and made by mostly the same team. 
After that PostgreSQL is explored in Section~\ref{sec:postgres}, as an open-source example with real world
impact. Mutable in Section~\ref{sec:mutable}, and LingoDB in Section~\ref{sec:lingodb} are recent, smaller, research
databases in the field. After that is a summary of the existing gaps in the literature in Section~\ref{sec:gaps}
as well as issues in the field as a whole.

 Originally, we started with compiled engines which were overtaken by popular Volcano 
 models, and HyPer reintroduced the idea of compiled engines. Later on, PostgreSQL took note 
 of the impact of HyPer, while Mutable and LingoDB criticized HyPer's approach with alternatives.


\section{System R}\label{sec:system-r}

System R was the first implementation of SQL, and was made in 1974 with a query compiling engine 
\cite{system-r}. Their vision was supporting ACID requirements, which was explained in seven dots points as ACID
was not a concept yet. It strived to run at a ``level of performance comparable to existing 
lower-function database systems". Reviewers go as far as calling their choice to 
use a compiler ``the most important decision in the design" \cite{system-r}.

Their choice to use a compiler was because the overhead of parsing, validity checking, and 
access path selection were not inside the running transaction. The compiler had pre-compiled
fragments of Cobol for reused functions to improve compile times, and the compiler was mostly
custom-made because there were not many tools at the time to support writing compilers \cite{system-r}. 
It shows that the idea of compiled queries is not new, but over time databases that chose 
less performant architectures such as the volcano model survived the market to now. This is because it 
made development easier and more correct \cite{database-concepts}.

\section{JAMDB}\label{sec:jamdb}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figures/jamdb-architecture.png}
    \caption{JAMDB Architecture.}
    \label{fig:jamdb_architecture}
    \cite{jamdb}
\end{figure}

In 2006, IBM's Research Center published an article about compiled query execution using the
JVM for an in-memory database. As shown in Figure~\ref{fig:jamdb_architecture}, they created
two pathways for code to compare runtimes. One fed into a volcano interpreted engine, and the
other used dynamic code generation to create a Java class that would run through the JVM. 


% TODO: this is a short paragraph that I could expand on. In fact, this is my opinion. 
% I don't remember if they say it in their research.
In Figure~\ref{fig:jamdb_interp_vs_compil} they also have an example of a 
produced interpreted plan (a) compared to a compiled plan (b) generated from Q1 of the TPC-H 
benchmark. The difference in complexity between the two shows how impactful their code generation
is because the inlining, casting, and number of method invocations is improved \cite{jamdb}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/jamdb-inter-vs-comp.png}
    \caption{JAMDB produced interpreted plan compared to compiled plan.}
    \label{fig:jamdb_interp_vs_compil}
    \cite{jamdb}
\end{figure}

Depending on the query, their results show JIT is 2-4x faster than interpreting, and that on average
their interpreted plans have twice as many memory accesses on primary, L3 and L2 memory,
and over ten times as many branch prediction failures \cite{jamdb}. The ending note is that while
these results are promising, this is a research database which is missing persistence, locking,
logging, recovery and space management. However, the fundamental issue is that developing a
compiled executor is significantly harder to maintain, and they recommend the future work
should be about simplifying making this type of database in the future \cite{jamdb}.

The issue with their comparison is that it is difficult to implement the compiler and interpreter to 
an equal quality. There are small things that can be improved or omitted; the Java
compiler is a production-ready tool whereas the interpreted engine is handwritten by 
their team. 

\section{HyPer}\label{sec:hyper}

HyPer is considered to be the pioneer in the field of JIT in databases because it is 
commercialised. It has enough features implemented for it to be used on a commercial scale,
and showed that it is possible to use JIT on that scale. The project started in 2010, with their flagship
paper releasing in 2011 for the compiler \cite{hyper-compiler-1}, in 2016 they were acquired 
by Tableau \cite{tableau-hyper}, and in 2018 they released another flagship paper 
about adaptive compilation \cite{hyper-compiler-2}. The database being commercialised
does pose issues for outside research as the source code is not accessible, but they
released a binary on their website that can be used for benchmarking.

In their 2011 paper on the compiler, the HyPer team identified that translating queries
into C or C++ introduced a significant overhead to their compilation latency compared to 
translating to LLVM. The paper proposed using pre-compiled C++ objects of common functions which are
tied together using generated LLVM IR. This LLVM IR code would be executed using the LLVM's 
JIT executor. Using LLVM IR allows them to produce better code than C++ because they could
configure specific features like the overflow flags, and it is also strongly typed
which they mention prevented many bugs that was inside their original C++ \cite{hyper-compiler-1}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/hyper-compile-times.png}
    \caption{HyPer OLAP performance compared to other engines.}
    \label{fig:hyper-compile-times}
    \cite{hyper-compiler-1}
\end{figure}

They managed to reduce their compile times by several factors as shown
in Figure~\ref{fig:hyper-compile-times}. Furthermore, in Figure~\ref{fig:hyper-profiler}
they show that they achieve many times less branches, branch mispredicts, and other 
measurements compared to MonetDB. This is due to HyPer's output having less code in its
compiled queries, which results in less hazards being encountered during runtime 
\cite{hyper-compiler-1}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/hyper-profiler.png}
    \caption{HyPer branching and cache locality benchmarks.}
    \label{fig:hyper-profiler}
    \cite{hyper-compiler-1}
\end{figure}

In a later paper in 2018, HyPer improved their compile time by splitting their compiler into multiple stages.
They used an interpreter on byte code generated from LLVM IR, on the next stage they can
run unoptimised machine code, and on the final stage they can run optimised machine code. In 
Figure~\ref{fig:hyper-adaptive} this is visualised alongside compile times of each stage.
Note also that they made the byte code interpreter themselves to be able to do this \cite{hyper-compiler-2}. 

\begin{figure}[H]
    \centering
    \includegraphics[scale=1]{figures/hyper-adaptive.png}
    \caption{HyPer execution modes and compile times.}
    \label{fig:hyper-adaptive}
    \cite{hyper-compiler-2}
\end{figure}

Another adjustment in this paper is that they improved whether the engine optimised their queries. They
argue that the optimiser's cost models as well as cardinality estimates are often inaccurate 
\cite{optimizer-bad-1} \cite{optimizer-bad-2}. Instead, they opt to split their execution into jobs
for workers, and during the work-stealing stage they log how long the job took. With the query plan 
and these figures, they calculate accurate estimates for jobs and the optimal level to compile them to.

They benchmark TPC-H Query 11 using 4 threads, and it shows that their adaptive execution beats
the alternatives of only using byte code by 40\%, unoptimised compilation by 10\% and optimised
compilation by 80\%. This is because their compilation stage is single threaded, so in the context 
of four threads the adaptive model can compile quickly on byte code, then while the byte code is running
start compiling a better version on a single thread while the others continue to work \cite{hyper-compiler-2}.

This combination of adding additional stages to their LLVM compiler, supporting multithreading between
compiling and execution, and a more accurate cost analysis transforms their approach into a viable JIT
application. A criticism with HyPer is that they are building a new JIT compiler from the 
ground-up which required significant engineering effort. In fact, most of the additions 
here are not unique to a database's perspective of its compiler, but are mostly ways 
to improve the compiler's latency. If they chose a compiler that focused on compilation latency, 
this might not have been an issue. However, this typically goes against the philosophy of JIT, and 
will be explored further in Mutable in Section~\ref{sec:mutable}.

\section{Umbra}\label{sec:umbra}

Umbra was created in 2020 by the same person that created HyPer, Thomas Neumann, and in their initial
paper they take modern ideas from in-memory database and apply them to an on-disk database. 
They reason that the recent improvements in SSDs has made it possible to create an on-disk database
which can achieve the same speeds as an in-memory database. It takes ideas from
LeanStore for buffer management and latches, multi-version concurrency control from HyPer,
and the compilation and execution strategies from HyPer as well. All of this produced a database
which is more flexible, scalable, faster than HyPer \cite{umbra} as seen in 
Figure~\ref{fig:umbra-benchmark}. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/umbra-benchmark.png}
    \caption{Umbra benchmarks.}
    \label{fig:umbra-benchmark}
    \cite{umbra}
\end{figure}

Following this, they used Umbra as a base for several other research ideas. A key one
is that they introduce a multi-level debugger \cite{umbra-debugger}. It is not viable 
to attach a debugger to the database in the same way that volcano models do, because the 
engine compiles it into a whole new program. Attaching it to the SQL with markers
added to the IR would also obfuscate how the operators work. To remedy this, they 
developed a time-travelling feature that replays a recording of the code generation 
to let them see the context \cite{umbra-debugger}.

In another paper, the approach of making their compiler have its own JIT stages has benefits.
The ability to change query optimisation choices in the middle of their execution time is added - such
as swapping the order of two joins.
This can be particularly useful when their cost model makes an inaccurate choice, and they find that 
it improved the runtime of data-centric queries by more than two times \cite{umbra-effec-compil}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figures/umbra-effec-queries.png}
    \caption{Umbra benchmarks after adaptive query processing (AQP).}
    \label{fig:umbra-effec-queries}
    \cite{umbra-effec-compil}
\end{figure}

In other databases, this feature is added by invoking the query optimiser multiple times;
either during runtime, or at the start with the ability to swap between the plans. Other systems 
use routing where the plan itself has swapping between optimisations built into the plan 
itself. Since Umbra is compiled, and compilation has a high overhead cost, they opted for the second design.
This also synergises well with using a JIT compiler. If the query changes an optimisation choice
early in, it only needs to compile the correct choice during runtime \cite{umbra-effec-compil}. 
In Figure~\ref{fig:umbra-effec-queries} they show a consistent improvement compared to before this change.


Overall, Umbra is rated as the most effective database in Clickhouse's benchmarks \cite{clickbench}, and
is currently the most exhaustive implementation of JIT inside a database. It does still have
the issue of engineering around a seemingly poor choice for a compiler, however, they also
take advantage of having access to compilation choices made by the compiler.

\section{PostreSQL}\label{sec:postgres}

Firstly, when it comes to PostgreSQL majority of their design decisions are not made in the context of
researchers writing an analysis that states something is worth doing with heavy empirical evidence. 
This is because it is open source software, and a number of features take the philosophy of: if someone
is willing to do it, and it's either optional or clearly positive, it may as well be added. As a result,
many of the sources about PostgreSQL are not going to be from peer-reviewed research papers. Most of the
sources here will be from their documentation, pull-requests, and online articles.

With that being said, PostgreSQL is impossible to ignore because is the most popular database. In 2024, 51.9\% of 
professional developers in the Stack Overflow survey said that they have done extensive work with it in the last year. 
A large fear in the database research community is that researchers are losing connection with their customers,
so it is important to investigate \cite{top-ten-fears}. 

Before PostgreSQL added JIT, there was significant discussion about HyPer and JIT compiled queries in 2017
\cite{psql-jit-discussion}. People mostly responded with doubt that someone is going to add support for full
query expression compilation or changing their model to a push-based one. Furthermore, rearchitecting a core 
component introduces significant risk.


JIT was added by Andres Freund to PostgreSQL, and he reasoned that they can benefit from JIT for CPU-heavy queries
by only compiling expressions (the \verb|x > 5| in \verb|SELECT * FROM tbl WHERE x > 5;|), and that tuple deformation
benefits significantly as well. He claims that generally when a query is CPU-heavy, it's because of expressions. Also, 
that tuple deformations interact with the cache, and have poor branch predictions in his benchmarks. 
A notable response in the pull 
request is in Figure~\ref{fig:pg-params-too-low} where Peter Eisentruat asks if the defaults are too low. 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/pg-min-params.png}
    \caption{Peter Eisentruat asking whether the defaults are too low.}
    \label{fig:pg-params-too-low}
    \cite{postgres-pr-2}
\end{figure}

This was released in PostgreSQL version 11, and then enabled by default in version 12, and has been enabled 
ever since (latest as of writing is 17) \cite{psql-jit-doc}. There was discussion about the release inside
the pgsql-hackers email thread where they justified that enabling it by default 
will give the feature far more exposure and testing \cite{psql-jit-release-plan}. However, when it was released 
the public reception was to disable it by default, with the United Kingdom's critical service for a COVID-19
dashboard collapsing with a 70\% failure rate \cite{psql-jit-failure}. This was due to their team having
automatic updates on their database, and the default settings on JIT caused their queries to run 2,229x slower 
in the dashboard \cite{psql-jit-failure}.

This shows that on open source projects the out of box experience for these features must keep the end user
in mind, and not only be based on benchmarks improving a theoretical scenario. Due to this, it is likely that a
substantial portion of the PostgreSQL community has started disliking the idea of JIT, and changing public opinions
can be difficult.

% TODO I should also talk about the slides on postgresql which has a 5x improvement on execution times or something


\section{Mutable}\label{sec:mutable}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/mutable-arch.png}
    \caption{Comparison of mutable to HyPer and Umbra.}
    \label{fig:mutable-arch}
    \cite{mutable-2}
\end{figure}
Mutable is a recent database which had its first publication in 2023, and its primary focus was being a minimal
framework for research to build off of \cite{mutable-1}. It was created by the research team specifically with 
a JIT paper in mind. They reflected on HyPer and Umbra and came to the conclusion that most of their work
was in the compiler, and what they need is to pick a compiler that prioritises latency instead \cite{mutable-1}. 


The architecture of mutable is compared to Umbra and HyPer in Figure~\ref{fig:mutable-arch}. A side 
effect to note is mutable does not have a deeply developed query optimiser, and they rely 
on the V8 engine instead. Their pipeline consists of turning the SQL query into WebAssembly and 
feeding that into the V8 engine \cite{mutable-2}. 

In the V8 engine's official blog they explain that the "Liftoff" component is added to improve the compiler latency \cite{wasm-liftoff}.
Most JIT systems are created with the idea that only pieces of code that are executed with a high number of repetitions 
are worth compiling, but in the context of browsers that is the case \cite{wasm-liftoff}. Inside the Liftoff
stage the compiler focuses on producing machine code as fast as possible without optimisations and then runs ``TurboFan", the
second stage compiler which runs in the background while execution is going. It focuses on optimisations, 
scheduling, instruction selection, and register allocation \cite{wasm-liftoff}. This is slightly
different to HyPer, as HyPer has byte code interpretation as an option as well \cite{hyper-compiler-2}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/mutable-benchmarks.png}
    \caption{Benchmarks produced by Mutable.}
    \label{fig:mutable-bench}
    \cite{mutable-2}
\end{figure}

The mutable team produced high quality benchmarks visible in Figure~\ref{fig:mutable-bench}, and in their results they found that 
they get similar execution and compile speeds to HyPer, and in many cases they beat them. Primarily the times HyPer has a better overall time 
is when it has multithreading enabled.

The downside of this approach is if they want to optimise mutable further in the same ways HyPer or Umbra did with 
query re-planning \cite{umbra-effec-compil}, they would likely need rearchitecting. However, their performance being 
comparable to HyPer with this much less work is significant.

\section{LingoDB}\label{sec:lingodb}
% TODO mention their work is similar to https://vldb.org/pvldb/vol15/p1119-sichert.pdf
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/lingo-db-arch.png}
    \caption{LingoDB architecture.}
    \label{fig:lingo-db-arch}
    \cite{lingo-db-1}
\end{figure}


LingoDB is a recent database with a novel approach of shifting the database query optimiser
into the compiler itself by leveraging MLIR \cite{lingo-db-1}. This minimises how much translation they need 
to do between layers. Traditionally, developers need to parse the SQL into relational algebra
operators, optimise on those, and parse the operators back into compiled code or an 
execution plan by hand. Since these shifts can be supported directly by
the compiler itself, it means ensuring these translations are performant is less of a concern and
that there is less manual implementation \cite{lingo-db-1}.

An overview of the LingoDB architecture can be seen in Figure~\ref{fig:lingo-db-arch}. 
They parse SQL into a relational algebra dialect inside MLIR, and they perform five optimisation
techniques on that. This means it is not a mature optimisation engine, but shows that they can apply 
standard optimisation techniques within the compiler. Following this, they lower the declarative
relational algebra that was produced into a combination of imperative dialects, which they 
can then lower to database-specific types and operations. 

The outcome is that they are less performant than HyPer, but do manage to be better than DuckDB (an 
on-disk dataframe-based database which is widely used \cite{lingo-db-1}) as seen in 
Figure~\ref{fig:lingo-db-bench}. However, their performance is not the impactful part of this research,
the meaningful part is that LingoDB has approximately 10,000 lines of code in its query execution module \cite{lingo-db-1}. In
comparison, Mutable is now at 22,944 lines of code (on doing a \verb|git clone| of their public 
GitHub repository \cite{mutable-1}). In the LingoDB paper, they compare this to being three times
less than DuckDB and five times less than NoisePage \cite{lingo-db-1}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figures/lingo-db-bench.png}
    \caption{LingoDB benchmarking.}
    \label{fig:lingo-db-bench}
    \cite{lingo-db-1}
\end{figure}


Their main suggestion for future work is for supporting parallelising workloads and adding a dialect for
distributed systems. In a later paper, they add lowerings for sub-operators for non-relational systems.
Sub-operators consist of logical functions such as hashing, filtering, or user-added functions \cite{lingo-db-2}. 
Umbra also added a similar feature to that before them \cite{umbra-user-operators}. The idea of LingoDB
is novel and appealing, but as it requires rearchitecting the entire database pipeline to be inside
the compiler, the idea is mostly not applicable to existing databases.



\section{Gaps in Literature}\label{sec:gaps}

The successful databases that were explored were HyPer and Umbra which managed to commercialise, and 
the other ones are research databases that in many cases did not support ACID, multithreading, or core
requirements such as generating indexes on tables. The open source database, PostgreSQL, that did 
integrate JIT was mostly disastrous and caused an outage for a critical service 
in the UK. However, the potential for JIT is large as it can improve PostgreSQL's performance 
by multiple times.

Michael Stonebraker, a Turing Award recipient and founder of Postgres writes that a fundamental issue
is that research has been forgotten by customers except for representatives
of the 0.01\% largest of database users \cite{top-ten-fears}. Commercial databases achieving performance
many times faster than open source applications is a sign of this. In many real-world applications,
the solution to your database being slow is not to tune your database, but to apply
an application level cache either on a single server or distributed across a cluster with significant manual 
effort \cite{ddia}. This is largely because of the number of obscure ways that ACID requirements can be violated,
and picking a slower, battle-hardened database is preferable \cite{ddia}. However, due to the popularity of 
PostgreSQL and other open source databases, improving them would still have a large impact.

Another issue is that writing a compiled query engine is difficult. This is one of the primary reasons 
developers opt for vectorised executions. There are not many debugging tools compared to stepping through
a volcano or vectorised model. Umbra has suggested and created one specifically for their database,
but it is questionable how transferable this is to other systems.

Comparing different compilers has been reasonably explored, but there are not many systems that explore MLIR.
LingoDB recommends using MLIR to rearchitect the entire database to function around the toolkit, however, 
in the majority of applications recreating a database (such as Postgres) is not a viable approach without 
substantial investment.

% Requirements for other parts of the thesis work can be found on the school
% web-pages~\cite{Noo05}.  The requirements below are for the written thesis
% only.

% The following format specifications must be adhered to for your thesis
% (the \LaTeX\ template available from the school ensures this):
% \begin{enumerate}
% \item The thesis must be written on \emph{A4 size paper}.
% \item The thesis must be typed or prepared using a \emph{word-processor}.
% \begin{itemize}
% \item For Undergraduate theses, you are encouraged to use both sides
%   of the paper.
% \item For Higher Degree Research theses, your submitted thesis must be
%    printed single-sided.
% \end{itemize}
% \item \emph{Margins} on all sides must be no less than \unit[20]{mm} (before
% binding).
% \item \emph{1.5 line spacing} (about \unit[8]{mm} per line) must be used.
% \item All sheets must be \emph{numbered}. The main body of the thesis must be
% numbered consecutively from beginning to end.  Other sections must either
% be included or have their own logical numbering system.
% \item The \emph{title page} must contain the following information:
% \begin{enumerate}
% \item University and School names.
% \item Title of Thesis/Project.
% \item Name of Author and student ID.
% \item The degree the thesis is submitted for.
% \item Submission date (month and year).
% \item Supervisor's name (for undergraduate theses).
% \end{enumerate}
% \item After the body of the thesis, the thesis \emph{must} contain a
%   Bibliography or {Ref}erences list as appropriate.

% Authors should confer with their supervisors and School about the
% style of their bibliography, as this varies between disciplines.
% \end{enumerate}

% \section{Other physical appearance}
% Other requirements to the physical appearance of your theses are:
% \begin{enumerate}
% \item \emph{Graphs, diagrams and photographs} should be inserted as close as
% possible to their \emph{first reference} in the text. Rotated
% graphs etc are to be arranged so as to be conveniently read, with the
% bottom edge to the outside of the page.
% \emph{Graphs and diagrams must be legible!}
% \item \emph{Supplementary material (for example CFD animations)} may be submitted either online or via external drive, and must be referred to within the text. The text should make sense without the supplementary material available. 
% \end{enumerate}

% \section{Submission}

% Finally, here are some requirements to the submission procedure. 

% \begin{enumerate}
% \item The \emph{author} of the thesis is \emph{responsible} for the preparation of the
% thesis before the deadline, proofreading the
% typescript and having corrections made as necessary.
% \item For undergraduate theses, there is a \emph{page limit} of 50 pages for the main body of the thesis.
% \end{enumerate}



% \chapter{Content Requirements}

% Students should consult the literature (e.g.~\cite{Sid99,StrWhi79,Coo64,GRS14})
% and other resources for material on how to write a good
% thesis.  The present document is only a very brief introduction as to what
% is expected.

% \nocite{NieLeh03,HasLehKwo05}

% \section{Structure}
% Most theses are structured very much like the present document.
% The main part of the thesis can be structured in many different ways,
% however, but must contain: a \emph{problem definition};
% \emph{theory} and \emph{considerations} on how to solve the problem;
% a description of the \emph{solution method} (dimensioning, construction,
% etc.);
% presentation of \emph{results} (measurements, simulations, etc.);
% a \emph{discussion} of the results (validity, deviations, comparison
% with previous solutions, etc.); and finally the \emph{conclusions}.

% \section{Style of writing}

% \begin{enumerate}

% \item Audience:
% The thesis must be addressed to engineers at the same level as the
% student but without the special knowledge gained during the thesis work.
% Such a third-person must be able to reconstruct the results on the basis
% of the thesis alone.

% \item
% Every used concept/symbol/abbreviation which is not widely know must be \emph{defined}.
% The wording should be \emph{short} and \emph{concise}.  
% Readable(!) \emph{figures} and \emph{graphs} enhances comprehensibility.

% \item Units.
% \emph{SI units} must be used.
% \end{enumerate}

% \section{Documentation}

% \begin{enumerate}
% \item
% The work must be well documented; i.e. enclosed must be the \emph{complete
% schematics} of designed electronic circuits/test set-ups and/or a
% \emph{program listing}, and/or etc.
% Documentation of \emph{simulation results} and/or \emph{measurement
% results} likewise.
% \item References:
% For every declaration/equation/method/etc., which is not widely known,
% a \emph{reference to the literature} must be given (or a `proof' if it is
% the authors own work).
% In case material is copied verbatim, quotes must be used.
% This is also the case when referring to partners
% work in the case of a Group Thesis.

% \item Plagiarism:
% Failure to give proper references to the literature is \emph{plagiarism}.
% Plagiarism is considered serious offence and severe penalties may apply.

% \end{enumerate}

