\chapter{Background}\label{ch:background}
\section{Database Architecture}

Most databases have an internal architecture similar to Figure~\ref{fig:database_structure}.
Structured Query Language (SQL) is parsed, then converted into a relational 
algebraic expression, which is used in the optimiser, and this is converted 
into an execution plan for the evaluation engine.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/database_structure.png}
    \caption{Database Structure}
    \label{fig:database_structure}
    \cite{database-concepts}
\end{figure}


For example, \verb|select salary from instructor where salary < 75000;| can be translated 
into $\sigma salary < 75000 (\Pi salary (instructor))$ inside the
relational-algebra expression stage (understanding the meaning of these operators 
is not needed at this stage). In most cases, there can be alternative ways to write
the algebraic expression, and there can also be multiple algorithms that can be used for
each operator. The optimiser would pick what it thinks is the best algorithm to use,
and the execution plan will generate a tree, or some representation of the query that 
can be executed. \cite{database-concepts}

Most query engines use a pull-based Volcano execution model.
This calls a \verb|next()| function on the root node, which requests one tuple from its
children. For instance, in Figure~\ref{fig:execution_tree}, the \verb|Project| 
operator would call hash join, which would then call its children until scan operators read 
from the disk, which passes the information back up \cite{everything-vector}.
\verb|Select| might filter this tuple and request the next, or a node
can change the tuple before returning it, such as \verb|MergeJoin| \cite{long-masters-thesis}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/execution-tree.png}
    \caption{Volcano operator model tree.}
    \label{fig:execution_tree}
    \cite{long-masters-thesis}
\end{figure}

This model can be problematic in modern hardware because unless a single tuple fills the L1 cache,
the CPU's caching will be underutilised. With modern hardware advancements making these caches larger, this
has also become a more common and impactful issue. As a result, modern databases use either vectorised execution
or compiled execution \cite{everything-vector}. Vectorised execution requests multiple tuples,
but this has downsides of more copying, and potentially overflowing the cache on operators such as merge join
\cite{everything-vector}. Compiled queries, on the other hand, will use caches almost fully but are
more challenging to develop.

Relational databases typically prioritise ACID - Atomicity, Consistency, Isolation, and Durability 
\cite{database-concepts}. Atomicity refers to transactions being treated as single units of work, 
consistency is that the data must be in a valid state before and after the query. Isolation means 
concurrent transactions do not interact with each other, and durability is that once something is 
committed it will stay committed even in system failure \cite{database-concepts}.

\section{Just-in-time (JIT) compilers}


Just-in-time (JIT) compilers work by compiling code to machine code or byte code after it has been run 
multiple times. They are mostly used by interpreted languages as an alternative to eliminate ill-effects on
performance \cite{long-masters-thesis}. Advanced compilers can run the compilation 
a different thread while still running the program then swap in  the compiled artifacts when it's done. 
They also have multiple layers, such as interpreting text, then byte code, then machine code, then 
optimised machine code \cite{hyper-compiler-2}. The main issue this introduces is that before the
code is compiled, performance is poor. Depending on the language, there can be support for caching
JIT compiled code, but in the context of database systems the produced machine code can require pointers
to addresses, so it is not always viable.


On-disk databases consider CPU-based operations to be O(1) time complexity because of how expensive
loading in pages from the disk is \cite{database-concepts}. As a result, JIT compilers
are considered in the context of in-memory databases because their transactions are not bottlenecked
by the speed of secondary memory. However, due to advances in secondary-storage
some papers question whether CPU-time is still constant, and compiling queries would optimise 
cache usage which would lower how many page faults occur \cite{umbra}.

In the context of databases, most applications of JIT can be split into compiling only specific sections
of queries (typically called EXP for expression), and others that compile the entire Query Execution Plan 
(QEP) \cite{empirical-analysis}. The results of this will be explored in Chapter~\ref{ch:lit-review}.

\section{Relevant Operating System features}\label{sec:os-feats}

In the context of a CPU with multiple cores, a CPU can transform a sequential execution
into a pipelined execution. Essentially, CPU instructions are split into Fetch (IF), Instruction
Decode (ID), Execute (EX) and Write-back (WB). The CPU can do multiple of these within a single cycle,
however, they need to be independent \cite{long-masters-thesis}.

CPUs can avoid waiting for the calculation of a branch by using branch prediction, however, 
if a branch is wrong it can cost cycles to fix; for some processors it is at 
least 17 cycles \cite{long-masters-thesis}. When a branch is incorrect, it could affect more 
than the thread that it's on, and it also has to reschedule when it is incorrect \cite{long-masters-thesis}. 
Improving this is impactful - benchmarks show that Java outperforms C++ even on the maximum optimisation due to the advantage
of collecting runtime statistics \cite{jamdb} \cite{java-vs-cpp}.

In 1999, a benchmarking paper measured four commercial databases and found 10-20\% of their execution time
was spent fixing bad branch predictions \cite{branch-misprediction}. These databases were kept anonymous in the paper 
because of
legal reasons, and this was also a long time ago, but it shows the importance of reducing branch prediction.
Similarly, research specifically into branch prediction has said "although branch prediction
attained 99\% accuracy in predicting static branches, ... branches are still a major bottleneck in
the system performance" \cite{branch-prediction-values}. Modern measurements still find 50\% of their query times
are spent resolving hardware hazards, such as mispredictions, with improvements in this area making their queries
2.6x faster \cite{dbms-branches}.

\section{MLIR and LLVM}

The LLVM Project supports building compilers, and a large portion of modern compilers use its
toolkit. It assists with optimising code, linking objects, and many other requirements different compilers
share to prevent rewriting code \cite{llvm}.

Multi-Level Intermediate Representation (MLIR) is another, newer toolkit that is tightly integrated
with the LLVM project \cite{mlir}. It's a higher-level tool that allows developers to express domain-specific dialects
that can operate on different abstraction levels. For instance, low-level languages like LLVM intermediate 
representation and data flow graphs can be represented within the same dialect \cite{mlir}.



