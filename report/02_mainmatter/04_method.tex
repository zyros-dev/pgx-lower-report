\chapter{Method}\label{ch:project}
In section~\ref{sec:design} the overarching design is described,
then section~\ref{sec:implementation} goes over the implementation.

\section{Design}\label{sec:design}
The first decision is which database and which compiler this project should use.
Something with strong extension support, wide-spread usage, high performance,
and a volcano execution model is needed as a base. For the compiler,
it would be ideal if they already use a similar interface to the target database
when they parse SQL, and have promising results in their performance. This
removes HyPer, Umbra, and System R, and leaves Mutable and LingoDB. LingoDB
parses its inputs with \texttt{pg\_query}, so it matches with PostgreSQL.

As a result, PostgreSQL and LingoDB were chosen. PostgreSQL offers strong support
for extensions, and it is possible to override its execution engine using runtime
hooks. An example of this Tiger data, which was explored in
section~\cite{todo}. The primary challenge with this is that LingoDB is a columnar,
in-memory database, so adjustments will be needed. Furthermore, LingoDB does
not support indexes, which can make the benchmarks against PostgreSQL unfair.
Another detail is that LingoDB's newer versions contain a large number of
features and optimisations that is not relevant to us, so to simplify implementation
effort the 2022 version was used from their initial paper.
% TODO: Write about tiger data, or use a different example.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{figures/system_design.drawio.pdf}
	\caption{System design with labels of component sources.}
	\label{fig:overall-system-design}
\end{figure}

LingoDB was integrated into PostgreSQL as seen in figure~\ref{fig:overall-system-design}.
The blue represents PostgreSQL components, with the pipeline on the left being
the whole of PostgreSQL. A query reaches the runtime hooks, which gets analyzed
by a hand-written analyzer for whether the query can be execution, and then parsed.
These hand writte components are annotated in light-peach.
This goes through the code LingoDB created, but with custom runtime hooks and
other small edits, annotated with green. Finally this is compiled into
LLVM IR, which has the runtime hooks for reading from PostgreSQL embedded inside.

In the case that a query fails, the system should still support returning the
results and gracefully roll over to PostgreSQL. This was done by ensuring the
AST parser entrance has a try-catch pattern that routes back to PostgreSQL
even in failures. However, this does not protect from system panics such as
segmentation faults.

The most time-consuming part of this is expected to be the AST Parser section,
because it will be receiving the plan tree with the optimisations from PostgreSQL.
LingoDB was designed to parse the query tree, which would come from the
"Parser" stage in figure~\ref{fig:overall-system-design}. 18 plan nodes and 14
expression nodes were implemented.

The final goal here is to support the TPC-H query set.
To drive this implementation, a test-driven approach was used where PostgreSQL's
\texttt{pg\_regress} module added support for creating SQL queries and defining
expected outputs. With this, a test set of basic queries was created which built
up to TPC-H queries. This allowed progressive node implementation during
development, and a quick way to validate changes are safe.

Node implementation ordering followed the dependency analysis. Foundational
nodes such as the sequential scan and projection are in virtually every query,
while other nodes build on top. By implementing in the dependency order, each new
node could be tested using the previously implemented nodes, and bugs can be
isolated.

\section{Implementation}\label{sec:implementation}

The primary system this project was developed on was a x86\_64 CPU (Ryzen 3600)
and on Ubuntu 25.04. The database was not tested on MacOS or Windows, and this
may lead to issues when installing it independently.

%===============================================================================
\subsection{Integrating LingoDB to PostgreSQL}\label{subsec:integrating-lingodb}
The project was started from \url{https://github.com/mkindahl/pg_extension},
then \texttt{ExecutorRun\_hook} inside of executor.h in PostgreSQL
was used \url{https://doxygen.postgresql.org/executor_8h_source.html} as the
entrance. Within PostgreSQL there are some surrounding steps since the intention
is not usually to replace the entire executor with these hooks, so the memory
context had to be activated and switched into.

Next, the \texttt{QueryDesc} pointer, which contains the query request,
was to be passed through to \texttt{C++}. This causes a design decision. Good practice
here is to use smart pointers to prevent memory leaks, but this object is large
and the source of truth about the request. Furthermore, the memory is handled by
the PostgreSQL memory contexts. It was decided that these objects will remain
as raw pointers, causing the \texttt{C++} to break conventions.

LingoDB was installed as a git submodule and set to a read-only permission.
This was maintained for reference purposes only, and the compilation phases
would be extracted. LingoDB used LLVM 14, and was upgraded to LLVM 20 to
modernise it and slightly better support with |C++20| (some workarounds were
required with LLVM 14 that could be skipped with LLVM 20). However,
since this is the C++ API for LLVM, a large amount of the LingoDB code
had to be adjusted to compile.

%===============================================================================
\subsection{Logging infrastructure}\label{subsec:logging}
PostgreSQL has its own logging infrastructure that routes through its |elog|
command, but it was decided that a two-layer logging infrastructure was required.
The first layer is the level, (|DEBUG|, |IR|, |TRACE|, |WARNING\_LEVEL|,
|ERROR\_LEVEL|, and more), and the second represents which layer of the design
the log is inside of (|AST\_TRANSLATE|, |RELALG\_LOWER|, |DB\_LOWER|, and more).
This meant if the AST translation was being worked on, all the logs in only that
section of the codebase could be enabled. The core benefit of this is that
the logs are lengthy so it becomes easier to navigate.

An issue that was encountered was that the LLVM/MLIR logs would route through
stderr, and this caused difficult to debug issues until the hook was found
to redirect this into |elog| as well. Subsection\~ref{subsec:debugging} will
explore one of the workarounds that was needed at this stage.

Lastly, for error handling mostly |std::runtime\_error| was utilised. This served
as a global way to log the stack trace and roll back to PostgreSQL's execution.
There initially was an implementation of error handling with severity levels
and messages, but the simplicity of a single command that rolled back to
PostgreSQL was more generally useful.

%===============================================================================
\subsection{Debugging Support}\label{subsec:debugging}
An important property of PostgreSQL is that each client connection creates a new
process. This means there are several layers to claw through to debug issues.
First, is the PostgreSQL postmaster, then the client connection, then within
that is the runtime hook entrance, which leads to $C++$, and inside $C++$
we will be compiling into a JIT runtime, and the bugs can happen inside there.
This poses a challenge for how to debug problems such as segmentation faults and
errors without any logging.

% messy paragraph
This was solved with a combination of the regression tests, unit testing, and
a script to connect |gdb| to dump the stack. The regression tests were already
explored, but the unit tests consist of testing anything unconnected to PostgreSQL.
The issue is that this extension creates a |pgx\_lower.so| which is installed
within PostgreSQL, then the PostgreSQL libraries are used from inside there.
This means if we run without being inside of PostgreSQL, no psql libaries can
be used. As a result, unit tests can only test MLIR functions. Most of the
unit tests were highly situational, and are used when a proper interactive GDB
connection was required within the IDE. Furthermore, unit tests allow the |stderr|
to be visible, which assists greatly with MLIR/LLVM errors that go to |stderr|
and nowhere else.

For the stack-dumping, a script was written, |debug-query.sh| which proved to
be the most useful approach for complex issues. It has the ability to create a
psql connection, git the process ID of the client connection, then connect GDB,
run a desired query, and dump the stack trace. In this way, the majority of
errors were tackled.

%===============================================================================
\subsection{Data Types}\label{subsec:data-types}
PostgreSQL has a large set of data types
(\url{https://www.postgresql.org/docs/current/datatype.html}),
and LingoDB has significantly less. However, for TPC-H we only require a subset
of these. Table ~\ref{tab:lingodb-type-capabilities}
shows which of the LingoDB types are used,
and table~\ref{tab:type-mapping} shows the type mappings. The two primary
workarounds that were implemented was for decimals and the various types of strings.
For decimals, i128 is enough precision for most of the TPC-H tests, and is what
LingoDB was using. However, adjustments had to be made to ensure impossible
to allocate values do not appear, so the precision was capped at |<32, 6>|
That is, 32 digits in the integer part, 6 digits in the decimal places.

For the date types, a comprimise was made that when it receives an interval
type with a months column, it will turn this into days and introduce errors.
However, since the TPC-H queries never use month intervals, this is acceptable.

\begin{table}[H]
	\centering
	\small
	\begin{tabular}{llp{4.5cm}}
		\toprule
		\textbf{DB Dialect Type}            & \textbf{LLVM Type}    & \textbf{Used by pgx-lower?} \\
		\midrule
		\texttt{!db.date<day>}              & \texttt{i64}          & Yes                         \\
		\texttt{!db.date<millisecond>}      & \texttt{i64}          & No                          \\
		\texttt{!db.timestamp<second>}      & \texttt{i64}          & Only if typmod specifies    \\
		\texttt{!db.timestamp<millisecond>} & \texttt{i64}          & Only if typmod specifies    \\
		\texttt{!db.timestamp<microsecond>} & \texttt{i64}          & Yes (default)               \\
		\texttt{!db.timestamp<nanosecond>}  & \texttt{i64}          & Only if typmod specifies    \\
		\texttt{!db.interval<months>}       & \texttt{i64}          & No                          \\
		\texttt{!db.interval<daytime>}      & \texttt{i64}          & Yes                         \\
		\texttt{!db.char<N>}                & \texttt{\{ptr, i32\}} & No (uses !db.string)        \\
		\texttt{!db.string}                 & \texttt{\{ptr, i32\}} & Yes                         \\
		\texttt{!db.decimal<p,s>}           & \texttt{i128}         & Yes                         \\
		\texttt{!db.nullable<T>}            & \texttt{\{T, i1\}}    & Yes                         \\
		\bottomrule
	\end{tabular}
	\caption{LingoDB type system full capabilities}
	\label{tab:lingodb-type-capabilities}
\end{table}

\begin{table}[H]
	\centering
	\small
	\begin{tabular}{lll}
		\toprule
		\textbf{PostgreSQL Type} & \textbf{DB Dialect Type}               & \textbf{LLVM Type}    \\
		\midrule
		\multicolumn{3}{l}{\textit{Integers}}                                                     \\
		\cmidrule{2-3}
		INT2 (SMALLINT)          & \texttt{i16}                           & \texttt{i16}          \\
		INT4 (INTEGER)           & \texttt{i32}                           & \texttt{i32}          \\
		INT8 (BIGINT)            & \texttt{i64}                           & \texttt{i64}          \\
		\addlinespace
		\multicolumn{3}{l}{\textit{Floating Point}}                                               \\
		\cmidrule{2-3}
		FLOAT4 (REAL)            & \texttt{f32}                           & \texttt{f32}          \\
		FLOAT8 (DOUBLE)          & \texttt{f64}                           & \texttt{f64}          \\
		\addlinespace
		\multicolumn{3}{l}{\textit{Boolean}}                                                      \\
		\cmidrule{2-3}
		BOOL                     & \texttt{i1}                            & \texttt{i1}           \\
		\addlinespace
		\multicolumn{3}{l}{\textit{String Types}}                                                 \\
		\cmidrule{2-3}
		TEXT / VARCHAR / BPCHAR  & \texttt{!db.string}                    & \texttt{\{ptr, i32\}} \\
		BYTEA                    & \texttt{!db.string}                    & \texttt{\{ptr, i32\}} \\
		\addlinespace
		\multicolumn{3}{l}{\textit{Numeric}}                                                      \\
		\cmidrule{2-3}
		NUMERIC(p,s)             & \texttt{!db.decimal<p,s>}              & \texttt{i128}         \\
		\addlinespace
		\multicolumn{3}{l}{\textit{Date/Time}}                                                    \\
		\cmidrule{2-3}
		DATE                     & \texttt{!db.date<day>}                 & \texttt{i64}          \\
		TIMESTAMP                & \texttt{!db.timestamp<s|ms|$\mu$s|ns>} & \texttt{i64}          \\
		INTERVAL                 & \texttt{!db.interval<daytime>}         & \texttt{i64}          \\
		\addlinespace
		\multicolumn{3}{l}{\textit{Nullable}}                                                     \\
		\cmidrule{2-3}
		Any nullable column      & \texttt{!db.nullable<T>}               & \texttt{\{T, i1\}}    \\
		\bottomrule
	\end{tabular}
	\caption{PostgreSQL type translation through DB dialect to LLVM}
	\label{tab:type-mapping}
\end{table}

This defines most of the supporting details, and the main two components of
the implementation can be described: the runtime patterns and the plan tree
translation.

%===============================================================================
\subsection{Runtime patterns}\label{subsec:runtime-patterns}
Runtime functions are used in LingoDB for difficult to implement methods in
LLVM, such as a sort algorithm. |pgx-lower| implemented reading tuples from
PostgreSQL, storing them as a result so that they can be streamed one by one,
adjusted several runtime implementations from LingoDB, and changed the sort and
hashtable implementations to rely on the PostgreSQL API rather than standard
collections.

Figure~\ref{fig:runtime-functions} shows the high-level components in a
runtime function. During SQL translation to MLIR, the frontend creates
\texttt{db.runtimecall} operations with a function name and arguments.
These operations are registered in the runtime function registry, which maps
each function name to either a \texttt{FunctionSpec} containing the mangled
C++ symbol name, or a custom lowering lambda. During the DBToStd
lowering pass, the \texttt{RuntimeCallLowering} pattern looks up each runtime
call in the registry and replaces it with a \texttt{func.call} operation
targeting the mangled C++ function. The JIT engine then links these function
calls to the actual compiled C++ runtime implementations, which handle
PostgreSQL-specific operations like tuple access, sorting via
\texttt{tuplesort}, and hash table management using PostgreSQL's memory
contexts. This pattern allows complex operations to be implemented once in C++
and reused across all queries, while maintaining type safety and null handling
semantics through the MLIR type system.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/runtime-functions.drawio.pdf}
	\caption{System design with labels of component sources.}
	\label{fig:runtime-functions}
\end{figure}

The PostgreSQL runtime implements zero-copy tuple access for reading and
result accumulation for output. When scanning a table,
\texttt{openpostgrestable()} creates a heap scan
using \texttt{heapbeginscan()}, and \texttt{readnexttuplefromtable()} stores a
pointer (not a copy) to each tuple in the global
\texttt{gcurrenttuplepassthrough} structure. JIT code
extracts fields via \texttt{extractfield()}, which uses \texttt{heapgetattr()}
and converts PostgreSQL \texttt{Datum} values to native types. For results,
\texttt{tablebuilderadd()} accumulates computed values as \texttt{Datum}
arrays in \texttt{ComputedResultStorage}. When a result tuple
completes, \texttt{addtupletoresult()} streams it back through
PostgreSQL's \texttt{TupleStreamer} by populating a \texttt{TupleTableSlot} and
calling the destination receiver, enabling direct integration with
PostgreSQL's tuple pipeline.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/pgsql-runtime.drawio.pdf}
	\caption{PostgreSQLRuntime.h component design}
	\label{fig:pgsql-runtime}
\end{figure}

The PostgreSQL runtime allows the JIT runtime to read from the psql tables, and
the design of it is visible in Figure~\ref{fig:pgsql-runtime}.
Generated JIT code invokes runtime functions implemented in the C++ layer,
including table operations (\texttt{open\_psql\_table}), field extraction
(\texttt{extract\_field<T>}), result building (\texttt{table\_builder\_add}),
and type conversions between PostgreSQL's \texttt{Datum} representation and
native types. These runtime functions interface with PostgreSQL's C API layer,
which handles heap access for reading tuples, memory management through
PostgreSQL's context system, and tuple streaming for returning results to the
executor. An important part is that when tuples are read from Postgres, 
only the pointers are stored within the C++ storage layer to maintain 
zero-copy semantics. 


Once stored, the JIT code can read from the batch and 
stream tuples back through the output pipeline as well. Streaming the tuples 
back means that the entire table does not build up in RAM,
and instead tuples are returned one by one. This was tested by doing larger table 
scans as avoiding this buildup is essential.

%===============================================================================
\subsection{Plan Tree Translation}\label{subsec:ast-patterns}
asdf
%-------------------------------------------------------------------------------
\subsubsection{Plan translation - Scans}\label{subsubsec:plan-scans}
asdf
%-------------------------------------------------------------------------------
\subsubsection{Plan translation - Aggregations}\label{subsubsec:plan-aggregations}
asdf
%-------------------------------------------------------------------------------
\subsubsection{Plan translation - Joins}\label{subsubsec:plan-joins}
asdf
%-------------------------------------------------------------------------------
\subsubsection{Plan translation - Subqueries}\label{subsubsec:plan-subqueries}
asdf
%-------------------------------------------------------------------------------
\subsubsection{Expression Translation - Variables, Constants, Parameters}\label{subsubsec:expr-vars-consts}
asdf
%-------------------------------------------------------------------------------
\subsubsection{Expression Translation - Nullability}\label{subsubsec:expr-nullability}
asdf
%-------------------------------------------------------------------------------
\subsubsection{Expression Translation - Operators}\label{subsubsec:expr-operators}
asdf

%===============================================================================
\subsection{Configuring JIT compilation settings}\label{subsec:jit-config}
asdf
%===============================================================================
\subsection{Profiling Support}\label{subsec:profiling}
asdf

%===============================================================================
\subsection{Website}\label{subsec:website}
asdf

%===============================================================================
\subsection{Benchmarking}\label{subsec:benchmarking}
asdf