\chapter{Method}\label{ch:project}
In Section~\ref{sec:design} the overarching design is described,
then Section~\ref{sec:implementation} goes over the implementation.

\section{Design}\label{sec:design}
A database and compiler selection required several criteria: strong extension support, wide-spread usage, high performance,
and a volcano execution model as a base. For the compiler,
ideally it would use a similar interface to the target database
when parsing SQL, demonstrate strong performance results, and be open source. 
Such criteria eliminated HyPer, Umbra, and System R, 
leaving Mutable and LingoDB. LingoDB
parses inputs with \texttt{pg\_query}, matching PostgreSQL well.

PostgreSQL and LingoDB were selected. PostgreSQL offers strong extension support,
enabling runtime hook-based execution engine overrides.
TimescaleDB (now TigerData), discussed in Section~\ref{sec:postgresql-related}, exemplifies such approaches \cite{timescaledb}. A significant challenge arose: LingoDB's columnar,
in-memory architecture required substantial adjustments. Additionally, LingoDB lacks
index support, potentially biasing benchmarks against PostgreSQL.
LingoDB's recent versions contain numerous
unneeded features and optimisations, so the 2022 version from their initial paper was used to simplify implementation
effort.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{figures/system_design.drawio.pdf}
	\caption{System design with labels of component sources.}
	\label{fig:overall-system-design}
\end{figure}

LingoDB was integrated into PostgreSQL as seen in Figure~\ref{fig:overall-system-design}.
Blue components represent PostgreSQL, with the left pipeline showing
the entire PostgreSQL execution flow. Queries reach runtime hooks, where a handwritten analyser determines executability before parsing.
Handwritten components appear in light-peach.
Processing continues through LingoDB code with custom runtime hooks and
minor edits (annotated in green). Finally, compilation produces
LLVM IR with embedded runtime hooks for PostgreSQL data access.

Query failures should still allow result return and graceful fallback to PostgreSQL. A try-catch pattern at
the AST parser entrance routes failed queries back to PostgreSQL.
However, such protection does not prevent system panics such as
segmentation faults.

AST Parser implementation was expected to be most time-consuming,
as it receives the optimised plan tree from PostgreSQL.
LingoDB was designed to parse query trees from the
Parser stage in Figure~\ref{fig:overall-system-design}. The implementation includes 18 plan nodes and 14
expression nodes.

TPC-H query support was the final goal.
Test-driven development drove implementation using PostgreSQL's
\texttt{pg\_regress} module for SQL query creation and expected output definition.
A progressive test set built from basic queries up to TPC-H queries.
Such progression enabled incremental node implementation during
development and quick validation of safe changes.

Table~\ref{tab:regression-tests} shows the complete regression test suite,
organized to progressively build complexity from single-row operations to
full TPC-H queries.

	{\footnotesize
		\begin{longtable}{p{4.5cm}p{9cm}}
			\caption{Regression test suite files and their aims} \label{tab:regression-tests}               \\
			\toprule
			\textbf{File Name}                    & \textbf{Aim}                                            \\
			\midrule
			\endfirsthead

			\multicolumn{2}{c}{\tablename\ \thetable\ -- \textit{Continued from previous page}}             \\
			\toprule
			\textbf{File Name}                    & \textbf{Aim}                                            \\
			\midrule
			\endhead

			\midrule
			\multicolumn{2}{r}{\textit{Continued on next page}}                                             \\
			\endfoot

			\bottomrule
			\endlastfoot

			1\_one\_tuple.sql                     & Single row insertion and selection                      \\
			2\_two\_tuples.sql                    & Multiple row retrieval                                  \\
			3\_lots\_of\_tuples.sql               & Large dataset handling (5000 rows)                      \\
			4\_two\_columns\_ints.sql             & Multiple integer columns                                \\
			5\_two\_columns\_diff.sql             & Mixed column types (INTEGER, BOOLEAN)                   \\
			6\_every\_type.sql                    & All supported data types with projections               \\
			7\_sub\_select.sql                    & Column subsetting across Boolean columns                \\
			8\_subset\_all\_types.sql             & Column subsets from multi-type tables                   \\
			9\_basic\_arithmetic\_ops.sql         & Arithmetic operators (+, -, *, /, \%)                   \\
			10\_comparison\_ops.sql               & Comparison operators (=, <>, !=, <, <=, >, >=)          \\
			11\_logical\_ops.sql                  & Logical operators (AND, OR, NOT)                        \\
			12\_null\_handling.sql                & NULL operations (IS NULL, COALESCE)                     \\
			13\_text\_operations.sql              & Text operations (LIKE, \textbar\textbar)                \\
			14\_aggregate\_functions.sql          & Aggregates without GROUP BY (SUM, COUNT, AVG, MIN, MAX) \\
			15\_special\_operators.sql            & BETWEEN, IN, CASE WHEN                                  \\
			16\_debug\_text.sql                   & CHAR(10) with LPAD                                      \\
			17\_where\_simple\_conditions.sql     & WHERE with simple comparisons                           \\
			18\_where\_logical\_combinations.sql  & WHERE with AND/OR/NOT combinations                      \\
			19\_where\_null\_patterns.sql         & WHERE with NULL checks and COALESCE                     \\
			20\_where\_pattern\_matching.sql      & WHERE with LIKE and IN operators                        \\
			21\_order\_by\_basic.sql              & ORDER BY single columns (integers, strings, decimals)   \\
			22\_order\_by\_multiple\_columns.sql  & ORDER BY multiple columns with mixed directions         \\
			23\_order\_by\_expressions.sql        & ORDER BY expressions (not supported - placeholder)      \\
			24\_order\_by\_with\_where.sql        & ORDER BY combined with WHERE                            \\
			25\_group\_by\_simple.sql             & GROUP BY with aggregations and ORDER BY                 \\
			26\_before\_check\_types.sql          & All PostgreSQL types with casts and aggregations        \\
			27\_group\_by\_having.sql             & GROUP BY with HAVING clause                             \\
			28\_group\_by\_with\_where.sql        & GROUP BY with WHERE filtering                           \\
			29\_expressions\_in\_aggregations.sql & Expressions in aggregates (arithmetic, ABS)             \\
			30\_test\_missing\_expressions.sql    & PostgreSQL expression types coverage                    \\
			31\_distinct\_statement.sql           & DISTINCT in SELECT and aggregates                       \\
			32\_decimal\_maths.sql                & Decimal arithmetic operations                           \\
			33\_basic\_joins.sql                  & INNER JOIN                                              \\
			34\_advanced\_joins.sql               & LEFT, RIGHT, SEMI, ANTI joins                           \\
			35\_nested\_queries.sql               & Nested and correlated subqueries                        \\
			36\_tpch\_minimal.sql                 & TPC-H minimal schema variant 1                          \\
			37\_tpch\_minimal\_2.sql              & TPC-H minimal schema variant 2                          \\
			38\_tpch\_minimal\_3.sql              & TPC-H minimal schema variant 3                          \\
			39\_tpch\_minimal.sql                 & TPC-H minimal schema variant 4                          \\
			40\_tpch\_not\_lowered.sql            & TPC-H queries without lowering                          \\
			41\_sorts.sql                         & Sorting with joins                                      \\
			42\_test\_relalg\_function.sql        & Direct RelAlg MLIR execution                            \\
			init\_tpch.sql                        & TPC-H table initialization                              \\
			tpch.sql                              & Full TPC-H benchmark in pgx-lower                       \\
			tpch\_no\_lower.sql                   & TPC-H inside pure PostgreSQL for validation             \\
		\end{longtable}
	}

Node implementation ordering followed the dependency analysis. Foundational
nodes such as the sequential scan and projection are in virtually every query,
while other nodes build on top. By implementing in the dependency order, each new
node could be tested using the previously implemented nodes, and bugs can be
isolated.

\section{Implementation}\label{sec:implementation}

The primary system this project was developed on was a x86\_64 CPU (Ryzen 3600)
and on Ubuntu 25.04. The database was not tested on MacOS or Windows, and this
may lead to issues when installing it independently.

%===============================================================================
\subsection{Integrating LingoDB to PostgreSQL}\label{subsec:integrating-lingodb}
The project was started from \url{https://github.com/mkindahl/pg_extension},
then \texttt{ExecutorRun\_hook} inside of executor.h in PostgreSQL
was used \url{https://doxygen.postgresql.org/executor_8h_source.html} as the
entrance. Within PostgreSQL, surrounding steps exist since the intention is not to replace the entire
executor with hooks, requiring memory context activation and switching.

Next, the \texttt{QueryDesc} pointer, containing the query request, needed to be passed
to C++. A design decision arose from this requirement: Good practice
here is to use smart pointers to prevent memory leaks, but this object is large
and the source of truth about the request. Furthermore, the memory is handled by
the PostgreSQL memory contexts. It was decided that these objects will remain
as raw pointers, causing the C++ to break conventions.

LingoDB was installed as a git submodule and set to a read-only permission.
This was maintained for reference purposes only, and the compilation phases
would be extracted. LingoDB used LLVM 14, and was upgraded to LLVM 20 to
modernise it and slightly better support with C++20 (some workarounds were
required with LLVM 14 that could be skipped with LLVM 20). However,
since this is the C++ API for LLVM, a large amount of the LingoDB code
had to be adjusted to compile.

%===============================================================================
\subsection{Logging infrastructure}\label{subsec:logging}
PostgreSQL has its own logging infrastructure that routes through its \texttt{elog}
command, but it was decided that a two-layer logging infrastructure was required.
The first layer is the level, (\texttt{DEBUG}, \texttt{IR}, \texttt{TRACE}, \texttt{WARNING\_LEVEL},
\texttt{ERROR\_LEVEL}, and more), and the second represents which layer of the design
the log is inside of (\texttt{AST\_TRANSLATE}, \texttt{RELALG\_LOWER}, \texttt{DB\_LOWER}, and more).
This meant if the AST translation was being worked on, all the logs in only that
section of the codebase could be enabled. The core benefit of this is that
the logs are lengthy so it becomes easier to navigate.

An issue that was encountered was that the LLVM/MLIR logs would route through
stderr, and this caused difficult-to-debug issues until the hook was found
to redirect this into \texttt{elog} as well. Subsection~\ref{subsec:debugging} will
explore one of the workarounds that was needed at this stage.

Lastly, for error handling mostly \texttt{std::runtime\_error} was utilised. This served
as a global way to log the stack trace and roll back to PostgreSQL's execution.
There initially was an implementation of error handling with severity levels
and messages, but the simplicity of a single command that rolled back to
PostgreSQL was more generally useful. If an error is thrown in pgx-lower, the
progress in compiling is dumped then it fully falls over to PostgreSQL, even if
the result is half-complete.

%===============================================================================
\subsection{Debugging Support}\label{subsec:debugging}
An important property of PostgreSQL is that each client connection creates a new
process. Debugging requires navigating several layers: the PostgreSQL postmaster, the client connection,
the runtime hook entrance, C++ code, and the JIT runtime.
Bugs can occur at any of these levels, making debugging challenging.
This poses a particular difficulty when dealing with segmentation faults and other errors
that lack logging information.

This was solved with a combination of the regression tests, unit testing, and
a script to connect \texttt{gdb} to dump the stack. The regression tests were already
explored, but the unit tests test components unconnected to PostgreSQL.
The issue is that this extension creates a \texttt{pgx\_lower.so} which is loaded
into PostgreSQL, and the PostgreSQL libraries are used from that context.
This means if we run without being inside PostgreSQL, no psql libraries can
be used. As a result, unit tests can only test MLIR functions. Most of the
unit tests were highly situational, and are used when a proper interactive GDB
connection was required within the IDE. Furthermore, unit tests allow the \texttt{stderr}
to be visible, which assists greatly with MLIR/LLVM errors that go to \texttt{stderr}
and nowhere else.

For the stack-dumping, a script was written, \texttt{debug-query.sh}, which proved to
be the most useful approach for complex issues. It has the ability to create a
psql connection, get the process ID of the client connection, then connect GDB,
run a desired query, and dump the stack trace. In this way, the majority of
errors were tackled.

%===============================================================================
\subsection{Data Types}\label{subsec:data-types}
PostgreSQL has a large set of data types
(\url{https://www.postgresql.org/docs/current/datatype.html}),
and LingoDB has significantly less. However, for TPC-H we only require a subset
of these. Table~\ref{tab:lingodb-type-capabilities}
shows which of the LingoDB types are used,
and Table~\ref{tab:type-mapping} shows the type mappings. The two primary
workarounds handle decimals and the various types of strings.
For decimals, \texttt{i128} provides enough precision for most TPC-H tests, which is what
LingoDB was using. However, adjustments had to be made to prevent values that cannot be allocated from appearing, so the precision was capped at \texttt{<32, 6>}.
That is, 32 digits in the integer part and 6 digits in the decimal places.

For the date types, a compromise was made that when it receives an interval
type with a months column, it will turn this into days and introduce errors.
However, since the TPC-H queries never use month intervals, this is acceptable.

\begin{table}[H]
	\centering
	\small
	\begin{tabular}{llp{4.5cm}}
		\toprule
		\textbf{DB Dialect Type}            & \textbf{LLVM Type}    & \textbf{Used by pgx-lower?} \\
		\midrule
		\texttt{!db.date<day>}              & \texttt{i64}          & Yes                         \\
		\texttt{!db.date<millisecond>}      & \texttt{i64}          & No                          \\
		\texttt{!db.timestamp<second>}      & \texttt{i64}          & Only if typmod specifies    \\
		\texttt{!db.timestamp<millisecond>} & \texttt{i64}          & Only if typmod specifies    \\
		\texttt{!db.timestamp<microsecond>} & \texttt{i64}          & Yes (default)               \\
		\texttt{!db.timestamp<nanosecond>}  & \texttt{i64}          & Only if typmod specifies    \\
		\texttt{!db.interval<months>}       & \texttt{i64}          & No                          \\
		\texttt{!db.interval<daytime>}      & \texttt{i64}          & Yes                         \\
		\texttt{!db.char<N>}                & \texttt{\{ptr, i32\}} & No (uses !db.string)        \\
		\texttt{!db.string}                 & \texttt{\{ptr, i32\}} & Yes                         \\
		\texttt{!db.decimal<p,s>}           & \texttt{i128}         & Yes                         \\
		\texttt{!db.nullable<T>}            & \texttt{\{T, i1\}}    & Yes                         \\
		\bottomrule
	\end{tabular}
	\caption{LingoDB type system full capabilities}
	\label{tab:lingodb-type-capabilities}
\end{table}

\begin{table}[H]
	\centering
	\small
	\begin{tabular}{lll}
		\toprule
		\textbf{PostgreSQL Type} & \textbf{DB Dialect Type}               & \textbf{LLVM Type}    \\
		\midrule
		\multicolumn{3}{l}{\textit{Integers}}                                                     \\
		\cmidrule{2-3}
		INT2 (SMALLINT)          & \texttt{i16}                           & \texttt{i16}          \\
		INT4 (INTEGER)           & \texttt{i32}                           & \texttt{i32}          \\
		INT8 (BIGINT)            & \texttt{i64}                           & \texttt{i64}          \\
		\addlinespace
		\multicolumn{3}{l}{\textit{Floating Point}}                                               \\
		\cmidrule{2-3}
		FLOAT4 (REAL)            & \texttt{f32}                           & \texttt{f32}          \\
		FLOAT8 (DOUBLE)          & \texttt{f64}                           & \texttt{f64}          \\
		\addlinespace
		\multicolumn{3}{l}{\textit{Boolean}}                                                      \\
		\cmidrule{2-3}
		BOOL                     & \texttt{i1}                            & \texttt{i1}           \\
		\addlinespace
		\multicolumn{3}{l}{\textit{String Types}}                                                 \\
		\cmidrule{2-3}
		TEXT / VARCHAR / BPCHAR  & \texttt{!db.string}                    & \texttt{\{ptr, i32\}} \\
		BYTEA                    & \texttt{!db.string}                    & \texttt{\{ptr, i32\}} \\
		\addlinespace
		\multicolumn{3}{l}{\textit{Numeric}}                                                      \\
		\cmidrule{2-3}
		NUMERIC(p,s)             & \texttt{!db.decimal<p,s>}              & \texttt{i128}         \\
		\addlinespace
		\multicolumn{3}{l}{\textit{Date/Time}}                                                    \\
		\cmidrule{2-3}
		DATE                     & \texttt{!db.date<day>}                 & \texttt{i64}          \\
		TIMESTAMP                & \texttt{!db.timestamp<s|ms|$\mu$s|ns>} & \texttt{i64}          \\
		INTERVAL                 & \texttt{!db.interval<daytime>}         & \texttt{i64}          \\
		\addlinespace
		\multicolumn{3}{l}{\textit{Nullable}}                                                     \\
		\cmidrule{2-3}
		Any nullable column      & \texttt{!db.nullable<T>}               & \texttt{\{T, i1\}}    \\
		\bottomrule
	\end{tabular}
	\caption{PostgreSQL type translation through DB dialect to LLVM}
	\label{tab:type-mapping}
\end{table}

\subsection{LingoDB Dialect Changes}\label{subsec:dialect-changes}
The MLIR dialects from LingoDB required modifications to support LLVM 20 and
pgx-lower's specific needs. Table~\ref{tab:dialect-summary} summarizes the key
changes across the DB, DSA, RelAlg, and Util dialects. The changes were primarily API compatibility updates with minimal semantic modifications.
The scope included 94 operations total across all dialects (93 in LingoDB, 94 in PGX)
and 21 types (identical count), with most changes addressing LLVM 20 API compatibility.

\begin{table}[H]
	\centering
	\small
	\begin{tabular}{lp{2.5cm}p{7.5cm}}
		\toprule
		\textbf{Category}        & \textbf{Count} & \textbf{Description}                                                  \\
		\midrule
		MLIR API Updates         & ~30 changes    & NoSideEffect → Pure, Optional → std::optional, added OpaqueProperties \\
		Include Path Changes     & 100\% of files & mlir/Dialect/ → lingodb/mlir/Dialect/                                 \\
		Dialect Renames          & 1              & Arithmetic → Arith (MLIR core change)                                 \\
		New Operations           & 1              & SetDecimalScaleOp in DSA                                              \\
		Modified Operations      & 2              & DSA\_SortOp (supports collections), BaseTableOp (added column\_order) \\
		Namespace Clarifications & 6 interfaces   & Added explicit cppNamespace declarations                              \\
		Convenience Features     & 4 dialects     & Added useDefaultTypePrinterParser = 1                                 \\
		Fastmath Support         & 2 patterns     & Added fastmath flags to arithmetic canonicalization                   \\
		Code Cleanup             & ~5 locations   & Removed comments, simplified logic                                    \\
		\bottomrule
	\end{tabular}
	\caption{Summary of key differences between LingoDB and pgx-lower dialects}
	\label{tab:dialect-summary}
\end{table}

This defines most of the supporting details. The main two components of
the implementation are the runtime patterns and the plan tree
translation.

%===============================================================================
\subsection{Query Analyser}\label{subsec:query-analyser}
While the query analyser is fully written, in the final state it routes all
queries through pgx-lower for testing. This enables testing new features and
identifying where failures occur. It functions by doing a depth-first search through the plan
tree and validating that the nodes are supported by the engine. In the
future, this component could be enhanced to decide whether a query is worth
running based on its cost metrics.

%===============================================================================
\subsection{Runtime patterns}\label{subsec:runtime-patterns}
Runtime functions are used in LingoDB for methods that are difficult to implement in
LLVM, such as sorting algorithms. \texttt{pgx-lower} adds several custom runtime implementations:
reading tuples from PostgreSQL and storing them for streaming, modifying LingoDB's
runtime implementations, and replacing the sort and hash table implementations to use
PostgreSQL's API instead of standard library functions.

Figure~\ref{fig:runtime-functions} shows the high-level components in a
runtime function. During SQL translation to MLIR, the frontend creates
\texttt{db.runtimecall} operations with a function name and arguments.
These operations are registered in the runtime function registry, which maps
each function name to either a \texttt{FunctionSpec} containing the mangled
C++ symbol name, or a custom lowering lambda. During the DBToStd
lowering pass, the \texttt{RuntimeCallLowering} pattern looks up each runtime
call in the registry and replaces it with a \texttt{func.call} operation
targeting the mangled C++ function. The JIT engine then links these function
calls to the actual compiled C++ runtime implementations, which handle
PostgreSQL-specific operations like tuple access, sorting via
\texttt{tuplesort}, and hash table management using PostgreSQL's memory
contexts. This pattern allows complex operations to be implemented once in C++
and reused across all queries, while maintaining type safety and null handling
semantics through the MLIR type system.

LingoDB had a code generation step in their CMakeLists, \texttt{gen\_rt\_def},
which supports this. It parses a given C++ file, then generates a header
file in the build files which has the mangled name lookup, so that the
developer does not need to reimplement that section repeatedly.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/runtime-functions.drawio.pdf}
	\caption{System design with labels of component sources.}
	\label{fig:runtime-functions}
\end{figure}

The PostgreSQL runtime implements zero-copy tuple access for reading and
result accumulation for output. When scanning a table,
\texttt{open\_postgres\_table()} creates a heap scan
using \texttt{heap\_begin\_scan()}, and
\texttt{read\_next\_tuple\_from\_table()} stores a
pointer (not a copy) to each tuple in the global
\texttt{g\_current\_tuple\_passthrough} structure. JIT code
extracts fields via \texttt{extract\_field()}, which uses \texttt{heap\_get\_attr()}
and converts PostgreSQL \texttt{Datum} values to native types. For results,
\texttt{table\_builder\_add()} accumulates computed values as \texttt{Datum}
arrays in \texttt{ComputedResultStorage}. When a result tuple
completes, \texttt{add\_tuple\_to\_result()} streams it back through
PostgreSQL's \texttt{TupleStreamer} by populating a \texttt{TupleTableSlot} and
calling the destination receiver, enabling direct integration with
PostgreSQL's tuple pipeline.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/pgsql-runtime.drawio.pdf}
	\caption{PostgreSQLRuntime.h component design}
	\label{fig:pgsql-runtime}
\end{figure}

The PostgreSQL runtime allows the JIT runtime to read from the psql tables, and
the design of it is visible in Figure~\ref{fig:pgsql-runtime}.
Generated JIT code invokes runtime functions implemented in the C++ layer,
including table operations (\texttt{open\_psql\_table}), field extraction
(\texttt{extract\_field<T>}), result building (\texttt{table\_builder\_add}),
and type conversions between PostgreSQL's \texttt{Datum} representation and
native types. These runtime functions interface with PostgreSQL's C API layer,
which handles heap access for reading tuples, memory management through
PostgreSQL's context system, and tuple streaming for returning results to the
executor. An important part is that when tuples are read from Postgres,
only the pointers are stored within the C++ storage layer to maintain
zero-copy semantics.

Once stored, the JIT code can read from the batch and
stream tuples back through the output pipeline as well. Streaming the tuples
out from JIT means that the entire table does not build up in RAM,
and instead tuples are returned one by one. This was tested by doing larger table
scans as avoiding this buildup is essential.

LingoDB's sort and hashtable runtimes were relying on \texttt{std::sort} and
\texttt{std::unordered\_map} respectively. This is problematic because as an
on-disk database we need to handle disk spillage in these scenarios. Rather than
reinventing these, leaning on psql's implementation of these solves these issues
and creates a blueprint for further implementations.

Most of the LingoDB lowerings bake metadata (such as table names) into the compiled
binary by JSON-encoding it as a string. Instead of that, for the sort and
hash table runtimes a specification pointer was used. Inside the plan translation
stage, a struct was built and allocated with the transaction memory context, then
the pointer to this was baked into the compiled binary instead. This enabled
these runtimes to trigger without doing JSON deserialisation, and creating the
operations them could skip this stage. This is something that a regular
compiler would be incapable of doing, because the binary needs to be a standalone
program, but in this context it can be relied upon.

%===============================================================================
\subsection{Plan Tree Translation}\label{subsec:ast-patterns}
The plan tree translation converts PostgreSQL's execution plan nodes into
RelAlg MLIR operations. Figure~\ref{fig:ast-impl} shows where this fits
into the broader design. Within the AST Parser component, we examine the
PostgreSQL tag on the node to determine the plan node type,
then a recursive descent parser starts translating. Each translation function follows a consistent pattern.
First, children of the plan are translated using post-order traversal.
Then, the node is translated into the MLIR relational algebra dialect, and a translation result is returned.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{figures/ast-impl.drawio.pdf}
	\caption{AST translation design and high-level steps in each function}
	\label{fig:ast-impl}
\end{figure}

The translation functions follow a consistent pattern, as shown in
Listing~\ref{lst:plan-translate-methods}. Each function takes the query
context and a PostgreSQL plan node pointer, performs the translation, and
returns a \texttt{TranslationResult}. The \texttt{QueryCtxT} object is passed down the tree,
and when mutated, a new instance is created for child nodes. Meanwhile,
\texttt{TranslationResult}s flow upward to represent each node's output, providing strong
type-correctness in theory. However, this pattern is not strictly enforced in practice.

\begin{lstlisting}[
	caption=Plan node translation method signatures. The expression nodes follow the same pattern.,
	label=lst:plan-translate-methods,
	breaklines=true
]
auto translate_plan_node(QueryCtxT& ctx, Plan* plan) -> TranslationResult;
auto translate_seq_scan(QueryCtxT& ctx, SeqScan* seqScan) -> TranslationResult;
auto translate_index_scan(QueryCtxT& ctx, IndexScan* indexScan) -> TranslationResult;
auto translate_index_only_scan(QueryCtxT& ctx, IndexOnlyScan* indexOnlyScan) -> TranslationResult;
auto translate_bitmap_heap_scan(QueryCtxT& ctx, BitmapHeapScan* bitmapScan) -> TranslationResult;
auto translate_agg(QueryCtxT& ctx, const Agg* agg) -> TranslationResult;
auto translate_sort(QueryCtxT& ctx, const Sort* sort) -> TranslationResult;
auto translate_limit(QueryCtxT& ctx, const Limit* limit) -> TranslationResult;
auto translate_gather(QueryCtxT& ctx, const Gather* gather) -> TranslationResult;
auto translate_gather_merge(QueryCtxT& ctx, const GatherMerge* gatherMerge) -> TranslationResult;
auto translate_merge_join(QueryCtxT& ctx, MergeJoin* mergeJoin) -> TranslationResult;
auto translate_hash_join(QueryCtxT& ctx, HashJoin* hashJoin) -> TranslationResult;
auto translate_hash(QueryCtxT& ctx, const Hash* hash) -> TranslationResult;
auto translate_nest_loop(QueryCtxT& ctx, NestLoop* nestLoop) -> TranslationResult;
auto translate_material(QueryCtxT& ctx, const Material* material) -> TranslationResult;
auto translate_memoize(QueryCtxT& ctx, const Memoize* memoize) -> TranslationResult;
auto translate_subquery_scan(QueryCtxT& ctx, SubqueryScan* subqueryScan) -> TranslationResult;
auto translate_cte_scan(QueryCtxT& ctx, const CteScan* cteScan) -> TranslationResult;
\end{lstlisting}

The 14 expression node types are documented in
Table~\ref{tab:expr-nodes}, and the 18 plan node types in Table~\ref{tab:plan-nodes}.
The subsections explain these more specifically.

\begin{table}[H]
	\centering
	\small
	\begin{tabular}{lllp{6cm}}
		\toprule
		\textbf{File} & \textbf{Node Tag}             & \textbf{Implementation Note}                       \\
		\midrule
		basic         & \texttt{T\_BoolExpr}          & Boolean AND/OR/NOT - with short-circuit evaluation \\
		basic         & \texttt{T\_Const}             & Constant value - converts Datum to MLIR constant   \\
		basic         & \texttt{T\_CoalesceExpr}      & COALESCE(...) - first non-null using if-else       \\
		basic         & \texttt{T\_CoerceViaIO}       & Type coercion - calls PostgreSQL cast functions    \\
		basic         & \texttt{T\_NullTest}          & IS NULL checks - generates nullable type tests     \\
		basic         & \texttt{T\_Param}             & Query parameter - looks up from context            \\
		basic         & \texttt{T\_RelabelType}       & Type relabeling - transparent wrapper              \\
		basic         & \texttt{T\_Var}               & Column reference - resolves varattno to column     \\
		complex       & \texttt{T\_Aggref}            & Aggregate functions - creates AggregationOp        \\
		complex       & \texttt{T\_CaseExpr}          & CASE WHEN ... END - nested if-else operations      \\
		complex       & \texttt{T\_ScalarArrayOpExpr} & IN/ANY/ALL with arrays - loops over elements       \\
		complex       & \texttt{T\_SubPlan}           & Subquery expression - materializes and uses result \\
		functions     & \texttt{T\_FuncExpr}          & Function calls - maps PostgreSQL functions to MLIR \\
		operators     & \texttt{T\_OpExpr}            & Binary/unary operators                             \\
		\bottomrule
	\end{tabular}
	\caption{Expression node translations}
	\label{tab:expr-nodes}
\end{table}

\begin{table}[H]
	\centering
	\small
	\begin{tabular}{lllp{6cm}}
		\toprule
		\textbf{File} & \textbf{Node Tag}           & \textbf{Implementation Note}                          \\
		\midrule
		agg           & \texttt{T\_Agg}             & Aggregation - AggregationOp with grouping keys        \\
		joins         & \texttt{T\_HashJoin}        & Hash join - InnerJoinOp with hash implementation      \\
		joins         & \texttt{T\_MergeJoin}       & Merge join - InnerJoinOp with merge semantics         \\
		joins         & \texttt{T\_NestLoop}        & Nested loop join - CrossProductOp or InnerJoinOp      \\
		scans         & \texttt{T\_BitmapHeapScan}  & Bitmap heap scan - SeqScan with quals                 \\
		scans         & \texttt{T\_CteScan}         & CTE scan - looks up CTE and creates BaseTableOp       \\
		scans         & \texttt{T\_IndexOnlyScan}   & Index-only scan - treated as SeqScan                  \\
		scans         & \texttt{T\_IndexScan}       & Index scan - treated as SeqScan                       \\
		scans         & \texttt{T\_SeqScan}         & Sequential scan - BaseTableOp with optional Selection \\
		scans         & \texttt{T\_SubqueryScan}    & Subquery scan - recursively translates subquery       \\
		utils         & \texttt{T\_Gather}          & Gather workers - pass-through (no parallelism)        \\
		utils         & \texttt{T\_GatherMerge}     & Gather merge - pass-through (no parallelism)          \\
		utils         & \texttt{T\_Hash}            & Hash node - pass-through to child                     \\
		utils         & \texttt{T\_IncrementalSort} & Incremental sort - delegates to Sort                  \\
		utils         & \texttt{T\_Limit}           & Limit/offset - LimitOp with count and offset          \\
		utils         & \texttt{T\_Material}        & Materialize - pass-through (no explicit op)           \\
		utils         & \texttt{T\_Memoize}         & Memoize - pass-through to child                       \\
		utils         & \texttt{T\_Sort}            & Sort operation - SortOp with sort keys                \\
		\bottomrule
	\end{tabular}
	\caption{Plan node translations}
	\label{tab:plan-nodes}
\end{table}

Several common node definitions are helpful to understand. Nodes commonly
have an \texttt{InitPlan} parameter, which is a function called
before the node executes and initializes variables such as parameters and
catalogue lookups. \texttt{targetlist} contains the output
of the node, and \texttt{qual} specifies which tuples should pass through.
Join nodes have left and right child trees,
typically referred to as inner and outer children. These signify the inner and
outer loops of the nested for-loop that is created.

%-------------------------------------------------------------------------------
\subsubsection{Expression Translation - Variables, Constants, Parameters}\label{subsubsec:expr-vars-consts}
PostgreSQL identifies values using variable nodes and parameter nodes. These are
tracked in a schema/column manager class and the \texttt{QueryCtxT} object. Variables are typically
defined within scans, while parameters are intermediate products.
Identifying them presented challenges due to multiple interacting identifiers (\texttt{varno}, \texttt{varattno},
and special values for index joins). To handle this complexity, a generic function was
added to the QueryCtxT object: \texttt{resolve\_var}. This function is used extensively
throughout the translation logic.

Parameters are mostly defined within the InitPlan, and one key type is
the cached scalar type.

%-------------------------------------------------------------------------------
\subsubsection{Plan translation - Scans}\label{subsubsec:plan-scans}
PostgreSQL supports multiple scan types: sequential scans, subquery scans, index scans, index-only scans,
bitmap heap scans, and CTE scans. However, all scan types except subquery and CTE scans
map to sequential scans in this implementation. This trade-off reduces implementation complexity at the cost of
query optimisation, particularly for index scans.

Index scans use special annotations for variables via \texttt{INDEX\_VAR},
which requires custom variable resolution. Additionally,
we handle the qualifiers (scan filters) \texttt{indexqual} and \texttt{recheckqual}
as generic filters. In PostgreSQL, these qualify at different stages, but since we skip
index implementation, both become generic filters here.

CTE scan plans are defined within the InitPlan of nodes, but still route through
the primary plan switch statement logic. Neither CTE plans nor subqueries
currently offer de-duplication to simplify implementation. That is, if a query
uses the same CTE reference or writes the same subquery twice, they will
currently be lowered into two different LLVM chunks of code rather than
congregated and referenced.

%-------------------------------------------------------------------------------
\subsubsection{Plan translation - Aggregations}\label{subsubsec:plan-aggregations}
Aggregation is a complicated node type with many properties. It includes an aggregation
strategy (ignored in favour of a simpler algorithm), splitting specification (not utilised),
and group columns. The node tracks the number of groups it produces and manages its own
operators such as COUNT, SUM, and more. Additionally, it uses special varnos for variable
lookups (represented as -2), requiring a new context object, and supports DISTINCT statements.

Most of the pain was with specific edge cases that arise in the
simplification. For instance, \texttt{COUNT(*)} behaves differently in combining
mode where parallel workers provide partial counts rather than raw rows,
requiring translation to SUM instead of CountRowsOp. Similarly,
\texttt{HAVING} clauses can reference aggregates not present in the SELECT list,
necessitating a discovery pass with \texttt{find\_all\_aggrefs()} to ensure
all required aggregates are computed before filtering. The use of magic
number varno=-2 to identify aggregate references, while necessary
to distinguish them from regular column references, breaks the normal
variable resolution flow and requires special handling throughout
the expression translator.

%-------------------------------------------------------------------------------
\subsubsection{Plan translation - Joins}\label{subsubsec:plan-joins}
For joins, two layers exist for translation: the type of join, and
the algorithm used by the join. The type of join refers to inner, semi, anti,
right-anti, left/right joins, and full joins. The semi and anti join types are
not specifically translated, and instead rely on \texttt{EXISTS}/
\texttt{NOT EXISTS} translations because they are semantically the same
operation.

The algorithm used by the join refers to merge, nestloop, or hash joins. Following
LingoDB's pattern, the merge joins are turned into hash joins so that
there does not have to be additional lowering code. A challenge was that
nest loops can carry parameters, so a new query context has to be created,
the parameter has to be registered and inserted into the lookups.

One issue in the joins implementation is preventing double computations. LingoDB handles
this by computing the inner join separately, building a vector of results, and then
iterating over the outer operation while reusing the pre-computed inner section.
This approach prevents duplicated computation at the cost of increased memory usage.
While theoretically acceptable, the vector would need to implement disk spillage.
In practice, memory usage did not become problematic enough to require this feature.
%-------------------------------------------------------------------------------
\subsubsection{Expression Translation - Nullability}\label{subsubsec:expr-nullability}
PostgreSQL tracks nullability information in the plan tree passed to pgx-lower.
However, LingoDB's lowering operations can create situations where previously non-null
objects become nullable through outer joins, aggregations, unions, and predicate evaluation.
Since nullability propagates like not-a-number (affecting everything it touches),
this introduces significant implementation complexity.

One note to be aware of is that LingoDB and PostgreSQL have inverted null flags.
That is, in LingoDB 1 means valid, and in PostgreSQL 1 means null. This
causes confusion with the runtime functions needing to invert flags back and
forth.

%-------------------------------------------------------------------------------
\subsubsection{Expression Translation - Operators and OID strings}\label{subsubsec:expr-operators}
Within operators, the primary challenge is the type conversions and quirks.
Comparing two BPCHARs requires adding padding for the surrounding space.
To implement implicit upcasting, a class was extracted from LingoDB's
DB dialect: \texttt{SQLTypeInference}. Rather than relying on PostgreSQL's OID system
for finding operations, operators are converted to strings (">", "<", etc.) for lookup.
This prevents issues with OID precision specifications that lead to unidentified operations.
The same approach is used in function nodes, aggregation functions, sort operations, and scalar maths.
When performing operations on different types, \texttt{SQLTypeInference} automatically upcasts both operands
to the larger datatype. For example, with \texttt{i16} + \texttt{i32}, both values are cast to \texttt{i32}.

%-------------------------------------------------------------------------------
\subsubsection{Translation - Others}\label{subsubsec:translation-others}
Many of these nodes are pass through nodes or delegated to another,
sibling node, such as \texttt{T\_Hash}, \texttt{T\_Material},
\texttt{T\_Memoize}, and \texttt{T\_RelabelType}. Furthermore, nodes
also come with executor hints and cost metrics which were skipped over rather
than dragged through LingoDB, as the optimisations were already done by
Postgres. IN/ANY operations are also converted into EXISTS operations,
several operations such as scalar subqueries are always marked as nullable,
and CastOps are also made frequently to defer casting to later layers.

%===============================================================================
\subsection{Configuring JIT compilation settings}\label{subsec:jit-config}
Not much tinkering was done with the JIT optimisation flags, the minimum
optimisation passes were used so that it can compile end-to-end, and
\texttt{llvm::CodeGenOptLevel::Default} was used as the optimisation level.
These optimisation passes consist of SROA, InstCombinePass, PromotePass,
LICM pass, reassociation pass, GVN pass, and simplify GVN pass.

These passes perform fundamental optimisations~\cite{llvm-passes}. SROA (Scalar
Replacement of Aggregates) promotes stack-allocated structures into SSA registers.
That is, an allocation on the stack is hoisted up into global space so that
the space is reused rather than reallocated every time.
InstCombine simplifies instructions through algebraic transformations, while PromotePass
elevates memory operations to register operations. LICM (Loop Invariant Code Motion)
moves loop-independent code outside of loops by hoisting to preheaders or sinking to exit blocks.
The reassociation pass reorders expressions to enable further optimisations.
GVN (Global Value Numbering) eliminates redundant computations by identifying values that must be equal.

The consensus appeared to be that \texttt{-O2} should be used on it
and moved on. This means it is possible to do more tuning work on this.

%===============================================================================
\subsection{Profiling Support}\label{subsec:profiling}
Code infrastructure was written to support magic-trace for profiling and isolating issues.
A dedicated machine was configured for this purpose: an Intel i5-6500T with 16 GB of RAM
and a Samsung MZVLB256HAHQ-000L7 NVMe disk.
This was particularly useful for isolating obvious bottlenecks
within the system and understanding the latency when compared to PostgreSQL.
Figure~\ref{fig:psql-magic} represents the flame chart for query 3, and has a
runtime of approximately 260 milliseconds. The functions that it calls are clear,
and you can see how the query runs over time.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.2]{figures/psql-q03-magictrace.png}
	\caption{PostgreSQL's magic-trace flame chart for TPC-H query 3 at scale factor 0.05 (approximately 5 MB of data)}
	\label{fig:psql-magic}
\end{figure}

The flame chart before any optimisations were applied is visible in
Figure~\ref{fig:pgx-magic-before}. In that chart it is visible that too much
time is spent inside the LLVM execution (those spikes in the last 2/3rds are
table reads). After adjusting how tuples are read, ensuring joins go to the
correct algorithm, introducing Postgres's tuple-slot reading API, and disabling
logs, the chart looks like Figure~\ref{fig:pgx-magic-after}. These adjustments
improved the latency from 4.5 seconds to approximately 400 milliseconds.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.2]{figures/pgx-lower-before-q03.png}
	\caption{pgx-lower's magic-trace flame chart for TPC-H query 3 at scale factor 0.05 before optimisation}
	\label{fig:pgx-magic-before}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.2]{figures/pgx-lower-after-q03.png}
	\caption{pgx-lower's magic-trace flame chart for TPC-H query 3 at scale factor 0.05 after optimisation}
	\label{fig:pgx-magic-after}
\end{figure}

Subsection~\ref{subsec:benchmarking} explains the specifics of running these in a stable way.

%===============================================================================
\subsection{Website}\label{subsec:website}
A small website was prepared so that users can interact with the lowerings and
the compiler without installing the system themselves at \url{https://pgx.zyros.dev/query}.
Keep in mind that it relies on caching the results, it has a scale factor of 0.01
(10 MB of data), and the pgx-lower system there is (as of writing),
running a debug build which has significantly longer runtimes. The
implementation for this is at \url{https://github.com/zyros-dev/pgx-lower-addons}.
The implementation uses several technologies: Python for the backend server, SQLite for query caching,
and React for the frontend. Docker containers support the reverse proxy with Nginx, while a
private Grafana dashboard provides health monitoring.

%===============================================================================
\subsection{Benchmarking and Validation}\label{subsec:benchmarking}
A challenge is that PostgreSQL contains a non-deterministic optimiser,
and many small factors can affect runs. For this reason, a python script
was created that reads from a YAML file, and does a benchmark run.
This means we can specify runs beforehand, and run them robustly over a long
period. Also, this benchmarking run computes a hash of the outputs
between PostgreSQL and pgx-lower to validate the outputs are correct between
all the runs, and the hashes were compared. This avoids storing large
amounts of data over time, while the issue can still be rediscovered in
a large batch of runs.

The benchmark configurations used are displayed in Listing~\ref{lst:benchmark-config}.
These configurations allow testing across different scale factors, with and without
indexes, and with varying iteration counts to understand performance characteristics.
With multiple iterations, graphs that contain distributions can be created.
These were decided by bucketing queries into small scale factor (0.01, or 1 MB
of data) to show the overhead cost of the JIT compiler, medium scale factor
(0.16) to show how Postgres scales while still keeping all the queries enabled
with indexes, and lastly scale factor 1 with the very time-consuming queries
completely disabled. These disabled queries would take on the order of hours in
PostgreSQL, so benchmarking them was too time-consuming.

To disable indexes, \texttt{cur.execute("SET enable\_indexscan = off;")} and
\texttt{cur.execute("SET enable\_bitmapscan = off;")} were used in conjunction.
This means when the benchmarks say index scan is disabled, the bit map scan
is as well.

\begin{lstlisting}[caption={Benchmark configurations for TPC-H testing},label={lst:benchmark-config}]
full:
  runs:
    - container: benchmark
      scale_factor: 0.01
      iterations: 5
      profile: false
      indexes: false
      skipped_queries: ""
      label: "SF=0.01, indexes disabled, 5 iterations"

    - container: benchmark
      scale_factor: 0.01
      iterations: 100
      profile: false
      indexes: false
      skipped_queries: "q07,q20"
      label: "SF=0.01, indexes disabled - excluding postgres {q07,q20}, 100 iterations"

    - container: benchmark
      scale_factor: 0.01
      iterations: 100
      profile: false
      indexes: true
      skipped_queries: ""
      label: "SF=0.01, indexes enabled, 100 iterations"

    - container: benchmark
      scale_factor: 0.16
      iterations: 5
      profile: false
      indexes: true
      skipped_queries: ""
      label: "SF=0.16, indexes enabled, 5 iterations"

    - container: benchmark
      scale_factor: 0.16
      iterations: 100
      profile: false
      indexes: true
      skipped_queries: "q17,q20"
      label: "SF=0.16, indexes enabled, excluding {q17,q20}, 100 iterations"

    - container: benchmark
      scale_factor: 1
      iterations: 100
      profile: false
      indexes: false
      skipped_queries: "q02,q17,q20,q21"
      label: "SF=1, indexes disabled, excluding {q02,q17,q20,q21}, 100 iterations"
\end{lstlisting}

One thing to note here is that it was decided that only PostgreSQL and pgx-lower
would be compared, rather than all the databases mentioned in
Chapter~\ref{ch:related-work}. As Section~\ref{sec:benchmarking} showed that
the impact of PostgreSQL's architecture being on disk makes it significantly
slower than any of the other databases.

The magic trace profiling also functions through this script, which is
what the \texttt{profile} tag there is for.