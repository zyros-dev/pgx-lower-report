\chapter{Method}\label{ch:project}
In section~\ref{sec:design} the overarching design is described,
then section~\ref{sec:implementation} goes over the implementation.

\section{Design}\label{sec:design}
The first decision is which database and which compiler this project should use.
Something with strong extension support, wide-spread usage, high performance,
and a volcano execution model is needed as a base. For the compiler,
it would be ideal if they already use a similar interface to the target database
when they parse SQL, and have promising results in their performance. This
removes HyPer, Umbra, and System R, and leaves Mutable and LingoDB. LingoDB
parses its inputs with \texttt{pg\_query}, so it matches with PostgreSQL.

As a result, PostgreSQL and LingoDB were chosen. PostgreSQL offers strong support
for extensions, and it is possible to override its execution engine using runtime
hooks. An example of this Tiger data, which was explored in
section~\cite{todo}. The primary challenge with this is that LingoDB is a columnar,
in-memory database, so adjustments will be needed. Furthermore, LingoDB does
not support indexes, which can make the benchmarks against PostgreSQL unfair.
Another detail is that LingoDB's newer versions contain a large number of
features and optimisations that is not relevant to us, so to simplify implementation
effort the 2022 version was used from their initial paper.
% TODO: Write about tiger data, or use a different example.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{figures/system_design.drawio.pdf}
	\caption{System design with labels of component sources.}
	\label{fig:overall-system-design}
\end{figure}

LingoDB was integrated into PostgreSQL as seen in figure~\ref{fig:overall-system-design}.
The blue represents PostgreSQL components, with the pipeline on the left being
the whole of PostgreSQL. A query reaches the runtime hooks, which gets analyzed
by a hand-written analyzer for whether the query can be execution, and then parsed.
These hand writte components are annotated in light-peach.
This goes through the code LingoDB created, but with custom runtime hooks and
other small edits, annotated with green. Finally this is compiled into
LLVM IR, which has the runtime hooks for reading from PostgreSQL embedded inside.

In the case that a query fails, the system should still support returning the
results and gracefully roll over to PostgreSQL. This was done by ensuring the
AST parser entrance has a try-catch pattern that routes back to PostgreSQL
even in failures. However, this does not protect from system panics such as
segmentation faults.

The most time-consuming part of this is expected to be the AST Parser section,
because it will be receiving the plan tree with the optimisations from PostgreSQL.
LingoDB was designed to parse the query tree, which would come from the
"Parser" stage in figure~\ref{fig:overall-system-design}. 18 plan nodes and 14
expression nodes were implemented.

The final goal here is to support the TPC-H query set.
To drive this implementation, a test-driven approach was used where PostgreSQL's
\texttt{pg\_regress} module added support for creating SQL queries and defining
expected outputs. With this, a test set of basic queries was created which built
up to TPC-H queries. This allowed progressive node implementation during
development, and a quick way to validate changes are safe.

Node implementation ordering followed the dependency analysis. Foundational
nodes such as the sequential scan and projection are in virtually every query,
while other nodes build on top. By implementing in the dependency order, each new
node could be tested using the previously implemented nodes, and bugs can be
isolated.

\section{Implementation}\label{sec:implementation}

The primary system this project was developed on was a x86\_64 CPU (Ryzen 3600)
and on Ubuntu 25.04. The database was not tested on MacOS or Windows, and this
may lead to issues when installing it independently.

%===============================================================================
\subsection{Integrating LingoDB to PostgreSQL}\label{subsec:integrating-lingodb}
The project was started from \url{https://github.com/mkindahl/pg_extension},
then \texttt{ExecutorRun\_hook} inside of executor.h in PostgreSQL
was used \url{https://doxygen.postgresql.org/executor_8h_source.html} as the
entrance. Within PostgreSQL there are some surrounding steps since the intention
is not usually to replace the entire executor with these hooks, so the memory
context had to be activated and switched into.

Next, the \texttt{QueryDesc} pointer, which contains the query request,
was to be passed through to \texttt{C++}. This causes a design decision. Good practice
here is to use smart pointers to prevent memory leaks, but this object is large
and the source of truth about the request. Furthermore, the memory is handled by
the PostgreSQL memory contexts. It was decided that these objects will remain
as raw pointers, causing the \texttt{C++} to break conventions.

LingoDB was installed as a git submodule and set to a read-only permission.
This was maintained for reference purposes only, and the compilation phases
would be extracted. LingoDB used LLVM 14, and was upgraded to LLVM 20 to
modernise it and slightly better support with |C++20| (some workarounds were
required with LLVM 14 that could be skipped with LLVM 20). However,
since this is the C++ API for LLVM, a large amount of the LingoDB code
had to be adjusted to compile.

%===============================================================================
\subsection{Logging infrastructure}\label{subsec:logging}
PostgreSQL has its own logging infrastructure that routes through its |elog|
command, but it was decided that a two-layer logging infrastructure was required.
The first layer is the level, (|DEBUG|, |IR|, |TRACE|, |WARNING\_LEVEL|,
|ERROR\_LEVEL|, and more), and the second represents which layer of the design
the log is inside of (|AST\_TRANSLATE|, |RELALG\_LOWER|, |DB\_LOWER|, and more).
This meant if the AST translation was being worked on, all the logs in only that
section of the codebase could be enabled. The core benefit of this is that
the logs are lengthy so it becomes easier to navigate.

An issue that was encountered was that the LLVM/MLIR logs would route through
stderr, and this caused difficult to debug issues until the hook was found
to redirect this into |elog| as well. Subsection\~ref{subsec:debugging} will
explore one of the workarounds that was needed at this stage.

Lastly, for error handling mostly |std::runtime\_error| was utilised. This served
as a global way to log the stack trace and roll back to PostgreSQL's execution.
There initially was an implementation of error handling with severity levels
and messages, but the simplicity of a single command that rolled back to
PostgreSQL was more generally useful.

%===============================================================================
\subsection{Debugging Support}\label{subsec:debugging}
An important property of PostgreSQL is that each client connection creates a new
process. This means there are several layers to claw through to debug issues.
First, is the PostgreSQL postmaster, then the client connection, then within
that is the runtime hook entrance, which leads to $C++$, and inside $C++$
we will be compiling into a JIT runtime, and the bugs can happen inside there.
This poses a challenge for how to debug problems such as segmentation faults and
errors without any logging.

% messy paragraph
This was solved with a combination of the regression tests, unit testing, and
a script to connect |gdb| to dump the stack. The regression tests were already
explored, but the unit tests consist of testing anything unconnected to PostgreSQL.
The issue is that this extension creates a |pgx\_lower.so| which is installed
within PostgreSQL, then the PostgreSQL libraries are used from inside there.
This means if we run without being inside of PostgreSQL, no psql libaries can
be used. As a result, unit tests can only test MLIR functions. Most of the
unit tests were highly situational, and are used when a proper interactive GDB
connection was required within the IDE. Furthermore, unit tests allow the |stderr|
to be visible, which assists greatly with MLIR/LLVM errors that go to |stderr|
and nowhere else.

For the stack-dumping, a script was written, |debug-query.sh| which proved to 
be the most useful approach for complex issues. It has the ability to create a 
psql connection, git the process ID of the client connection, then connect GDB,
run a desired query, and dump the stack trace. In this way, the majority of 
errors were tackled.

%===============================================================================
\subsection{Data Types}\label{subsec:data-types}
asdf

%===============================================================================
\subsection{Runtime patterns}\label{subsec:runtime-patterns}
asdf
%===============================================================================
\subsection{AST patterns}\label{subsec:ast-patterns}
asdf
%-------------------------------------------------------------------------------
\subsubsection{Plan translation - Scans}\label{subsubsec:plan-scans}
asdf
%-------------------------------------------------------------------------------
\subsubsection{Plan translation - Aggregations}\label{subsubsec:plan-aggregations}
asdf
%-------------------------------------------------------------------------------
\subsubsection{Plan translation - Joins}\label{subsubsec:plan-joins}
asdf
%-------------------------------------------------------------------------------
\subsubsection{Plan translation - Subqueries}\label{subsubsec:plan-subqueries}
asdf
%-------------------------------------------------------------------------------
\subsubsection{Expression Translation - Variables, Constants, Parameters}\label{subsubsec:expr-vars-consts}
asdf
%-------------------------------------------------------------------------------
\subsubsection{Expression Translation - Nullability}\label{subsubsec:expr-nullability}
asdf
%-------------------------------------------------------------------------------
\subsubsection{Expression Translation - Operators}\label{subsubsec:expr-operators}
asdf

%===============================================================================
\subsection{Configuring JIT compilation settings}\label{subsec:jit-config}
asdf
%===============================================================================
\subsection{Profiling Support}\label{subsec:profiling}
asdf

%===============================================================================
\subsection{Website}\label{subsec:website}
asdf

%===============================================================================
\subsection{Benchmarking}\label{subsec:benchmarking}
asdf