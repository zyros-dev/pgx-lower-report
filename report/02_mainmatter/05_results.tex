\chapter{Results and Discussion}\label{ch:results}
\section{Results}\label{sec:results}

The following results were produced with the method detailed in subsection~\ref{subsec:benchmarking}.
Box plots were stacked on top of the graphs, representing the 5th, 25th, 50th,
75th, and 95th percentiles. Any outliers were marked with a hollow circle, and
if they were inconvenient to show (such as in figure~\ref{fig:bench-mem-diff-plots}),
an arrow annotation is utilised. Matplotlib and Seaborn were used to make these
in Python.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{figures/mine_box_plots.pdf}
	\caption{Overall benchmarking represented with box plots}
	\label{fig:bench-box-plots}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{figures/mine_diff_plots.pdf}
	\caption{Difference in latency benchmarks between PostgreSQL and pgx-lower}
	\label{fig:bench-diff-plots}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{figures/mine_memory_plots.pdf}
	\caption{Peak memory usage of queries}
	\label{fig:bench-mem-plots}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{figures/mine_memory_diffs.pdf}
	\caption{Difference in peak memory usage of queries}
	\label{fig:bench-mem-diff-plots}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{figures/mine_branch_miss_plots.pdf}
	\caption{Branch miss rate}
	\label{fig:branch-miss-plot}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{figures/mine_branches_plots.pdf}
	\caption{Number of branches}
	\label{fig:num-branch-plot}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{figures/mine_llc_miss_plots.pdf}
	\caption{Last-level-cache miss plots}
	\label{fig:llc-miss-plot}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{figures/mine_ipc_plots.pdf}
	\caption{Instructions per (CPU) cycle plot}
	\label{fig:ipc-plot}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.15]{figures/q20-index-on-sf0.16-psql.png}
	\caption{PostgreSQL TPC-H query 20 indexes enabled at SF = 0.16. Runtime: 15 minutes}
	\label{fig:q20-psql-profile}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.15]{figures/q20-pgx-profile.png}
	\caption{pgx-lower TPC-H query 20 indexes enabled at SF = 0.16. Runtime: 1.18 seconds}
	\label{fig:q20-pgx-profile}
\end{figure}

\section{Discussion}\label{sec:Discussion}
To reiterate, the goal is to show that using the extension system is a viable
approach to introduce compiled queries into battle-tested databases while
maintaining their ACID properties. Previous studies have already found that
this model has speed benefits.

In terms of latency, results are mixed.
Figures~\ref{fig:bench-box-plots} and~\ref{fig:bench-diff-plots} show that without indexes at scale factor 0.01,
queries 7 and 20 are orders of magnitude slower than other queries.
This pattern repeats at scale factor 0.16 even with indexes enabled.
In contrast, pgx-lower shows consistent performance across these queries. The reason is visible in
figures~\ref{fig:q20-psql-profile} and~\ref{fig:q20-pgx-profile}: PostgreSQL uses a nested loop join
while pgx-lower uses a hash join.
Appendices~\ref{appendix:q20-sql},~\ref{appendix:q20-postgres}, and~\ref{appendix:q20-pgxlower} show the query and both systems' plan trees. In pgx-lower's output,
joins are annotated with "hash", indicating hash join usage. This comparison
is not entirely fair because pgx-lower uses a hash join while PostgreSQL uses
a nested loop join (NestLoop\_T), which would benefit from indexing.

A notable detail in figure~\ref{fig:q20-pgx-profile} is the minimal time spent in LLVM code generation. Of the 1.18 second total query execution time,
only 14.93 milliseconds is spent in \texttt{llvm::SelectionDAGISel::runOnMachineFunction}, the LLVM IR to machine code conversion step. However, total compilation
time is 236 milliseconds, with most of this spent in MLIR passes. This suggests
MLIR optimization is more expensive than LLVM code generation. One explanation is that the LLVM ORC JIT does minimal work per query and spreads compilation across the execution in the background.

For RAM usage, figures~\ref{fig:bench-mem-plots} and~\ref{fig:bench-mem-diff-plots} show pgx-lower and PostgreSQL
have similar peak memory usage. This is encouraging because it demonstrates that LingoDB's in-memory operations were successfully adapted to PostgreSQL's disk-oriented architecture. Figure~\ref{fig:bench-mem-diff-plots} shows the maximum difference is just over 3 megabytes, with a mean difference of 0.34 megabytes across all queries.

The branch prediction in figure~\ref{fig:branch-miss-plot} shows potential.
At scale factor 1, pgx-lower's median
branch miss rate was 0.16\%, compared to Postgres's 0.28\%. However,
at a smaller scale factor with smaller queries this is inverted such that
PostgreSQL had a median of 0.38\% and pgx-lower had 1.09\%. This means that
the compiled JIT runtime has a better branch prediction rate, but potentially
the code within the compilation stage, or before warm-up, has a worse rate.
The difference could be attributed to the number of branches, visible in
figure~\ref{fig:num-branch-plot}. PostgreSQL had a median of 28,430,465
branches, while pgx-lower had 195,299,021.50; 6.87 times more than PostgreSQL.
This increase in the number of branches could mean that there are more low-hanging fruit.

For the last level cache miss rate, pgx-lower appears to perform better than
PostgreSQL at smaller scale factors and appears similar to PostgreSQL at larger
ones, which can be seen in figure~\ref{fig:llc-miss-plot}. PostgreSQL
has a median 31.20\% miss rate at scale factor 0.01, while pgx-lower has a
median of 6.19\%. When increased to a scale factor of 1, PostgreSQL has a median
of 33.16\%, and pgx-lower has a median of 34.81\%. An explanation for this could
be that the LLVM compiler stage uses its cache much more effectively, but the
actual JIT is approximately the same. This is expected with the current approach,
but this would improve if the JIT stage is permitted to dynamically change
the plan, such as how HyPer does.

\subsection{Test Validity}\label{subsec:test-validity}
Increasing the number of iterations to make the outputs more reliable seems to
be successful, and the variation is not too large. Some queries, such as in
scale factor 0.01 with indexes disabled, in figure~\ref{fig:bench-box-plots},
show the outliers do become extreme. On query 8, PostgreSQL had an outlier
of 5008 milliseconds while the median was 12.42 milliseconds. However, the
coefficient of variation was only 0.44\% overall during the
latency measurements in the scale factor 0.01. It's stable overall,
but vital to do repeated tests to exclude these outliers from the system.

These variations will primarily be caused by PostgreSQL's optimiser containing
genetic algorithms, as mentioned in section~\ref{sec:postgresql-background}. It
can cause plans to change significantly and makes them non-deterministic.
While these tests were done on an isolated docker container in a Linux machine
running minimal processes, system interrupts can also affect the results.

\subsection{Future work}\label{subsec:future-work}
Since replacing PostgreSQL's execution engine with a JIT-focused one is quite
generic, there are a number of directions future work can take. The system
itself can be improved by fully implementing the plan nodes, and it
could be optimised further. However, it's important to ensure the
final product is useful, so other base databases, compilers, languages, and
execution engines that could be integrated can be considered.

To progress this system further and make it a full extension, the plan tree
nodes need to be fully implemented, and the query analyser can be improved.
Only the minimum amount for TPC-H was implemented here, but pg\_bench
and isolationtester should also be used before suggesting this system to
users. This includes implementing indexes, WINDOW functions, the other
22 missing plan nodes, and the missing execution nodes.

The other core work is optimising the system, both with existing research
and clearer ways to use the PostgreSQL API. Namely: pre-compiling
functions in PostgreSQL into LLVM, then inlining them instead of
crossing the LLVM-C++ boundary; adaptive compilation/query planning;
and Umbra's LeanStore-style buffering system. Most of these
optimisations are only applicable inside an
LLVM/MLIR system, though if WebAssembly is used instead, many of
them can be skipped. More generally, parallelism can be improved,
JIT compilation tuning, cross-platform support, subquery
deduplication, and further optimisations.

In databasing and research broadly, it is vital to keep in mind whether the
research is useful and impactful, as per Michael Stonebraker's concern that
the field is merely polishing a round ball~\cite{top-ten-fears}. For
successful projects, the dollar-cost of a live system database is typically
much smaller than the profitability of the project itself, so paying for
higher throughput is commonly not a concern, and better gains for latency
can be made by adding a custom caching mechanism to the service such as Redis.
These complex queries are primarily in OLAP systems, and large-scale OLAP
systems will usually move away from PostgreSQL and into a more scalable
database such as ClickHouse, or Apache HIVE. For this reason, there may be
better systems to develop this architecture on.

PostgreSQL was chosen due to its large popularity and LingoDB was used
because it reasonably matched PostgreSQL's interfaces while being open source.
While PostgreSQL is reasonably good for this approach, and this can solve a
real problem with it, there might be a better database. Most of these dedicated
OLAP systems will already be using a JIT or vectorized approach, though.

During development, using LingoDB provided helpful constraints and made
development easier since it is an established system. However, LingoDB's
columnar, in-memory architecture required extensive modifications to suit
its needs. It also contains a query optimisation engine, which is unnecessary
because PostgreSQL already has a thorough optimisation system. It would
be better to build the engine, or find a better suited base system.
The ideal here would be Umbra based on its description, but it is closed
source. Another potential approach is to take an established OLAP system's
engine (such as ClickHouse), and route to that instead of PostgreSQL's
depending on the analyser's rules.

MLIR was useful to give a strong set of dialect systems, but the main reason
LingoDB used it was to give database optimisations clear layers. Furthermore,
the LLVM/MLIR ecosystem targets ahead of time compilation, or longer-running
JIT systems. While WebAssembly is appealing here because it targets
short-lived processes, we would not be able to inline functions in the future.
Either way, switching to a different compiler, or away from C++ into C or
Rust is appealing.

This establishes a large set of different directions this research can take.
The most appealing direction is attempting to take NoisePage or ClickHouse
and inserting it into PostgreSQL as a drop-in engine replacement.
This is a more complete ecosystem, and the primary work will be around the
adapters to adjust queries. Furthermore, pg\_duckdb has already done
this, but it is a vectorised engine.
