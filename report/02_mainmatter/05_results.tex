\chapter{Results and Discussion}\label{ch:results}
\section{Results}\label{sec:results}

The following results were produced with the method detailed in subsection~\ref{subsec:benchmarking}.
Box plots were stacked on top of the graphs, representing the 5th, 25th, 50th,
75th, and 95th percentiles. Any outliers were marked with a hollow circle, and
if they were inconvenient to show (such as in figure~\ref{fig:bench-mem-diff-plots}),
an arrow annotation is utilised. Matplotlib and Seaborn were used to make these
in Python.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{figures/mine_box_plots.pdf}
	\caption{Overall benchmarking represented with box plots}
	\label{fig:bench-box-plots}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{figures/mine_diff_plots.pdf}
	\caption{Difference in latency benchmarks between PostgreSQL and pgx-lower}
	\label{fig:bench-diff-plots}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{figures/mine_memory_plots.pdf}
	\caption{Peak memory usage of queries}
	\label{fig:bench-mem-plots}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{figures/mine_memory_diffs.pdf}
	\caption{Difference in peak memory usage of queries}
	\label{fig:bench-mem-diff-plots}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{figures/mine_branch_miss_plots.pdf}
	\caption{Branch miss rate}
	\label{fig:branch-miss-plot}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{figures/mine_branches_plots.pdf}
	\caption{Number of branches}
	\label{fig:num-branch-plot}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{figures/mine_llc_miss_plots.pdf}
	\caption{Last-level-cache miss plots}
	\label{fig:llc-miss-plot}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{figures/mine_ipc_plots.pdf}
	\caption{Instructions per (CPU) cycle plot}
	\label{fig:ipc-plot}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.15]{figures/q20-index-on-sf0.16-psql.png}
	\caption{PostgreSQL TPC-H query 20 indexes enabled at SF = 0.16. Runtime: 15 minutes}
	\label{fig:q20-psql-profile}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.15]{figures/q20-pgx-profile.png}
	\caption{pgx-lower TPC-H query 20 indexes enabled at SF = 0.16. Runtime: 1.18 seconds}
	\label{fig:q20-pgx-profile}
\end{figure}

\section{Discussion}\label{sec:Discussion}
To reiterate, the goal is to show that using the extension system is a viable
approach to introduce compiled queries into battle-tested databases while
maintaining their ACID properties. Previous studies have already found that
this model has speed benefits.

In terms of latency for queries, there are mixed results.
In figure~\ref{fig:bench-box-plots} and figure~\ref{fig:bench-diff-plots}
it is visible that if PostgreSQL has indexes disabled, at scale factor 0.01
the latency of queries 7 and 20 is multiple magnitudes longer than the other queries,
and this happens again at scale factor 0.16 even with the indexing enabled.
In the differences plot, we can see that this clearly does not happen with
pgx-lower, and the reason for this is visible in figure~\ref{fig:q20-psql-profile}
and figure~\ref{fig:q20-pgx-profile}. PostgreSQL opted to do a nested join
instead of relying on an index, while pgx-lower opted to build a hash join.
Appendix~\ref{appendix:q20-sql}, \ref{appendix:q20-postgres},
\ref{appendix:q20-pgxlower} show the initial query, PostgreSQL's plan tree, and
pgx-lower's output after the optimisation pass. Within the pgx-lower's output,
the joins have a "hash" annotation which means they go towards the hash join.
Overall, this means that it is not an entirely fair comparison on these
queries because pgx-lower gets to use a hash join while PostgreSQL is utilising
a NestLoop\_T, which would have been indexed with the indexing enabled.

An interesting detail in figure~\ref{fig:q20-pgx-profile} is that
the LLVM to bytecode compiler is very minimal. Out of the 1.18 second query,
only 14.93 milliseconds is spent inside
\texttt{llvm::SelectionDAGISel::runOnMachineFunction}, which is the step that
converts the LLVM IR into executable code. However, the overall compiler is 236
milliseconds, with much of it being inside the MLIR passes themselves. That
means this MLIR code appears to be much heavier than the LLVM optimisations.
One explanation for this is that the LLVM ORC JIT compiler does a minimal amount
of work, and it happens in the background during execution.

For the RAM usage, figure~\ref{fig:bench-mem-plots} and
figure~\ref{fig:bench-mem-diff-plots} show that pgx-lower and PostgreSQL's
peak memory usage is approximately aligned. This is ideal as it means the
in-memory functionality of LingoDB was sufficiently moved to on-disk in
pgx-lower. Figure~\ref{fig:bench-mem-diff-plots} in particular shows
the difference is only a bit over 3 megabytes at most, and the tests showed
there is a mean difference across all these queries of 0.34 megabytes.

The branch prediction in figure~\ref{fig:branch-miss-plot} shows potential.
At scale factor 1, pgx-lower's median
branch miss rate was 0.16\%, compared to Postgres's 0.28\%. However,
at a smaller scale factor with smaller queries this is inverted such that
PostgreSQL had a median of 0.38\% and pgx-lower had 1.09\%. This means that
the compiled JIT runtime has a better branch prediction rate, but potentially
the code within the compilation stage, or before warm-up, has a worse rate.
The difference could be attributed to the number of branches, visible in
figure~\ref{fig:num-branch-plot}. PostgreSQL had a median of 28,430,465
branches, while pgx-lower had 195,299,021.50; 6.87 times more than PostgreSQL.
This increase in the number of branches could mean that there are more low-hanging fruit.

For the last level cache miss rate, pgx-lower appears to perform better than
PostgreSQL at smaller scale factors and appears similar to PostgreSQL at larger
ones, which can be seen in figure~\ref{fig:llc-miss-plot}. PostgreSQL
has a median 31.20\% miss rate at scale factor 0.01, while pgx-lower has a
median of 6.19\%. When increased to a scale factor of 1, PostgreSQL has a median
of 33.16\%, and pgx-lower has a median of 34.81\%. An explanation for this could
be that the LLVM compiler stage uses its cache much more effectively, but the
actual JIT is approximately the same. This is expected with the current approach,
but this would improve if the JIT stage is permitted to dynamically change
the plan, such as how HyPer does.

\subsection{Test Validity}\label{subsec:test-validity}
Increasing the number of iterations to make the outputs more reliable seems to
be successful, and the variation is not too large. Some queries, such as in
scale factor 0.01 with indexes disabled, in figure~\ref{fig:bench-box-plots},
show the outliers do become extreme. On query 8, PostgreSQL had an outlier
of 5008 milliseconds while the median was 12.42 milliseconds. However, the
coefficient of variation was only 0.44\% overall during the
latency measurements in the scale factor 0.01. It's stable overall,
but vital to do repeated tests to exclude these outliers from the system.

These variations will primarily be caused by PostgreSQL's optimiser containing
genetic algorithms, as mentioned in section~\ref{sec:postgresql-background}. It
can cause plans to change significantly and makes them non-deterministic.
While these tests were done on an isolated docker container in a Linux machine
running minimal processes, system interrupts can also affect the results.

\subsection{Future work}\label{subsec:future-work}
Since replacing PostgreSQL's execution engine with a JIT-focused one is quite
generic, there are a number of directions future work can take. The system
itself can be improved by fully implementing the plan nodes, and it
could be optimised further. However, it's important to ensure the
final product is useful, so other base databases, compilers, languages, and
execution engines that could be integrated can be considered.

To progress this system further and make it a full extension, the plan tree
nodes need to be fully implemented, and the query analyser can be improved.
Only the minimum amount for TPC-H was implemented here, but pg\_bench
and isolationtester should also be used before suggesting this system to
users. This includes implementing indexes, WINDOW functions, the other
22 missing plan nodes, and the missing execution nodes.

The other core work is optimising the system, both with existing research
and clearer ways to use the PostgreSQL API. Namely: pre-compiling
functions in PostgreSQL into LLVM, then inlining them instead of
crossing the LLVM-C++ boundary; adaptive compilation/query planning;
and Umbra's LeanStore-style buffering system. Most of these
optimisations are only applicable inside an
LLVM/MLIR system, though if WebAssembly is used instead, many of
them can be skipped. More generally, parallelism can be improved,
JIT compilation tuning, cross-platform support, subquery
deduplication, and further optimisations.

In databasing and research broadly, it is vital to keep in mind whether the
research is useful and impactful, as per Michael Stonebraker's concern that
the field is merely polishing a round ball~\cite{top-ten-fears}. For
successful projects, the dollar-cost of a live system database is typically
much smaller than the profitability of the project itself, so paying for
higher throughput is commonly not a concern, and better gains for latency
can be made by adding a custom caching mechanism to the service such as Redis.
These complex queries are primarily in OLAP systems, and large-scale OLAP
systems will usually move away from PostgreSQL and into a more scalable
database such as ClickHouse, or Apache HIVE. For this reason, there may be
better systems to develop this architecture on.

PostgreSQL was chosen due to its large popularity and LingoDB was used
because it reasonably matched PostgreSQL's interfaces while being open source.
While PostgreSQL is reasonably good for this approach, and this can solve a
real problem with it, there might be a better database. Most of these dedicated
OLAP systems will already be using a JIT or vectorized approach, though.

During development, using LingoDB provided helpful constraints and made
development easier since it is an established system. However, LingoDB's
columnar, in-memory architecture required extensive modifications to suit
its needs. It also contains a query optimisation engine, which is unnecessary
because PostgreSQL already has a thorough optimisation system. It would
be better to build the engine, or find a better suited base system.
The ideal here would be Umbra based on its description, but it is closed
source. Another potential approach is to take an established OLAP system's
engine (such as ClickHouse), and route to that instead of PostgreSQL's
depending on the analyser's rules.

MLIR was useful to give a strong set of dialect systems, but the main reason
LingoDB used it was to give database optimisations clear layers. Furthermore,
the LLVM/MLIR ecosystem targets ahead of time compilation, or longer-running
JIT systems. While WebAssembly is appealing here because it targets
short-lived processes, we would not be able to inline functions in the future.
Either way, switching to a different compiler, or away from C++ into C or
Rust is appealing.

This establishes a large set of different directions this research can take.
The most appealing direction is attempting to take NoisePage or ClickHouse
and inserting it into PostgreSQL as a drop-in engine replacement.
This is a more complete ecosystem, and the primary work will be around the
adapters to adjust queries. Furthermore, pg\_duckdb has already done
this, but it is a vectorised engine.
