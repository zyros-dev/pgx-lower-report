\chapter{Preliminary Results and Preparation}\label{ch:prelim}

Extending PostgreSQL with MLIR is a large task. As a matter of fact, as mentioned in Section~\ref{sec:postgres},
compiling entire query expressions as been debated before in PostgreSQL discussions, and the replies were
that the author will probably not do it. An issue is that showing the necessary skills to do this would take
long - PostgreSQL's codebase is in the millions of lines, and MLIR tutorials are lengthy. Furthermore,
there are no significant design choices to be made as this is almost a minimum viable product.

Specialised courses have been taken: COMP4403 at the University of Queensland on Compilers and Interpreters,
as well as University of New South Wales' COMP9315, database implementations, where PostgreSQL itself is
extended in coursework, and COMP6771, Advanced C++. Further, two years of experience working in 
a market maker Java and tuning its JIT compiler have been accumulated with over 80,000 
lines of code written there.

This chapter is split into making an easily reproducible way to benchmark databases with Docker in Section~\ref{sec:bench},
and then the potential for improvement inside of PostgreSQL is measured with \verb|prod| in Section~\ref{sec:potential}. Finally,
a section of LingoDB is walked through at a very high level to understand its architecture in Section~\ref{sec:lingo-walk}

% \section{LingoDB MLIR Walkthrough}\label{sec:lingdb-mlir-walk}

\section{Benchmarking reviewed databases}\label{sec:bench}

Recreating benchmarks from the reviewed studies makes it possible to directly compare the
final results of the thesis. It provides a reference point for assessing the impact of 
changes made during the project, and demonstrates technical competence in producing these.
Furthermore, many of these papers have not been independently validated, so reproducing their
results serves as a confirmation of their findings.

An important requirement for this is that the results can be moved between systems,
so Docker was chosen to containerise the applications. In many contexts, unless the user
is a medium-sized corporation they will be using a form of containerisation while cloud hosting 
their database \cite{ddia}. Additionally, if everything has the overhead of containerisation
then the results will stay fair between them. 

TPC-H is a widely used benchmark on databases and was chosen \cite{tpch}. The mutable repository has a solid 
foundation for tools that support benchmarking \cite{mutable-1}, so that was used to produce
one docker container that supports HyPer, mutable, DuckDB, and PostgeSQL. LingoDB also had developed
TPC-H tooling and was put into a separate docker container. Once the docker containers were created
on a local machine, they were uploaded to a server on DigitalOcean with 128GB of RAM and 16 CPUs. 
Running \verb|lscpu| showed it was a INTEL(R) XEON(R) GOLD 6548N. Then they were run
with a scale factor of one (the database is approximately one gigabyte) with five repetitions of each 
query and put into Figure~\ref{fig:created-benchmark-results}.

The results of these benchmarks in Figure~\ref{fig:created-benchmark-results} show that Postgres is
significantly slower than the rest, and with Postgres removed from the graph HyPer and DuckDB with
multiple cores are the fastest, and DuckDB is the slowest. The others contend for positions depending 
on the query.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{figures/benchmarks.png}
    \caption{Benchmarking results.}
    \label{fig:created-benchmark-results}
\end{figure}

The primary difficulties while building this was installing all the necessary libraries, as well as
compiling things correctly. It was a significant time sink to prepare these containers. The Dockerfiles are 
available at \url{https://github.com/zelestis/benchmarking-dockers}.

\section{Analysing how much of an impact JIT could have}\label{sec:potential}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{figures/pperf_psql.png}
    \caption{PostgreSQL time spent in the CPU, measured with prof.}
    \label{fig:pprof-psql}
\end{figure}

A key requirement is there must be a visible amount of impact to improve PostgreSQL by. As such, the same 
set of queries as Section~\ref{sec:bench} were chosen to see how much of the time is spent on reading
from secondary storage. The test was conducted on a local computer with a Samsung MZVLB256HAHQ-000L7 
NVMe, 16 GB of RAM and an Intel i5-6500T. While running the tests from ~\ref{sec:bench}, \verb|perf|
was attached to the query and this data was given to \verb|prof| \cite{perf_man_page}. With the 
visualisations that \verb|prof| produces, the key sections were identified and turned into a graph 
in Figure~\ref{fig:pprof-psql}.


As seen in Figure~\ref{fig:pprof-psql}, the CPU time for PostgreSQL varied from between 34.87\% and 
76.56\% of the query with an average of 49.32\%. The rest of the functions contribute to reading from I/O,
and represent the profiler measuring samples inside the function (not cumulative of child methods). These methods 
were identified by observing the graph made by \verb|prof| and finding all the child methods. The potential of JIT
is to improve that CPU time by several factors, and we can expect a ceiling of improvement of between 2-3x in 
the case the CPU time reduces to 0\% as a result of JIT. 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/perf-stats.png}
    \caption{PostgreSQL query profiling statistics collected with prof.}
    \label{fig:prof-psql-2}
\end{figure}

In Figure~\ref{fig:prof-psql-2}, we can see these queries had a worst case of 0.69\% of branch predictions missed,
and at worst a 43.02\% of LLC loads are missed. A 0.69\% could have room for improvement, as mentioned in
Section~\ref{sec:os-feats}, systems can have a 99\% branch prediction accuracy but it's still a significant bottleneck.
LLC loads are how many last-level-caches get loaded, and a miss rate of 43.02\% means the cache could be significantly
utilised. 

\section{LingoDB Codebase Exploration}\label{sec:lingo-walk}

This section is dedicated to exploring the LingoDB codebase at a high level for how it turns a query
into machine code. Direct files with key points were explored, as visible in Figure~\ref{fig:lingodb-walk}. 
The code is accessible at \url{https://github.com/lingo-db/lingo-db} \cite{lingo-db-1}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.38]{figures/lingo-db-walkthrough.png}
    \caption{LingoDB sql-to-mlir.cpp high level stages.}
    \label{fig:lingodb-walk}
\end{figure}

The process begins with the main method inside \verb|sql-to-mlir.cpp|. This creates an MLIR
DialectRegistry, and adds all the dialects they have created to it. Following this, they parse
the SQL expression with their Parser.cpp, which uses PostgreSQL libraries inside \verb|parse_expr.h|
and \verb|parse_node.h|. This brings an abstract syntax tree for them to use back up to \verb|sql-to-mlir.cpp|, 
and a \verb|mlir::OpBuilder| is used in a loop to translate the query blocks through the dialects. At the end,
they can use the produced result to produce LLVM IR.

