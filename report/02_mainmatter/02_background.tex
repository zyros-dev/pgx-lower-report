\chapter{Background}\label{ch:background}

%===============================================================================
\section{Database Background}\label{sec:database_background}
%-------------------------------------------------------------------------------
Most databases follow the structure shown in Figure~\ref{fig:database_structure}.
Database systems parse \emph{Structured Query Language} (SQL) into \emph{relational algebra} (RA),
optimise it, execute it, and materialise the results into a table \cite{database-concepts}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/database_structure.png}
	\caption{Database Structure.}
	\label{fig:database_structure}
	\cite{database-concepts}
\end{figure}

Non-compiler databases use a \emph{volcano operator model tree}, such as
Figure~\ref{fig:execution_tree} \cite{long-masters-thesis}. A \texttt{produce()} function at the root node
calls its children's \texttt{produce()}, until it calls a leaf node,
which calls \texttt{consume()} on itself, then that calls its parent's
\texttt{consume()} function. In other words, a post-order traversal through
the tree where tuples are dragged upwards.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/execution-tree.png}
	\caption{Volcano operator model tree.}
	\label{fig:execution_tree}
	\cite{long-masters-thesis}
\end{figure}

Classical models suffer a fundamental issue: heavy under-utilisation of hardware
\cite{branch-misprediction}. If only a single tuple is pulled at a time, the CPU
caches are barely used. An i5-570, a popular CPU in 2010 had an 8\,MB L3 cache,
but in 2024 an i5-14600K has a 24\,MB L3 cache \cite{passmark_i5760},
\cite{techpowerup_i5_14600k}. For disks, in 2010 the Seagate Barracuda 7200.12
was popular, which had a sustained read of 138\,MB/s, but in 2022 the Samsung
V-NAND SSD 990 PRO released with a sustained read of 7450\,MB/s
\cite{seagate_barracuda_7200_12,samsung_990_pro}. Such dramatic increases
could mean the algorithms can fundamentally change.

These observations led to the \emph{vectorized} and \emph{compiled execution models}. The vectorised model pulls multiple tuples
up in a group rather than one at a time. A core advantage
is that \emph{instructions per CPU cycle} (IPC) can increase through \emph{single
	instruction, multiple data} (SIMD) operations \cite{everything-vector}. However,
this can cause deep copy operations to be required, or more disk spillage
than necessary \cite{sequntial-execution}. For instance, if a sort or a join allocates new space
that is too large for the cache, the handling can become poor.
Section~\ref{sec:jit_background} and Chapter~\ref{ch:related-work} explore compilation approaches.

Relational databases prioritise ACID requirements - Atomicity, Consistency,
Isolation and Durability \cite{database-concepts}. As a critical requirement in database systems,
ACID compliance is usually one of the main reasons people choose relational
databases \cite{ddia}. Atomicity is defined as transactions are a single unit
of work, while consistency means the database must be in a valid state
before and after the
query. Isolation means concurrent transactions do not interact with each other,
and durability is defined as once something is committed it will stay committed.
\cite{database-concepts}

Most of the recent databasing research has focused on the optimiser,
which is visible in Figure~\ref{fig:database_structure}. Common optimisation techniques include
reordering join statements (flipping the left and right sides), and
predicate push down, where a conditional/filter on a node is moved
onto a lower node \cite{inersjo2021}. Additionally, extracting common subexpressions
prevents recomputation, while constant folding evaluates constant operations inside
the optimiser rather than at runtime. Despite these advances, query optimisers frequently
produce suboptimal plans \cite{optimizer-bad-1,optimizer-bad-2}. Another essential pattern involves genetic algorithms
being used in optimisers for determining things like join orders, which can
cause the outputted plan to be non-deterministic \cite{utesch1997geqo}.
A thesis could be written just to summarise the list of optimisations.

Within traditional and volcano databases, the cache is managed through buffer techniques.
Fixed-size pages (such as eight kilobytes) are read and loaded into
a \emph{buffer pool} object which holds them in RAM. Memory placement in L1/L2/L3 and RAM caches depends on access
patterns, and is managed by the operating system or environment
\cite{database-concepts}. Buffer pools employ different caching strategies
(least-recently-used, most-recently-used, etc.)
based on the situation \cite{effelsberg1984buffer}. Cache effectiveness can be
measured by \emph{last level cache hit rate} (LLC), which represents
how many instructions were served from the CPU cache \cite{llc_intel}.

Databases are commonly split into \emph{Online Transaction Processing} (OLTP) and
\emph{Online Analytical Processing} (OLAP). OLTP focuses on
supporting atomicity, running multiple queries at once, and typically
handles the work profile of an online service that frequently performs key-value lookups.
On the other hand, OLAP databases focus on analytical work profiles
where aggregations are requested or operations span large
chunks of the database \cite{ddia}. PostgreSQL implements
a hybrid architecture supporting both OLTP and OLAP operations
\cite{herrera2021hybrid}. Debate continues about whether
such hybrid designs remain useful, as load pressure
on user-serving databases commonly causes reliability issues
\cite{mooncake_htap}.

The layout of database tables within memory is either columnar or tuple/
row-oriented \cite{ddia}. In a columnar database, a table is stored
as a set of arrays or
lists so that when iterated over, only specific columns need to be iterated.
This can improve the compression ratio, and horizontal tables can ignore
a large number of columns. On the other hand, row-based means that entire-table
iteration requires less round-trips and could have better caching in contexts
such as joins \cite{ddia}. It is common to see columnar databases in OLAP
systems and row-oriented in OLTP \cite{ddia}.

%-------------------------------------------------------------------------------
\section{JIT Background}\label{sec:jit_background}
%-------------------------------------------------------------------------------
\emph{Just-in-time} (JIT) compilers work with multiple layers of compilation such
as raw interpretation of bytecode, unoptimised machine code, and optimised
machine code. They are primarily used in interpreted languages to improve
performance \cite{long-masters-thesis}. Advanced JIT compilers can run the
primary program while background threads improve code optimisation
and swap in the optimised version when ready \cite{hyper-compiler-2}.
Such multi-stage approaches
provide faster initial compilation and faster development cycles.

Due to branch-prediction optimisation, JIT compilers can be faster than
ahead of time compilers. In 1999, a benchmarking paper measured four
commercial databases and found $10\%-20\%$ of their execution time was spent
fixing poor predictions \cite{branch-misprediction}.
Modern measurements still find $50\%$ of their query times are spent resolving
hardware hazards, such as mispredictions, with improvements in this area
making their queries $2.6\times$ faster \cite{dbms-branches}. Azul's JIT compiler
measurements also show that speculative optimisations in JIT can lead
to $50\%$ performance gains \cite{azul_jit_aot}.

Defining good branch prediction is difficult. A reasonable baseline is that a
$1\%$ misprediction rate is too high and should be optimised in low latency environments
\cite{farrier2025branch}. Such baselines lack formality and rest on empirical knowledge
\cite{branch-prediction-values}. Depending on the CPU, a branch mispredict can cost between 10 and
35 CPU cycles, with a typical range being 14-25 cycles \cite{eyerman2006branch}.
With 1 branch every 10 instructions
and a $5\%$ misprediction rate, a 20\,cycle penalty per misprediction translates to approximately
$10\%$ of runtime spent resolving mispredictions \cite{eyerman2006branch}.

In the context of databases, compilers fall into two categories: those that compile only
\emph{expressions} (typically called EXP), and those that compile the
entire \emph{Query Execution Plan} (QEP) \cite{empirical-analysis}. Within PostgreSQL
itself, they have EXP support using \texttt{llvm-jit}, but QEP is not supported.
Chapter~\ref{ch:related-work} examines a variety of research databases.

\section{LLVM and MLIR}\label{sec:llvm-mlir}
The \emph{LLVM Project} is a compiler infrastructure that eliminates the need to
re-implement common compiler optimisations, making it easier to build
compilers~\cite{llvm-paper}. LLVM does not stand for anything in particular,
and \emph{Multi-Level Intermediate Representation} (MLIR) is another, newer
toolkit that is
tightly coupled with the LLVM project~\cite{mlir-paper}. It provides a
framework to define custom dialects and progressively \emph{lower} them to
machine code. Developers can define high-level dialects that others can target
and build upon.

LLVM defines a language-independent \emph{intermediate representation}
(IR) based on \emph{Static Single Assignment} (SSA) form, while MLIR extends
this concept \cite{llvm-paper}. SSA is a common layout for IRs where each
variable is assigned once and immutable; thus the name.
The architecture follows a three-phase design: a front-end parses source code
and generates LLVM IR, an optimiser applies a series of transformation
passes to improve code quality, and a back-end generates machine code for
the target architecture. MLIR extends this concept by introducing a
flexible dialect system that
enables progressive lowering through multiple levels of
abstraction~\cite{mlir-paper}. This addresses software fragmentation
in the compiler ecosystem, where projects were creating incompatible high-level
IRs in front of LLVM, and improves compilation for
heterogeneous hardware by allowing target-specific optimisations
at appropriate abstraction levels.

LLVM's \emph{On-Request-Compilation} (ORC) JIT is a system for
building JIT compilers with support for lazy compilation,
concurrent compilation, and runtime optimisation~\cite{llvm-jit-orc}.
ORC can compile code on-demand as it is needed, reducing startup
time by deferring compilation of functions until they are first
called. The JIT supports concurrent compilation across
multiple threads and provides built-in dependency tracking
to ensure code safety during parallel execution. This makes ORC particularly
suitable for dynamic language implementations, REPLs
(Read-Eval-Print Loops), and high-performance JIT compilers.

\section{WebAssembly and others}\label{sec:webassembly}
The \emph{V8} compiler used for WebAssembly has a unique architecture because
it targets short-lived programs. Majority of JIT applications
are used for long-running services, but this is used for web pages which are
opened and closed frequently. To mitigate this, they have a two-phase
architecture where code is first compiled with \emph{Liftoff} for a quick
startup, then hot functions are recompiled with \emph{TurboFan} \cite{wasm}.
Liftoff aims to create machine code as fast as possible and skip optimisations.

LLVM provides a WebAssembly backend that enables compilation of C and C++ code 
to WebAssembly. The compilation process uses \texttt{clang} to make LLVM IR, which is then 
processed by the WebAssembly backend to produce WebAssembly object files.
This means C/C++ functions from existing codebases (such as PostgreSQL) can be 
pre-compiled to LLVM IR, inlined into WebAssembly, and then lowered 
to WebAssembly bytecode \cite{emscripten_llvm_wasm}.

Other common JIT compilers are the \emph{Java Virtual Machine} (JVM), SpiderMonkey
(Mozilla Firefox's JIT), JavaScriptCore/Nitro (Safari/Webkit), PyPy, various
python JIT compilers, LuaJIT for Lua, HHVM for PHP, Rubinius for Ruby,
RyuJIT for C\#, and more \cite{jit_attacks_survey}. The JVM has also been used for compiled
query execution engines \cite{jamdb}.

\section{PostgreSQL Background}\label{sec:postgresql-background}
PostgreSQL relies on \emph{memory contexts}, which are an extension of \emph{arena
	allocators}. An arena allocator is a data structure that supports allocating
memory and freeing
the entire data structure. This improves memory safety by consolidating
allocations into a single location. A memory context can create child contexts,
and when a context is freed it also frees all the children of this context,
making this a tree of arena allocators. There is a set of statically defined
memory contexts: TopMemoryContext, TopTransactionContext, CurTransactionContext,
TransactionContext, which are managed through PostgreSQL's \emph{Server Programming
	Interface} (SPI) \cite{postgresql_spi_memory}.

PostgreSQL defines \emph{query trees}, \emph{plan trees}, \emph{plan nodes}, and \emph{expression nodes}.
A query tree is the initial version of the parsed SQL, which is passed through
the optimiser, and the output of that is called a plan tree. The nodes in
these plan trees can broadly be identified as plan nodes or expression nodes.
Plan nodes include an implementation detail (aggregation, scanning a table,
nest loop joins) and expression nodes consist of individual operations (binaryop, null test, case expressions) \cite{postgresql_querytree}.

PostgreSQL provides the \texttt{EXPLAIN} command to inspect query execution
plans, which is essential for understanding and optimising query performance
\cite{postgresql_explain}. This command displays the execution plan that the
planner generates, including cost estimates and optional execution statistics,
making it a useful tool for database optimisation and analysis.

\section{Database Benchmarking}\label{sec:database-benchmarking}
% cite pg_bench
Benchmarking a database is difficult because of the variety of
workloads. Many systems create their own benchmarking libraries,
such as \texttt{pg\_bench} by PostgreSQL \cite{postgresql_pgbench} or
LinkBench \cite{linkbench} by Facebook, but in academics the
more common benchmarks are from
the Transaction Processing Council, which is a group that defines benchmarks
\cite{tpch_analyzed}.  Over the years they have made TPC-C for an order-entry
environment, TPC-E, for a broker firm's operations, TPC-DS for a decision
support benchmark. TPC-H is the most common in research,
where the H informally means ``hybrid''. It has a mix of analytical and
transactional elements inside it \cite{tpch_analyzed}.

When evaluating benchmark results, the \emph{coefficient of variation} (CV) is used.
This is a standardised measure of dispersion that expresses the standard deviation
as a percentage of the mean~\cite{stats-libretexts-cv}. It is calculated as
$CV = \frac{s}{\bar{X}} \cdot 100$, where $s$ is the sample standard deviation
and $\bar{X}$ is the sample mean. The coefficient of variation is particularly
useful when comparing datasets with different units or scales, providing a unit-free
measure for variation.
