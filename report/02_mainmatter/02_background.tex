\chapter{Background}\label{ch:background}

%===============================================================================
\section{Database Background}\label{sec:database_background}
%-------------------------------------------------------------------------------
The majority of databases are structured like Figure~\ref{fig:database_structure}.
Structure Query Language (SQL) is parsed, turned to RA (relational-algebra),
optimized, executed, then materialized into a table \cite{database-concepts}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/database_structure.png}
	\caption{Database Structure}
	\label{fig:database_structure}
	\cite{database-concepts}
\end{figure}

For non-compiler databases they use a volcano operator model tree, such as
Figure~\ref{fig:execution_tree} \cite{long-masters-thesis}. The root node has a \texttt{produce()} function
which calls its children's \texttt{produce()}, until it calls a leave node,
which calls \texttt{consume()} on itself, then that calls its parent's
\texttt{consume()} function. In other words, a post-order traversal through
this tree where tuples are dragged upwards.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/execution-tree.png}
	\caption{Volcano operator model tree.}
	\label{fig:execution_tree}
	\cite{long-masters-thesis}
\end{figure}

The fundamental issue with this classical model is that it is heavily
under-utilising the hardware \cite{branch-misprediction}. If only a single tuple is pulled, our CPU
caches are barely used. An i5-570, a popular CPU in 2010 had a 8MB L3 cache,
but in 2024 an i5-14600K has a 24MB L3 cache \cite{passmark_i5760},
\cite{techpowerup_i5_14600k}. For disks, in 2010 the Segate Barracuda 7200.12
was popular, which had a sustained read of 138MB/s, but in 2022 the Samsung
V-NAND SSD 990 PRO released with a sustained read of 7450MB/s
\cite{seagate_barracuda_7200_12} \cite{samsung_990_pro}. Increases this
large could mean the algorithms can fundamentally change.

This has lead to the vectorized execution model and
the compiled model. With the vectorized model, multiple tuples are pulled
up in a group rather than one at a time. A core advantage
is that instructions per CPU cycle (IPC) can increase through single
instruction, multiple data (SIMD) operations \cite{everything-vector}. However,
this can cause deep copy operations to be required, or more disk spillage
than necessary. For instance, if a sort or a join allocates new space
that is too much for the cache, the handling can become poor.
The alternative approach, compilation, will be explored in
section~\ref{sec:jit_background} and chapter~\ref{ch:related-work}.

Relational databases prioritise ACID requirements - Atomicity, Consistency,
Isolation and Durability \cite{database-concepts}. This is a critical
requirement in this type of system, and usually one of the main reasons people
pick a relational database. Atomicity refers to transactions are a single unit
of work, consistency means it must be in a valid state before and after the
query, isolation means concurrent transactions do not interact with each other,
and durability means once something is committed it will stay committed.
\cite{database-concepts}

It is common for on-disk databases to consider the cost of CPU operations to
be constant or cost-free \cite{database-concepts}. This is partially
due to when these systems were made, the disks were much slower and
the caches were much smaller. In part A of this project, this was disproved
for PostgreSQL as it was found that the time spent in the CPU was
substantial: between $34.87\%$ and $76.56\%$ with an average of
$49.32\%$ across the tested queries.

Arguably most of the recent databasing research has been inside the optimiser
which is visible in figure~\ref{fig:database_structure}. This includes
reordering join statements where the left and right sides are flipped,
predicate pushdown where a conditional/filter on a node is moved
onto a lower node, extracting common subexpressions to prevent recomputation,
constant folding where constant operations are evaluated inside
the optimiser rather than in runtime, and more \cite{inersjo2021}. A list of
optimisations would be long enough for a thesis to be written to summarise them.

Within traditional and volcano databases, the cache is managed through a set
of buffer techniques while reading tuples. This works by the database
reading a page, which usually has a fixed size and can be configured,
such as 8KB. This is loaded into a buffer pool object which holds it inside
of RAM. The operating system or environment manages where this memory goes
inside the context of the L1/L2/L3 and ram caches depending on the access
patterns \cite{database-concepts}. This buffer pool can change caching
strategies depending on the
context, such as last-recently-used, most-recently-used, and more, which
can be decided in the optimiser \cite{effelsberg1984buffer}. The effectiveness
of this can be measured with last level cache hit rate measurements (LLC)
which represents how many of the instructions were resolved inside the CPU
cache \cite{llc_intel}.

Databases are commonly split into Online Transaction Processing (OLTP) and
Online Analytical Processing (OLAP). OLTP has a focus on
supporting atomicity, running multiple queries at once, and typically
supports the work profile of an online service that does key-value lookups
frequently. On the other hand, OLAP databases focus on analytical work profiles
where an aggregation is requested, or some operation that spans a large
chunk of the database \cite{ddia}. OLAP systems can be highly distributed,
such as Apache Hive such that a large amount of compute can be used across
the system \cite{apache_hive}. In the context of PostgreSQL, it is a
hybrid architecture that has support for both these operations
\cite{herrera2021hybrid}. There is currently debate about whether
this hybrid architecture is useful anymore because putting pressure
on your database serving users commonly causes reliability issues
\cite{mooncake_htap}.

%-------------------------------------------------------------------------------
\section{JIT Background}\label{sec:jit_background}
%-------------------------------------------------------------------------------
Just-in-time (JIT) compilers work with multiple layers of compilation such
as raw interpretation of bytecode, unoptimized machine code, and optimized
machine code. They are mostly used by interpreted languages to eliminate
the ill-effects on performance \cite{long-masters-thesis}. Advanced compilers
can run the primary program, then dedicate some background threads to
improving the optimisation of the code, and swap it over to the optimized
version when it is ready \cite{hyper-compiler-2}. This means the intial
compilation can be faster, and the development cycle can go faster, as well as
other benefits.

Due to branch-prediction optimization, JIT compilers can be faster than
ahead of time compilers. In 1999, a benchmarking paper measured four
commercial databases and found $10\%-20\%$ of their execution time was spent
fixing poor predictions \cite{branch-misprediction}.
Modern measurements still find 50\% of their query times are spent resolving
hardware hazards, such as mispredictions, with improvements in this area
making their queries 2.6x faster \cite{dbms-branches}. The Azul JIT compiler
measured that their JIT solution's speculative optimizations can lead up to $50\%$ performance gains \cite{azul_jit_aot}.

It is difficult to evaluate what a good branch prediction value is,
but a reasonable baseline is $1\%$ is too high and should be optimised in
a low latency environment \cite{farrier2025branch}. This is not a formal
definition, and is more based on tribal knowledge.  Depending on the
CPU, a branch mispredict can cost between 10 and 35 CPU cycles, with a
safe interval being 14-25. Meaning, if there is
1 branch every 10 instruction, with a $5\%$ misprediction rate
and a 20 cycle penalty per misprediction, $10\%$ of the runtime
will be spent fixing mispredictions \cite{eyerman2006branch}.

In the context of databases, most compilers can be split into only compiling
expressions (typically called EXP for expression), and others that compile the
entire Query Execution Plan (QEP) \cite{empirical-analysis}. Within PostgreSQL
itself, they have EXP support using $llvm-jit$. A variety of research databases
will be examined in chapter~\ref{ch:related-work}.

\section{LLVM and MLIR}\label{sec:llvm-mlir}
The LLVM Project is a compiler infrastructure that supports creating
compilers so that common, but complex, compiler optimisations do not have
to be re- implemented~\cite{llvm-paper}. Multi-Level
Intermediate Representation (MLIR) is another, newer toolkit that is
tightly coupled with the LLVM project~\cite{mlir-paper}. It adds a
framework to define dialects, and lower through these dialects into machine code. One of the primary benefits of this is if you make
a compiler, you can define a high level dialect, then another person
can target your custom high-level dialect.

LLVM defines a language-independent intermediate representation
(IR) based on Static Single Assignment (SSA) form and MLIR is an extension of
this \cite{llvm-paper}. The architecture follows a three-phase design: a
front-end parses source code
and generates LLVM IR, an optimizer applies a series of transformation
passes to improve code quality, and a back-end generates machine code for
the target architecture. MLIR extends this concept by introducing a
flexible dialect system that
enables progressive lowering through multiple levels of
abstraction~\cite{mlir-paper}. This addresses software fragmentation
in the compiler ecosystem, where projects were creating incompatible high
-level IRs in front of LLVM, and improves compilation for
heterogeneous hardware by allowing target-specific optimisations
at appropriate abstraction levels.

LLVM's On-Request-Compilation (ORC) JIT is a system for
building JIT compilers with support for lazy compilation,
concurrent compilation, and runtime optimization~\cite{llvm-jit-orc}.
ORC can compile code on-demand as it is needed, reducing startup
time by deferring compilation of functions until they are first
called. The JIT supports concurrent compilation across
multiple threads and provides built-in dependency tracking
to ensure code safety during parallel execution. This makes ORC particularly
suitable for dynamic language implementations, REPLs
(Read-Eval-Print Loops), and high-performance JIT compilers.

\section{WebAssembly and others}\label{sec:webassembly}
% Also, a paragraph on what is WASM, why is wasm special
The V8 compiler used for WebAssembly has a unique architecture because
it targets short-lived programs. Majority of JIT applications
are used for long-running services, but this is used for web pages which are
opened and closed frequently. To mitigate this, they have a two-phase
architecture where code is first compiled with Liftoff for a quick startup,
then hot functions are recompiled with TurboFan \cite{wasm}. Liftoff aims
to create machine code as fast as possible and skip optimisations.

Other common JIT compilers are the Java Virtual Machine (JVM), SpiderMonkey
(Mozilla Firefox's JIT), JavaScriptCore/Nitro (Safari/Webkit), PyPy, various
python JIT compilers, LuaJIT for Lua, HHVM for PHP, Rubinius for Ruby,
RyuJIT for $C\#$, and more \cite{jit_attacks_survey}. These all target
different work profiles.

\section{PostgreSQL Background}\label{sec:postgresql-background}
PostgreSQL relies on memory contexts, which are an extension of arena
allocators. An arena allocator is a data structure that supports allocating
memory and freeing
the entire data structure. This improves memory safety by consolidating
allocations into a single location. A memory context can create child contexts,
and when a context is freed it also frees all the children of this context,
turning it into a tree of arena allocators. There is a set of statically defined
memory contexts: TopMemoryContext, TopTransactionContext, CurTransactionContext,
TransactionContext, which are managed through PostgreSQL's Server Programming
Interface (SPI) \cite{postgresql_spi_memory}.

PostgreSQL defines query trees, plan trees, plan nodes, and expression nodes.
A query tree is the initial version of the parsed SQL, which is passed through
the optimiser which is then called a plan tree. These stages are visible in
figure~\ref{fig:database_structure}. The nodes in these plan trees
can broadly be identified as plan nodes or expression nodes.
Plan nodes include an implementation detail (aggregation, scanning a table,
nest loop joins) and expression nodes consist of individual operations (binaryop, null test, case expressions) \cite{postgresql_querytree}.

PostgreSQL provides the \texttt{EXPLAIN} command to inspect query execution
plans, which is essential for understanding and optimizing query performance
\cite{postgresql_explain}. This command displays the execution plan that the
planner generates, including cost estimates and optional execution statistics,
making it a critical tool for database optimization and analysis.

\section{Database Benchmarking}\label{sec:database-benchmarking}
% cite pg_bench
Benchmarking a database is a difficult task because there is a variety of
workloads. Many systems create their own benchmarking libraries,
such as $pg_bench$ by PostgreSQL \cite{postgresql_pgbench} or
LinkBench \cite{linkbench} by Facebook, but in academics the
more common benchmarks are from
the Transaction Processing Council, which is a group that defines benchmarks
\cite{tpch_analyzed}.  Over the years they have made TPC-C for an order-entry
environment, TPC-E, for a broker firm's operations, TPC-DS for a decision
support benchmark. The most common one in research appears to be TPC-H,
where the H informally means "hybrid". It has a mix of analytical and
transactional elements inside it \cite{tpch_analyzed}.
