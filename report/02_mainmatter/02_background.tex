% TODO
% 1. explain query optimisations more and what some common optimisations are
% 2. What's OLTP vs OLAP? 
% 3. buffer tables, paging and typical techniques for improving db latency
% 4. vectorisation deep dive; how does SIMD operate and help
% 5. hardware evolution over time for disk-read speeds


\chapter{Background}\label{ch:background}

%================================================================================ 
\section{Fundamentals}\label{sec:fundamentals}
%================================================================================ 
%-------------------------------------------------------------------------------- 
\subsection{Database Background}\label{subsec:database_background}
%-------------------------------------------------------------------------------- 
The majority of databases are structured like Figure~\ref{fig:database_structure}.
Structure Query Language (SQL) is parsed, turned to RA (relational-algebra),
optimized, executed, then materialized into a table.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/database_structure.png}
	\caption{Database Structure}
	\label{fig:database_structure}
	\cite{database-concepts}
\end{figure}

For non-compiler databases they use a volcano operator model tree, such as
Figure~\ref{fig:execution_tree}The root node has a \texttt{produce()} function
which calls its children's \texttt{produce()}, until it calls a leave node,
which calls \texttt{consume()} on itself, then that calls its parent's
\texttt{consume()} function. In other words, a post-order traversal through
this tree where tuples are dragged upwards.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/execution-tree.png}
	\caption{Volcano operator model tree.}
	\label{fig:execution_tree}
	\cite{long-masters-thesis}
\end{figure}

The fundamental issue with this classical model is that it is heavily
under utilising the hardware. If we are only pulling up a single tuple, our CPU
caches are barely used. This has lead to the vectorized execution model and
the compiled model. With the vectorized model, we pull up groups of tuples
instead. However, this leads to problems where it's common to have too many
copy operations instead of having a pointer going upwards. For instance, if a
sort or a join allocates new space that is too much for the cache, the
handling can become poor. With the compiled approach, it introduces a lot of
implementation complexity.

Relational databases prioritise ACID requirements - Atomicity, Consistency,
Isolation and Durability \cite{database-concepts}. This is a critical
requirement in this type of system, and usually one of the main reasons people
pick a relational database. Atomicity refers to transactions are a single unit
of work, consistency means it must be in a valid state before and after the
query, isolation means concurrent transactions do not interact with each other,
and durability means once something is committed it will stay committed.
\cite{database-concepts}

It is common for on-disk databases to consider the cost of CPU operations to
be $O(1)$ \cite{database-concepts}. This is partially due to when these
systems were made, the disks were much slower and the caches were much slower.
In part A of this project, this was disproved for PostgreSQL as it was found
that the time spent in the CPU was substantial: between $34.87\%$ and $76.56\%$
with an average of $49.32\%$ across the tested queries.
% TODO: Do I need a citation to myself?

%-------------------------------------------------------------------------------- 
\subsection{JIT Background}\label{subsec:jit_background}
%-------------------------------------------------------------------------------- 
Just-in-time (JIT) compilers function by having multiple layers of compilation and
are mostly used by interpreted languages to eliminate the ill-effects on
performance \cite{long-masters-thesis}. Advanced compilers can run the primary
program, then dedicate some background threads to improving the optimisation
of the code, and swap it over to the optimized version when it is ready
\cite{hyper-compiler-2}.

Due to branch-prediction optimization, JIT compilers can become faster than
ahead of time compilers. In 1999, a benchmarking paper measured four
commercial databases and found $10\%-20\%$ of their execution time was spent
fixing poor predictions \cite{branch-misprediction}. similarly, research
specifically into branch prediction has said, "although branch prediction
attained 99\% accuracy in predicting static branches, ... branches are still a
major bottleneck in the system performance" \cite{branch-prediction-values}.
Modern measurements still find 50\% of their query times are spent resolving
hardware hazards, such as mispredictions, with improvements in this area
making their queries 2.6x faster \cite{dbms-branches}. The Azul JIT compiler
measured that their JIT solution's speculative optimizations can lead up to $50\%$
performance gains \cite{azul_jit_aot}.

% TODO: I should mention what is a good branch prediction rate here

In the context of databases, most compilers can be split into only compiling
expressions (typically called EXP for expression), and others that compile the
entire Query Execution Plan (QEP) \cite{empirical-analysis}. Within PostgreSQL
itself, they have EXP support using $llvm-jit$.

\subsection{LLVM and MLIR}\label{subsec:llvm-mlir}

The LLVM Project is a compiler infrastructure that supports making compilers
so that common, but complex, compiler optimisations do not have to be re-
implemented. Multi-Level Intermediate Representation is another, newer toolkit
that is tightly coupled with the LLVM project. It adds a framework to define
dialects, and lower through these dialects. One of the primary benefits of
this is if you make a compiler, you can define a high level dialect, then
another person can target your custom high-level dialect.

% TODO: I need a lot more detail about how LLVM functions. What even is a dialect,
% what is a lowering, what does any of this stuff mean! This can be like... 4-8 
% paragraphs honestly. 

\subsection{WebAssembly and others}\label{subsec:llvm-mlir}
% Also, a paragraph on what is WASM, why is wasm special

\subsection{PostgreSQL Background}\label{subsec:postgresql-background}
% TODO: Provide sources
An arena allocator is a data structure that supports allocating memory and freeing
the entire data structure. This improves memory safety by consolidating
allocations into a single location. Within PostgreSQL, memory contexts are used
which is an advancement of this concept. There is a set of statically defined
memory contexts (TopMemoryContext, TopTransactionContext, CurTransactionContext,
TransactionContext, they are defined in the mmgr README), and with these you
can create child contexts. When a context is freed, all the child contexts are
also freed.

PostgreSQL defines query trees, plan trees, plan nodes, and expression nodes.
A query tree is the initial version of the parsed SQL, which is passed through
the optimiser which is then called a plan tree. The nodes in these plan trees
can broadly be identified as plan nodes or expression nodes.
Plan nodes include an implementation detail (aggregation, scanning a table,
nest loop joins) and expression nodes consist of individual operations (binaryop,
null test, case expressions).
% TODO: Include a graph of the PostgreSQL path and annotate in the previous 
% paragraph where the nodes are produced

\subsection{Database Benchmarking}\label{subsec:database-benchmarking}
Need to include information here about common benchmarks, and how the industry
has gone towards TPC-H.
