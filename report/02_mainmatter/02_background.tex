\chapter{Background}\label{ch:background}

%===============================================================================
\section{Database Background}\label{sec:database_background}
%-------------------------------------------------------------------------------
Most databases follow the structure shown in Figure~\ref{fig:database_structure}.
Database systems parse Structured Query Language (SQL) into relational algebra (RA),
optimize it, execute it, and materialize the results into a table \cite{database-concepts}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/database_structure.png}
	\caption{Database Structure}
	\label{fig:database_structure}
	\cite{database-concepts}
\end{figure}

Non-compiler databases use a volcano operator model tree, such as
Figure~\ref{fig:execution_tree} \cite{long-masters-thesis}. A \texttt{produce()} function at the root node
calls its children's \texttt{produce()}, until it calls a leaf node,
which calls \texttt{consume()} on itself, then that calls its parent's
\texttt{consume()} function. In other words, a post-order traversal through
the tree where tuples are dragged upwards.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/execution-tree.png}
	\caption{Volcano operator model tree.}
	\label{fig:execution_tree}
	\cite{long-masters-thesis}
\end{figure}

Classical models suffer a fundamental issue: heavy under-utilisation of hardware \cite{branch-misprediction}. If only a single tuple is pulled, CPU
caches are barely used. An i5-570, a popular CPU in 2010 had an 8 MB L3 cache,
but in 2024 an i5-14600K has a 24 MB L3 cache \cite{passmark_i5760},
\cite{techpowerup_i5_14600k}. For disks, in 2010 the Seagate Barracuda 7200.12
was popular, which had a sustained read of 138 MB/s, but in 2022 the Samsung
V-NAND SSD 990 PRO released with a sustained read of 7450 MB/s
\cite{seagate_barracuda_7200_12,samsung_990_pro}. Such dramatic increases
could mean the algorithms can fundamentally change.

These observations led to the vectorized and compiled execution models. The vectorized model pulls multiple tuples
up in a group rather than one at a time. A core advantage
is that instructions per CPU cycle (IPC) can increase through single
instruction, multiple data (SIMD) operations \cite{everything-vector}. However,
this can cause deep copy operations to be required, or more disk spillage
than necessary \cite{sequntial-execution}. For instance, if a sort or a join allocates new space
that is too much for the cache, the handling can become poor.
Section~\ref{sec:jit_background} and Chapter~\ref{ch:related-work} explore compilation approaches.

Relational databases prioritise ACID requirements - Atomicity, Consistency,
Isolation and Durability \cite{database-concepts}. A critical requirement in database systems,
ACID compliance is usually one of the main reasons people choose relational databases. Atomicity means transactions are a single unit
of work, while consistency means it must be in a valid state before and after the
query. Isolation means concurrent transactions do not interact with each other,
and durability means once something is committed it will stay committed.
\cite{database-concepts}

On-disk databases often assume CPU operation costs are constant or negligible \cite{database-concepts}. Such assumptions stem from the era
in which these systems were developed, when disks were much slower and caches were much smaller. Previous analysis disproved
such assumptions for PostgreSQL, finding that CPU time constitutes
substantial overhead: between $34.87\%$ and $76.56\%$ with an average of
$49.32\%$ across tested queries.

Most of the recent databasing research has focused on the optimiser,
which is visible in Figure~\ref{fig:database_structure}. Common optimization techniques include
reordering join statements where the left and right sides are flipped, and
predicate push down, where a conditional/filter on a node is moved
onto a lower node \cite{inersjo2021}. Additionally, extracting common subexpressions
prevents recomputation, while constant folding evaluates constant operations inside
the optimiser rather than at runtime. Despite these advances, query optimisers frequently
produce suboptimal plans \cite{optimizer-bad-1,optimizer-bad-2}. Another essential pattern involves genetic algorithms
being used in optimisers for determining things like join orders, which can
cause the outputted plan to be non-deterministic \cite{utesch1997geqo}.
A thesis could be written just to summarise the list of optimisations.

Within traditional and volcano databases, the cache is managed through buffer techniques.
Fixed-size pages (typically eight kilobytes) are read and loaded into
a buffer pool object which holds them in RAM. Memory placement in L1/L2/L3 and RAM caches depends on access
patterns, managed by the operating system or environment \cite{database-concepts}. Buffer pools employ different caching strategies (last-recently-used, most-recently-used, etc.)
based on the situation, decisions often made by the optimiser \cite{effelsberg1984buffer}. Cache effectiveness is measured by last level cache hit rate (LLC), which represents
how many instructions were resolved inside the CPU cache \cite{llc_intel}.

Databases are commonly split into Online Transaction Processing (OLTP) and
Online Analytical Processing (OLAP). OLTP focuses on
supporting atomicity, running multiple queries at once, and typically
handles the work profile of an online service that frequently performs key-value lookups.
On the other hand, OLAP databases focus on analytical work profiles
where aggregations are requested or operations span large
chunks of the database \cite{ddia}. OLAP systems can be highly distributed,
such as Apache Hive, which allows compute to be distributed across
the system \cite{apache_hive}. PostgreSQL implements
a hybrid architecture supporting both OLTP and OLAP operations
\cite{herrera2021hybrid}. Debate continues about whether
such hybrid designs remain useful, as load pressure
on user-serving databases commonly causes reliability issues
\cite{mooncake_htap}.

%-------------------------------------------------------------------------------
\section{JIT Background}\label{sec:jit_background}
%-------------------------------------------------------------------------------
Just-in-time (JIT) compilers work with multiple layers of compilation such
as raw interpretation of bytecode, unoptimized machine code, and optimized
machine code. They are primarily used with interpreted languages to eliminate
the ill-effects on performance \cite{long-masters-thesis}. Advanced compilers can run the primary program while background threads improve code optimisation
and swap in the optimized version when ready \cite{hyper-compiler-2}. Such multi-stage approaches
provide faster initial compilation and faster development cycles.

Due to branch-prediction optimisation, JIT compilers can be faster than
ahead of time compilers. In 1999, a benchmarking paper measured four
commercial databases and found $10\%-20\%$ of their execution time was spent
fixing poor predictions \cite{branch-misprediction}.
Modern measurements still find $50\%$ of their query times are spent resolving
hardware hazards, such as mispredictions, with improvements in this area
making their queries $2.6\times$ faster \cite{dbms-branches}. Azul's JIT compiler
measurements show that speculative optimisations can lead up to $50\%$ performance gains \cite{azul_jit_aot}.

Evaluating good branch prediction is difficult. A reasonable baseline is that a
$1\%$ misprediction rate is too high and should be optimised in low latency environments
\cite{farrier2025branch}. Such baselines lack formality but rest on empirical knowledge
\cite{branch-prediction-values}. Depending on the CPU, a branch mispredict can cost between 10 and
35 CPU cycles, with a safe range being 14-25 cycles. With 1 branch every 10 instructions
and a $5\%$ misprediction rate, a 20 cycle penalty per misprediction translates to approximately
$10\%$ of runtime spent resolving mispredictions \cite{eyerman2006branch}.

In the context of databases, compilers fall into two categories: those that compile only
expressions (typically called EXP), and those that compile the
entire Query Execution Plan (QEP) \cite{empirical-analysis}. Within PostgreSQL
itself, they have EXP support using \texttt{llvm-jit}. Chapter~\ref{ch:related-work} examines a variety of research databases.

\section{LLVM and MLIR}\label{sec:llvm-mlir}
The LLVM Project is a compiler infrastructure that eliminates the need to
re-implement common compiler optimisations, making it easier to build compilers~\cite{llvm-paper}. Multi-Level
Intermediate Representation (MLIR) is another, newer toolkit that is
tightly coupled with the LLVM project~\cite{mlir-paper}. It provides a framework to define custom dialects and progressively lower them to machine code. Developers can define high-level dialects that others can target and build upon.

LLVM defines a language-independent intermediate representation
(IR) based on Static Single Assignment (SSA) form, while MLIR extends
this concept \cite{llvm-paper}. The architecture follows a three-phase design: a
front-end parses source code
and generates LLVM IR, an optimiser applies a series of transformation
passes to improve code quality, and a back-end generates machine code for
the target architecture. MLIR extends this concept by introducing a
flexible dialect system that
enables progressive lowering through multiple levels of
abstraction~\cite{mlir-paper}. This addresses software fragmentation
in the compiler ecosystem, where projects were creating incompatible high
-level IRs in front of LLVM, and improves compilation for
heterogeneous hardware by allowing target-specific optimisations
at appropriate abstraction levels.

LLVM's On-Request-Compilation (ORC) JIT is a system for
building JIT compilers with support for lazy compilation,
concurrent compilation, and runtime optimisation~\cite{llvm-jit-orc}.
ORC can compile code on-demand as it is needed, reducing startup
time by deferring compilation of functions until they are first
called. The JIT supports concurrent compilation across
multiple threads and provides built-in dependency tracking
to ensure code safety during parallel execution. This makes ORC particularly
suitable for dynamic language implementations, REPLs
(Read-Eval-Print Loops), and high-performance JIT compilers.

\section{WebAssembly and others}\label{sec:webassembly}
The V8 compiler used for WebAssembly has a unique architecture because
it targets short-lived programs. Majority of JIT applications
are used for long-running services, but this is used for web pages which are
opened and closed frequently. To mitigate this, they have a two-phase
architecture where code is first compiled with Liftoff for a quick startup,
then hot functions are recompiled with TurboFan \cite{wasm}. Liftoff aims
to create machine code as fast as possible and skip optimisations.

Other common JIT compilers are the Java Virtual Machine (JVM), SpiderMonkey
(Mozilla Firefox's JIT), JavaScriptCore/Nitro (Safari/Webkit), PyPy, various
python JIT compilers, LuaJIT for Lua, HHVM for PHP, Rubinius for Ruby,
RyuJIT for C\#, and more \cite{jit_attacks_survey}. The JVM has also been used for compiled
query execution engines \cite{jamdb}. These all target different work profiles.

\section{PostgreSQL Background}\label{sec:postgresql-background}
PostgreSQL relies on memory contexts, which are an extension of arena
allocators. An arena allocator is a data structure that supports allocating
memory and freeing
the entire data structure. This improves memory safety by consolidating
allocations into a single location. A memory context can create child contexts,
and when a context is freed it also frees all the children of this context,
turning it into a tree of arena allocators. There is a set of statically defined
memory contexts: TopMemoryContext, TopTransactionContext, CurTransactionContext,
TransactionContext, which are managed through PostgreSQL's Server Programming
Interface (SPI) \cite{postgresql_spi_memory}.

PostgreSQL defines query trees, plan trees, plan nodes, and expression nodes.
A query tree is the initial version of the parsed SQL, which is passed through
the optimiser which is then called a plan tree. These stages are visible in
Figure~\ref{fig:database_structure}. The nodes in these plan trees
can broadly be identified as plan nodes or expression nodes.
Plan nodes include an implementation detail (aggregation, scanning a table,
nest loop joins) and expression nodes consist of individual operations (binaryop, null test, case expressions) \cite{postgresql_querytree}.

PostgreSQL provides the \texttt{EXPLAIN} command to inspect query execution
plans, which is essential for understanding and optimizing query performance
\cite{postgresql_explain}. This command displays the execution plan that the
planner generates, including cost estimates and optional execution statistics,
making it a critical tool for database optimisation and analysis.

\section{Database Benchmarking}\label{sec:database-benchmarking}
% cite pg_bench
Benchmarking a database is difficult because of the variety of
workloads. Many systems create their own benchmarking libraries,
such as \texttt{pg\_bench} by PostgreSQL \cite{postgresql_pgbench} or
LinkBench \cite{linkbench} by Facebook, but in academics the
more common benchmarks are from
the Transaction Processing Council, which is a group that defines benchmarks
\cite{tpch_analyzed}.  Over the years they have made TPC-C for an order-entry
environment, TPC-E, for a broker firm's operations, TPC-DS for a decision
support benchmark. TPC-H is the most common in research,
where the H informally means "hybrid". It has a mix of analytical and
transactional elements inside it \cite{tpch_analyzed}.

When evaluating benchmark results, the \textit{coefficient of variation} (CV) is used.
This is a standardized measure of dispersion that expresses the standard deviation
as a percentage of the mean~\cite{stats-libretexts-cv}. It is calculated as
$CV = \frac{s}{\bar{X}} \cdot 100$, where $s$ is the sample standard deviation
and $\bar{X}$ is the sample mean. The coefficient of variation is particularly
useful when comparing datasets with different units or scales, providing a unit-free
measure for relative comparison of measurement precision.
