% Still 9 TODOs scattered throughout the text
\chapter{Related Work}\label{ch:related-work}

This chapter summarises relevant works in the compiled queries space and their
architectures. Originally, the industry began with compiled query engines, but
this was overtaken by volcano models as they simplified the implementation
details with little cost at the time. However, now analytical engines are
examining compilers again.

This begins with PostgreSQL and their extension system in
section~\ref{sec:postgresql-related}, in section~\ref{sec:system-r} system R
will be explored as the classical
example, followed by HyPer and Umbra in section~\ref{sec:hyper} and
section~\ref{sec:umbra} which re-introduced the concept. Mutable in
section~\ref{sec:mutable} and LingoDB in section~\ref{sec:lingodb} are research
databases. Lastly, PostgreSQL will be examined in section~\ref{sec:postgresql-related}
as it uses expression-based compilation and there has been an attempt to create
a compiled engine before.

%-------------------------------------------------------------------------------- 
\section{PostgreSQL and Extension Systems}\label{sec:postgresql-related}
%-------------------------------------------------------------------------------- 
PostgreSQL is a battle-tested system and is currently the most
popular database in the world with $51.9$ of developers in a stackoverflow survey
saying they use it extensively in 2024 \cite{stackoverflow2024}. Within the context of compiled queries this means
the database itself cannot be treated as a research system. Changes directly to
it requires heavy-testing, but also, these changes will not be peer-reviewed
research. Instead, it is pull-requests online with more casual interaction.

There has been significant discussion about HyPer and JIT with regards to
PostgreSQL in 2017 \cite{postgres-pr-2}. The general response is doubt that someone will add support
for full compiling full query expressions, and rearchitecting such a core
component introduces large risk.

However, in September 2017 Andres Freund started implementing JIT support for
expressions \cite{psql-jit-release-plan}. The reasoning was that most of the CPU time is in the expression
components, (e.g. y > 8 in SELECT * from table WHERE x > 8;). Furthermore,
there are significant benefits to tuple deformation as it interacts with the
cache and has poor branch prediction.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/pg-min-params.png}
	\caption{Peter Eisentruat asking whether the defaults are too low.}
	\label{fig:pg-params-too-low}
	\cite{postgres-pr-2}
\end{figure}

In the pull request Peter Eisentruat asked whether the default JIT
settings are too low, but in version 11 of PostgreSQL they went ahead with the
release but with the JIT disabled by default. This didn't get much usage,
and they decided that enabling it by default it would give it exposure and testing.
However, when released, the United Kingdom's critical service for a COVID-19
dashboard automatically updated and spiked to a 70\% failure rate as
some of their queries ran 2,229x slower \cite{psql-jit-failure}. This affected
the general reception that JIT features should be disabled by default, and
has lead to people having negative opinions about JIT and compiled queries.

Two cases where QEP query compilation with PostgreSQL was implemented were found.
The first is Vitesse DB, which made a series of public posts about getting
people to assist with testing it. They became generally available in 2015,
but their website is offline now and there is not much mention of them.
The second was at a PgCon presentation and achieved a 5.5x speedup on TPC-H query
1, and has more documentation. However, they did not publicize their methods or
show that it's easy for people to use.
% TODO, maybe I should include more detail about the pgcon one.
% vitesse db https://www.postgresql.org/message-id/CAJNt7=Z6w5%2BwyeTKKryWavMcWOenc6ZhPaaaddwuZ6HtcJWxcg@mail.gmail.com
% ispras https://www.pgcon.org/2017/schedule/attachments/467_PGCon%202017-05-26%2015-00%20ISPRAS%20Dynamic%20Compilation%20of%20SQL%20Queries%20in%20PostgreSQL%20Using%20LLVM%20JIT.pdf

Other database systems also support extensions, and there are many systems that
rely on PostgreSQL's extension system. MySQL, ClickHouse, DuckDB, Oracle
Extensible Optimizer all support similar operations. This means more than
only PostgreSQL can be extended in this same manner rather than creating
databases from scratch.

%-------------------------------------------------------------------------------- 
\section{System R}\label{sec:system-r}
%-------------------------------------------------------------------------------- 
System R is a flagship paper in the databasing space that introduced SQL,
compiling engines, and ACID \cite{system-r}. Their vision described ACID requirements, but was
explained as seven dot points as it was not a concept yet. Their goal was to
run at a "level of performance comparable to existing lower-function database
systems." Reviewers commented that the compiler is the most important part of
the design.

Due to the implementation overhead of parsing, validity checking, and access
path selection, a compiler was appealing. These were not supported within the
running transaction by default, and they leveraged pre-compiled fragments of
Cobol for the reused functions to improve their compile times. This was
completely custom-made at the time because there were not many tools to support
writing compilers. System R shows the idea of compiled queries is as old
as databases, and over time the priorities of the systems changed.

%-------------------------------------------------------------------------------- 
\section{HyPer}\label{sec:hyper}
%-------------------------------------------------------------------------------- 
% TODO: lol, poorly written paragraph
% TODO: Maybe need to comment why we're going into so much detail on HyPer.
HyPer was a flagship system, and Umbra supersedes it. Both were made
by Thomas Neumann, and the core sign is of its viability is that HyPer
was purchased by Tableau in 2016 to be used in production \cite{tableau-hyper}. This shows
that it is is possible to use an in-memory JIT database at scale. The project
began in 2010, with their flagship paper releasing in 2011 for the compiler
component \cite{hyper-compiler-1}, and in 2018 they released another flagship paper about
adaptive compilation \cite{hyper-compiler-2}. However, the database being commercialised poses issues
for outside research because the source code is not accessible, but there is a
binary on their website that can be used for benchmarking.

Their 2011 paper on the compiler identifies that translating queries into C or
C++ introduced significant overhead compared to compiling into LLVM. As a result,
they suggested using pre-compiled C++ objects of common functions then
inlining them into the LLVM IR. This LLVM IR is executed by the LLVM's JIT
executor. By utilising LLVM IR, they can take advantage of overflow flags and
strong typing which prevent numerous bugs in their original C++ approach.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/hyper-compile-times.png}
	\caption{HyPer OLAP performance compared to other engines.}
	\label{fig:hyper-compile-times}
	\cite{hyper-compiler-1}
\end{figure}

HyPer shows they reduced their compile time by doing this in figure XYZ by many
multiples, and in figure ABC they show they achieve many times less branches,
branch mispredicts, and other measurements compared to their baseline of MonetDB.
The cause of this is HyPer's output had less code in the compiled queries.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/hyper-profiler.png}
	\caption{HyPer branching and cache locality benchmarks.}
	\label{fig:hyper-profiler}
	\cite{hyper-compiler-1}
\end{figure}

Hyper continues on in 2018 where they separated the compiler into multiple rounds.
They introduced an interpreter on the byte code generated from LLVM IR, then they
can run unoptimised machine code, and on the final stage they can run optimised
machine code. Figure XYZ visualises this with the compile times of each stage.
However, they had to create the byte code interpreter themselves to enable this.

The 2018 paper also improved their query optimisation by adding a dynamic
measurement for how long live queries are taking. This is because the
optimiser's cost model did not lead to accurate measurements for compilation
timing. Instead, they introduced an execution stage for workers, then
in a "work-stealing" stage they log how long the job took. With a combination
of the measurements and the query plan, they calculate estimates for jobs
and optimal levels to compile them to.

This was benchmarked with TPC-H Query 11 using 4 threads, and they found
the adaptive execution was faster than only using bytecode by $40\%$, unoptimised
compilation by $10\%$ and optimised compilation by $80\%$. The cause of this is that
the compilation stage is single threaded, while with multiple threads they can
compile in the background while execution is running.

Utilising additional stages of the LLVM compiler, improving the cost model, and
supporting multi threading the compilation and execution combined into a viable
JIT compiled-query application. The primary criticism is that they effectively
wrote the JIT compiler from the ground-up, which requires large amounts
of engineering time. Majority of the additions here are not unique to a database's
JIT compiler, and are mostly ways to target the compiler's latency.

%-------------------------------------------------------------------------------- 
\section{Umbra}\label{sec:umbra}
%-------------------------------------------------------------------------------- 
Umbra was created in 2020 by Thomas Neumann, the creator of HyPer, and the
main change is that they show it is possible to use the in-memory database
concepts from HyPer inside of an on-disk database \cite{umbra}. The core reason for this is
the recent improvements of SSDs and buffer management advances. They take
concepts from LeanStore for the buffer management and latches, then multi-version
concurrency, compilation, and execution strategies from HyPer. This combination
led to an on-disk database that is scalable, flexible and faster than HyPer.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{figures/umbra-benchmark.png}
	\caption{Umbra benchmarks.}
	\label{fig:umbra-benchmark}
	\cite{umbra}
\end{figure}

% I skipped a paragraph about multi-level debugger here

A novel optimisation they introduced later was enabling the compiler to
change the query plan \cite{umbra-effec-compil}. That is, they can use the metrics collected during
execution to swap the order of joins, or the type of join being used.
This improved the runtime of the data-centric queries by two-times. Some other
databases introduce this concept by invoking the query optimiser multiple
times, but since their compiler is invoked multiple times during execution this
adds additional benefit.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{figures/umbra-effec-queries.png}
	\caption{Umbra benchmarks after adaptive query processing (AQP).}
	\label{fig:umbra-effec-queries}
	\cite{umbra-effec-compil}
\end{figure}

Umbra is currently ranked as the most effective database on ClickHouse's
benchmarking \cite{clickbench}. The main complaint of the compiler being too heavy is still there,
but it shows the advantage of having direct access to the JIT compiler with
its adaptive compilation to change optimisation choices.
% I wonder if I can say more was included in the original literature review?
% I guess the thesis is a standalone work though haha

%-------------------------------------------------------------------------------- 
\section{Mutable}\label{sec:mutable}
%-------------------------------------------------------------------------------- 
In 2023, Mutable presented the concept of using a low latency JIT
compiler (WebAssembly) rather than a heavy one in their initial paper \cite{mutable-1}.
Its primary purpose, however, is to serve as a framework for implementing other
concepts in database research so that they do not need to rewrite the framework
later \cite{mutable-2}. However, using WebAssembly meant they can omit most of the optimisations
that HyPer did while maintaining a higher of performance. Furthermore, they have
a minimal query optimiser and instead rely on the V8 engine.

The V8 engine contains a "Liftoff" component that adds an early-stage execution
step to lower the initial overhead of running the query \cite{wasm-liftoff}. The liftoff component
produces machine code as fast as possible while skipping opimisations, then
"turbofan" is a second-stage compiler that runs in the background
while execution is running. However, HyPer has a direct bytecode interpreter
which can result in a lower time to execution.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/mutable-arch.png}
	\caption{Comparison of mutable to HyPer and Umbra.}
	\label{fig:mutable-arch}
	\cite{mutable-2}
\end{figure}lation phase graph

Mutable's benchmarks show they achieve similar compile and execution times to
HyPer, and outperform them in many cases \cite{mutable-2}. While pushing Mutable to the
same performance as HyPer or Umbra would require re-architecting, achieving this
performance within the implementation effort is a significant outcome.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{figures/mutable-benchmarks.png}
	\caption{Benchmarks produced by Mutable.}
	\label{fig:mutable-bench}
	\cite{mutable-2}
\end{figure}

%-------------------------------------------------------------------------------- 
\section{LingoDB}\label{sec:lingodb}
%-------------------------------------------------------------------------------- 
% I skipped the overview,
LingoDB piloted in 2022 and proposed using the MLIR framework to create the
optimisation layers \cite{lingo-db-1}. In most databases, the system parses the SQL into a
query tree, then relational algebra, then this is optimised using manual
implementations, and parse this into a plan tree for execution, or compile
this into a binary. With MLIR, this pipeline changes into parsing the plan
tree into a high-level dialect in MLIR, then doing optimisation passes on the
plan itself, and the LLVM compiler can be used directly to turn this into
LLVM, streamlining the process.

% todo lol
The LingoDB architecture can be seen in figure XYZ, which begins by parsing the
SQL into a relational algebra dialect. These dialects are defined using
MLIR's dialect system, and supported through code generation. Their compiler
is defined by a relational algebra dialect, a database dialect that represents
SOMETHING, a DSA dialect that represents, a utility dialect that represents
SOMETHING, and the final LLVM output. This splits the state of the intermediate
representation into three stages: relational algebra, a mixed dialect, and
finally the LLVM.

Their result is that they are less performant than HyPer, but do better than
DuckDB \cite{lingo-db-1}. This performance is not their key output, rather, it is that they can
implement the standard optimisation patterns within the compiler. Another feat
is that they are approximately 10,000 lines of code in the query execution model,
and Mutable is at 22,944 lines for their code despite skipping query
optimisation. Within LingoDB's paper they also compare this to being three times
less than DuckDB and five times less than NoisePage.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{figures/lingo-db-bench.png}
	\caption{LingoDB benchmarking.}
	\label{fig:lingo-db-bench}
	\cite{lingo-db-1}
\end{figure}

In later research, LingoDB also explores obscure operations such as GPU
acceleration, using the Torch-MLIR project's dialect, representing queries
as sub-operators for acceleration, non-relational systems, and more \cite{lingo-db-2}.
For our purposes, the appealing part of their architecture is that they use
$pg_query$ to parse the incoming SQL, which means their parser is the closest
to PostgreSQL's. This will be explored in the design in REFERENCE.
% idk why \texttt doesn't work here.

%-------------------------------------------------------------------------------- 
\section{Benchmarking}\label{sec:benchmarking}
%-------------------------------------------------------------------------------- 
% TODO: Maybe this section should change places? Meh.
These systems produced their own benchmarks and could selectively pick which
systems to involve, so a recreation of the benchmarks was done. DuckDB, HyPer,
Mutable, LingoDB and PostgreSQL were all compared to one another, and is
visible in Figure~\ref{fig:created-benchmark-results}. TPC-H was used as most
of the involved pieces used it themselves \cite{tpch_analyzed}, and docker containers were
chosen to make deploying it easier. These benchmarks were
created by relying on the Mutable codebase as they had significant infrastructure
to support this, and is visible at
\url{https://github.com/zyros-dev/benchmarking-dockers}.

The benchmarks show that PostgreSQL is significantly slower than the rest, likely
because it is an on-disk database and most of the others are in-memory. With
PostgreSQL removed from the graph, HyPer and DuckDB are the fastest,
and with a single core DuckDB is the slowest.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.3]{figures/benchmarks.png}
	\caption{Benchmarking results.}
	\label{fig:created-benchmark-results}
\end{figure}

To identify how much potential gain there is in a major on-disk database,
\texttt{perf} was used on PostgreSQL during TPC-H queries 1, 3, 6, 12 and 14 in
figure~\ref{fig:pprof-psql} \cite{perf_man_page}. These queries were chosen because the Mutable
code infrastructure directly supported them. This shows that the CPU time
varied from between 34.87\% and 76.56\%, with an average of 49.32\%. These
metrics were identified
using the \texttt{prof} graph. With this much time in the CPU, it is
clear that the queries can become several times faster if optimised.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{figures/pperf_psql.png}
	\caption{PostgreSQL time spent in the CPU, measured with prof.}
	\label{fig:pprof-psql}
\end{figure}

As stated in section~\ref{sec:jit_background}, one core acceleration of
JIT
is improving the branch predictions \cite{branch-misprediction}. To measure this, \texttt{perf} was
used on the same set of queries as shown in Table~\ref{tbl:prof-psql}. This
shows that
at worst, 43.02\% of LLC loads are missed \cite{llc_intel}. A 0.69\% branch miss rate has room
for improvement, as systems with even a 99\% branch prediction accuracy can
have a significant bottleneck from it.

% TODO: I think this stuff should just be represented with a graph
% in the results section
\begin{table}[H]
	\centering
	\footnotesize
	\caption{Preliminary PostgreSQL query profiling statistics collected with \texttt{perf}.}
	\label{tbl:prof-psql}
	\begin{tabular}{l@{\hspace{0.5em}}rrrrr@{\hspace{0.5em}}l}
		\toprule
		\textbf{Metric} & \textbf{Q1}      & \textbf{Q3}      & \textbf{Q6}      & \textbf{Q12}     & \textbf{Q14}     & \textbf{Unit} \\
		\midrule
		\multicolumn{7}{c}{\textit{Absolute Values}}                                                                                   \\
		\midrule
		Task-clock      & 992.88           & 566.44           & 349.64           & 514.66           & 368.84           & msec          \\
		Page faults     & 4230             & 3236             & 109              & 497              & 1928             & occurrences   \\
		Cycles          & 2.72e9           & 1.57e9           & 9.76e8           & 1.42e9           & 1.03e9           & occurrences   \\
		Instructions    & 6.49e9           & 2.63e9           & 1.61e9           & 2.44e9           & 1.69e9           & occurrences   \\
		Branches        & 1.03e9           & 4.45e8           & 2.79e9           & 4.34e8           & 2.89e8           & occurrences   \\
		Branch misses   & \textbf{1.21e6}  & \textbf{2.67e6}  & \textbf{1.36e6}  & \textbf{3.01e6}  & \textbf{1.21e6}  & occurrences   \\
		LLC loads       & 5.17e6           & 7.00e6           & 4.48e6           & 6.11e6           & 4.88e6           & occurrences   \\
		LLC load misses & \textbf{1.76e6}  & \textbf{2.84e6}  & \textbf{1.93e6}  & \textbf{2.37e6}  & \textbf{2.07e6}  & occurrences   \\
		\midrule
		\multicolumn{7}{c}{\textit{Normalized Values}}                                                                                 \\
		\midrule
		Task-clock      & 0.104            & 0.048            & 0.032            & 0.041            & 0.034            & CPUs          \\
		Page faults     & 4260             & 5713             & 311              & 966              & 5227             & /sec          \\
		Cycles          & 2.743            & 2.777            & 2.79             & 2.768            & 2.789            & GHz           \\
		Instructions    & 2.38             & 1.67             & 1.65             & 1.71             & 1.64             & insn/cycle    \\
		Branches        & 1040             & 786              & 797              & 843              & 785              & M/sec         \\
		Branch misses   & \textbf{0.12\%}  & \textbf{0.60\%}  & \textbf{0.49\%}  & \textbf{0.69\%}  & \textbf{0.42\%}  & of branches   \\
		LLC loads       & 5.203            & 12.354           & 12.807           & 11.868           & 13.22            & M/sec         \\
		LLC load misses & \textbf{34.12\%} & \textbf{40.65\%} & \textbf{43.02\%} & \textbf{38.87\%} & \textbf{42.37\%} & of LL-cache   \\
		\bottomrule
	\end{tabular}
\end{table}

%-------------------------------------------------------------------------------- 
\section{Gaps in Literature}\label{sec:gaps}
%-------------------------------------------------------------------------------- 

A core gap is the extension system within existing database. HyPer and Umbra
managed to commercialise their systems, but the other databases are strictly
research systems and some do not support ACID, multithreading, or other core
requirements such as index scans. Michael Stonebraker, a Turing Award recipient and the founder of
PostgreSQL, writes that a fundamental issue in research is that they have
forgotten the usecase of the systems and target the $0.01\%$ of users \cite{top-ten-fears}.
These commercial databases reaching high performance is a symptom of this.
Testing the wide variety of ACID requirements is a significant undertaking.

The other issue is writing these compiled query engines is a large undertaking,
and the core reason why vectorised execution has gained more popularity in
production systems. Debugging a compiled program within a database is challenging,
and while solutions have been offered, such as Umbra's debugger \cite{umbra-debugger}, it is still
challenging and questionable how transferable those tools are.

Relying on an extension system such that it's an optional feature means users
can install the optimisations, and tests can be done with production systems
without requesting pull requests into the system itself. Since these are large
source code changes, it adds political complexity to have the solution
added to the official system without production proof of it being used.
The result of this would be an useable compiler accelerator, that can easily
be installed into existing systems, and once used in many scenarios is
easier to add to the official system. % TODO badly written lol

%================================================================================ 
\section{Aims}\label{sec:lit-rev}
%================================================================================ 
% need to state what advantages we have for keeping postgres, ACID, etc
Tying this together, this piece aims to integrate a research compiler into a
battle-tested system by using an extension system. This addresses the gap
of these systems being difficult to use widely, and potential to integrate it
into the original system once stronger correctness and speed optimisations have
been shown. Accomplishing this shows there is a way to rely on previous
ACID-compliance and supporting code infrastructure.

A key output is showing that the system can operate within
the same order of magnitude as the target system. The purpose of this is to
ensure other optimisations can be applied to fit the surrounding database later,
but the expectation is not to be faster than it.

One concern is these databases are large systems while the research systems
are smaller. This increases the testing difficulty because a complete system
has more variables, such as genetic algorithms in the query optimiser that makes
performance non-deterministic. To counter this, a large number of benchmarks can
be executed, and a standard deviation can be calculated.