\chapter{Related Work}\label{ch:related-work}

This chapter summarises relevant works in the compiled query space and their
architectures. The database industry originally favoured compiled query engines \cite{system-r}, but
the volcano model subsequently dominated because it simplified implementation
with minimal performance cost at the time. However, modern analytical engines
are revisiting compilation approaches \cite{everything-vector}.

This chapter begins with PostgreSQL's extension system in
section~\ref{sec:postgresql-related}, followed by System R in section~\ref{sec:system-r} as a classical example
of early compilation. HyPer and Umbra (sections~\ref{sec:hyper} and~\ref{sec:umbra})
re-introduced compilation in modern databases. Two research databases are then examined: Mutable
(section~\ref{sec:mutable}) and LingoDB (section~\ref{sec:lingodb}). Finally,
PostgreSQL's own expression-based JIT compilation and prior compilation attempts are discussed.

%-------------------------------------------------------------------------------- 
\section{OLAP Systems}\label{sec:olap-systems}
%-------------------------------------------------------------------------------- 

Compiled query engines primarily benefit OLAP workloads, since
OLTP workloads typically involve simpler retrieval queries \cite{ddia}.
At scale, Apache Hive is commonly used, but it is a data warehouse
system rather than a database, storing data in Hadoop's distributed file system,
which is closer to flat storage~\cite{apache_hive}. The common OLAP
databases are MonetDB, SnowFlake, ClickHouse, RedShift and Vectorwise
\cite{htap_survey}. For our context, understanding ClickHouse, NoisePage,
DuckDB and extensions that turn PostgreSQL into an OLAP database are important.

ClickHouse and NoisePage are standalone systems, while DuckDB is embedded
and in-process, similar to SQLite. ClickHouse is a columnar, disk-oriented
database with a vectorised execution engine with optional LLVM compilation
for expressions (EXP) \cite{clickhouse}. NoisePage was created by the Carnegie
Mellon Database group, and is columnar and in-memory
with full query expression compilation (QEP), with a single node. They targeted
ML-driven self optimisation in their research, but they were archived in February 21, 2023 \cite{noisepage}. DuckDB is marketed
as the SQLite for analytical loads with in-memory disk spillage, or like a
more sophisticated Pandas DataFrame \cite{duckdb}. Their engine supports
vectorised execution because JIT would add too much overhead to their
lightweight philosophy.

%-------------------------------------------------------------------------------- 
\section{PostgreSQL and Extension Systems}\label{sec:postgresql-related}
%-------------------------------------------------------------------------------- 
PostgreSQL is a battle-tested system and the most
popular database, with 51.9\% of developers in a Stack Overflow survey
reporting extensive use in 2024 \cite{stackoverflow2024}. In the context of compiled queries, this means
PostgreSQL cannot be treated as a research prototype. Direct modifications require extensive testing, and
such changes face casual code review via pull requests rather than formal peer review.

Building extensions for PostgreSQL and making entire companies around these
extensions is not a new endeavour. Three such examples are Citus~\cite{citus},
TimescaleDB~\cite{timescaledb}, and Apache AGE~\cite{apache_age}. Citus
aims to add more horizontal scaling through sharding, TimescaleDB, now
rebranded as TigerData, transforms the engine into a time series database,
and Apache AGE turns it into a graph database. These all have thousands
of GitHub stars, but TimescaleDB especially had over 500 paying customers in
2022, and claimed their revenue climbed 20x by 2024 \cite{timescale_2024_update}
\cite{timescaledb_series_c}. This shows that the model of relying on the
extension system is a robust, travelled road.

There have also been several extensions that attempt to make PostgreSQL more
suited to OLAP workloads, with the most relevant one being pg\_duckdb
\cite{pg_duckdb}. This system replaces PostgreSQL's engine with DuckDB which
allows it to take advantage of the vectorised engine, and it is reasonably
popular with roughly 2700 stars on GitHub. Hydra is also worth mentioning,
but this is closer to TimescaleDB \cite{hydra_columnar} and makes the system
columnar with compression support. ParadeDB's pg\_analytics initially started
in the same way as pg\_duckdb, but pivoted into supporting search functionality
instead, similar to Elasticsearch.

There has been significant discussion about HyPer and JIT in regard to
PostgreSQL in 2017 \cite{postgres-pr-1,postgres-pr-2}. The general response expressed doubt that
someone would add support
for full query compilation, noting that rearchitecting such a core
component introduces significant risk.

However, in September 2017 Andres Freund started implementing JIT support for
expressions \cite{psql-jit-release-plan}. The reasoning was that most of the CPU time is in the expression
components, (such as y > 8 in SELECT * from table WHERE x > 8;). Furthermore,
there are significant benefits to tuple deformation as it interacts with the
cache and has poor branch prediction. PostgreSQL's JIT implementation is
documented in the official PostgreSQL documentation \cite{psql-jit-doc}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/pg-min-params.png}
	\caption{Peter Eisentraut asking whether the defaults are too low.}
	\label{fig:pg-params-too-low}
	\cite{postgres-pr-2}
\end{figure}

In a pull request, Peter Eisentraut questioned whether the default JIT settings
were too low. Despite this concern, PostgreSQL version 11 was released with JIT disabled by default.
This conservative approach resulted in limited adoption initially, so they later decided to enable
JIT by default to increase exposure and testing opportunities \cite{psql-jit-discussion}.
However, when released, the United Kingdom's critical service for a COVID-19
dashboard automatically updated and spiked to a 70\% failure rate as
some of their queries ran 2,229x slower \cite{psql-jit-failure}. This incident reinforced
the view that JIT features should be disabled by default, and
has led to negative opinions about JIT and compiled queries.

Two cases where QEP query compilation with PostgreSQL was implemented were found.
The first is Vitesse DB, which made a series of public posts about getting
people to assist with testing it. They became generally available in 2015,
but their website is offline now and there is not much mention of them.
The second was at a PgCon presentation and achieved a 5.5x speed-up on TPC-H query
1, and has more documentation \cite{pgcon_jit_2017}. However, they did not publicize their implementation or show that it's easy for people to use.

This implementation of full JIT in PostgreSQL was preceded by
profiling work which showed that different TPC-H benchmarks place pressure on
different nodes, as shown in figure~\ref{fig:pgcon-profiling}. This is important
because they can make informed decisions about where to focus their efforts.
Their core method is generating a function that represents a node in the
plan tree, then inlining the function into the final LLVM IR \cite{pgcon_jit_2017}
in a push-based model. Another
interesting method that is used is pre-compiling the C code into LLVM,
then inlining that into the LLVM IR. This avoids runtime linking back to the C
code \cite{pgcon_jit_2017}, similar to approaches taken by HyPer \cite{hyper-compiler-1}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/pgcon_profiling.png}
	\caption{PGCon Dynamic Compilation of SQL Queries Profiling~\cite{pgcon_jit_2017}.}
	\label{fig:pgcon-profiling}
	\cite{postgres-pr-2}
\end{figure}

Other database systems also support extensions, and there are many systems that
rely on PostgreSQL's extension system. MySQL, ClickHouse, DuckDB, Oracle
Extensible Optimizer all support similar operations. This means more than
only PostgreSQL can be extended in this same manner rather than creating
databases from scratch.

%-------------------------------------------------------------------------------- 
\section{System R}\label{sec:system-r}
%-------------------------------------------------------------------------------- 
System R is a flagship paper in the databasing space that introduced SQL,
compiling engines, and ACID \cite{system-r}. Their vision described ACID requirements, but was
explained as seven dot points as it was not a concept yet. Their goal was to
run at a "level of performance comparable to existing lower-function database
systems." Reviewers commented that the compiler is the most important part of
the design.

Due to the implementation overhead of parsing, validity checking, and access
path selection, a compiler was appealing. These were not supported within the
running transaction by default, and they leveraged pre-compiled fragments of
Cobol for the reused functions to improve their compile times. This was
completely custom-made at the time because there were not many tools to support
writing compilers. System R shows the idea of compiled queries is as old
as databases, and over time the priorities of the systems changed.

%-------------------------------------------------------------------------------- 
\section{HyPer}\label{sec:hyper}
%-------------------------------------------------------------------------------- 
HyPer was a flagship system, and Umbra supersedes it. These are important
systems in the JIT-database space as they developed many of the core features.
Both were made by Thomas Neumann, and a core sign of its viability is that HyPer
was purchased by Tableau in 2016 to be used in production \cite{tableau-hyper}. This shows
that it is possible to use an in-memory JIT database at scale. The project
began in 2010, with their flagship paper releasing in 2011 for the compiler
component \cite{hyper-compiler-1}, and in 2018 they released another flagship paper about
adaptive compilation \cite{hyper-compiler-2}. However, the database being commercialised poses issues
for outside research because the source code is not accessible, but there is a
binary on their website that can be used for benchmarking.

Their 2011 paper on the compiler identifies that translating queries into C or
C++ introduced significant overhead compared to compiling into LLVM. As a result,
they suggested using pre-compiled C++ objects of common functions then
inlining them into the LLVM IR. This LLVM IR is executed by the LLVM's JIT
executor. By utilising LLVM IR, they can take advantage of overflow flags and
strong typing which prevent numerous bugs in their original C++ approach.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/hyper-compile-times.png}
	\caption{HyPer OLAP performance compared to other engines.}
	\label{fig:hyper-compile-times}
	\cite{hyper-compiler-1}
\end{figure}

Figure~\ref{fig:hyper-compile-times} demonstrates that HyPer reduced compile time by many multiples.
Figure~\ref{fig:hyper-profiler} shows they achieved many times fewer branches and branch mispredictions
compared to their MonetDB baseline \cite{monetdb_x100}. This improvement resulted from HyPer's
output containing less code in the compiled queries.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/hyper-profiler.png}
	\caption{HyPer branching and cache locality benchmarks.}
	\label{fig:hyper-profiler}
	\cite{hyper-compiler-1}
\end{figure}

Hyper continues on in 2018 where they separated the compiler into multiple rounds.
They introduced an interpreter on the byte code generated from LLVM IR, then they
can run unoptimised machine code, and on the final stage they can run optimised
machine code. Figure~\ref{fig:hyper-adaptive} visualises this with
the compile times of each stage.
However, they had to create the byte code interpreter themselves to enable this.

\begin{figure}[H]
	\centering
	\includegraphics[scale=1]{figures/hyper-adaptive.png}
	\caption{HyPer execution modes and compile times.}
	\label{fig:hyper-adaptive}
	\cite{hyper-compiler-2}
\end{figure}

The 2018 paper also improved their query optimisation by adding a dynamic
measurement for how long live queries are taking. This is because the
optimiser's cost model did not lead to accurate measurements for compilation
timing. Instead, they introduced an execution stage for workers, then
in a "work-stealing" stage they log how long the job took. With a combination
of the measurements and the query plan, they calculate estimates for jobs
and optimal levels to compile them to.

This approach was evaluated using TPC-H Query 11 with 4 threads. The adaptive
execution strategy outperformed bytecode-only execution by $40\%$, unoptimised
compilation by $10\%$, and optimised compilation by $80\%$. This improvement occurs because
the single-threaded compilation can run in parallel with query execution on other threads.

Utilising additional stages of the LLVM compiler, improving the cost model, and
supporting multi threading the compilation and execution combined into a viable
JIT compiled-query application. The primary criticism is that they effectively
wrote the JIT compiler from the ground-up, which requires large amounts
of engineering time. Majority of the additions here are not unique to a database's
JIT compiler, and are mostly ways to target the compiler's latency.

%-------------------------------------------------------------------------------- 
\section{Umbra}\label{sec:umbra}
%-------------------------------------------------------------------------------- 
Umbra, created in 2020 by Thomas Neumann (creator of HyPer), demonstrates that HyPer's in-memory
database concepts can be applied to on-disk systems \cite{umbra}. This became possible due to
recent improvements in SSDs and buffer management. Umbra integrates concepts from LeanStore for
buffer management and latching, combined with multi-version concurrency, compilation, and execution
strategies from HyPer. This hybrid approach produced an on-disk database that is scalable, flexible,
and faster than HyPer itself.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{figures/umbra-benchmark.png}
	\caption{Umbra benchmarks.}
	\label{fig:umbra-benchmark}
	\cite{umbra}
\end{figure}

% I skipped a paragraph about multi-level debugger here

A novel optimisation they introduced later was enabling the compiler to
change the query plan \cite{umbra-effec-compil}. That is, they can use the metrics collected during
execution to swap the order of joins, or the type of join being used.
This improved the runtime of the data-centric queries by a factor of two. Some other
databases introduce this concept by invoking the query optimiser multiple
times, but since their compiler is invoked multiple times during execution this
adds additional benefit.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{figures/umbra-effec-queries.png}
	\caption{Umbra benchmarks after adaptive query processing (AQP).}
	\label{fig:umbra-effec-queries}
	\cite{umbra-effec-compil}
\end{figure}

Umbra is currently ranked as the most effective database on ClickHouse's
benchmarking \cite{clickbench}. The main complaint of the compiler being too heavy is still there,
but it shows the advantage of having direct access to the JIT compiler with
its adaptive compilation to change optimisation choices. Additionally, Umbra supports
user-defined operators, enabling efficient integration of custom algorithms into the database \cite{umbra-user-operators}.

%-------------------------------------------------------------------------------- 
\section{Mutable}\label{sec:mutable}
%-------------------------------------------------------------------------------- 
In 2023, Mutable presented the concept of using a low-latency JIT
compiler (WebAssembly) rather than a heavy one in their initial paper \cite{mutable-1}.
Its primary purpose, however, is to serve as a framework for implementing other
concepts in database research so that they do not need to rewrite the framework
later \cite{mutable-2}. However, using WebAssembly meant they can omit most of the optimisations
that HyPer did while maintaining higher performance. Furthermore, they have
a minimal query optimiser and instead rely on the V8 engine.

The V8 engine contains a "Liftoff" component that adds an early-stage execution
step to lower the initial overhead of running the query \cite{wasm-liftoff}. The LiftOff component
produces machine code as fast as possible while skipping optimisations, then
"turbofan" is a second-stage compiler that runs in the background
while execution is running. However, HyPer has a direct bytecode interpreter
which can result in a lower time to execution.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/mutable-arch.png}
	\caption{Comparison of mutable to HyPer and Umbra.}
	\label{fig:mutable-arch}
	\cite{mutable-2}
\end{figure}

Mutable's benchmarks show they achieve similar compile and execution times to
HyPer, and outperform them in many cases \cite{mutable-2}. While pushing Mutable to the
same performance as HyPer or Umbra would require re-architecting, achieving this
performance within the implementation effort is a significant outcome.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{figures/mutable-benchmarks.png}
	\caption{Benchmarks produced by Mutable.}
	\label{fig:mutable-bench}
	\cite{mutable-2}
\end{figure}

%-------------------------------------------------------------------------------- 
\section{LingoDB}\label{sec:lingodb}
%-------------------------------------------------------------------------------- 
% I skipped the overview,
LingoDB, introduced in 2022, proposed using the MLIR framework for optimisation
layers \cite{lingo-db-1}. Traditional databases follow a standard pipeline: parsing SQL into a query
tree, converting to relational algebra, optimizing using manual implementations, creating a plan
tree, and either executing or compiling to a binary. MLIR streamlines this by parsing directly
to a high-level MLIR dialect, applying optimisation passes to the plan, and using LLVM
compilation directly without intermediate conversions.

The LingoDB architecture can be seen in figure~\ref{fig:lingo-db-arch}, which begins
by parsing SQL into a relational algebra dialect. These dialects are defined using
MLIR's dialect system and code generation. The compiler consists of multiple dialect layers:
a relational algebra dialect for high-level queries, a database dialect for database-specific
types and operations, a DSA dialect for data structures and algorithms, and a utility
dialect for support functions. This multi-stage design splits the intermediate representation
into three levels: relational algebra, a mixed dialect layer, and finally LLVM code
\cite{lingo-db-1}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/lingo-db-arch.png}
	\caption{LingoDB architecture~\cite{lingo-db-1}}
	\label{fig:lingo-db-arch}
	\cite{lingo-db-1}
\end{figure}

Their result is that they are less performant than HyPer, but do better than
DuckDB \cite{lingo-db-1}. This performance is not their key output, rather, it is that they can
implement the standard optimisation patterns within the compiler. Another feat
is that they are approximately 10,000 lines of code in the query execution model,
and Mutable is at 22,944 lines for their code despite skipping query
optimisation. Within LingoDB's paper they also compare this to being three times
less than DuckDB and five times less than NoisePage.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{figures/lingo-db-bench.png}
	\caption{LingoDB benchmarking.}
	\label{fig:lingo-db-bench}
	\cite{lingo-db-1}
\end{figure}

In later research, LingoDB also explores obscure operations such as GPU
acceleration, using the Torch-MLIR project's dialect, representing queries
as sub-operators for acceleration, non-relational systems, and more \cite{lingo-db-2}.
For our purposes, the appealing part of their architecture is that they use
\texttt{pg\_query} to parse the incoming SQL, which means their parser is the closest
to PostgreSQL's. This will be explored in the design in Section~\ref{sec:design}.
% idk why \texttt doesn't work here.

%-------------------------------------------------------------------------------- 
\section{Benchmarking}\label{sec:benchmarking}
%-------------------------------------------------------------------------------- 
% Maybe this section should change places? Meh.
These systems produced their own benchmarks and could selectively pick which
systems to involve, so a recreation of the benchmarks was done. DuckDB, HyPer,
Mutable, LingoDB and PostgreSQL were all compared to one another, and is
visible in Figure~\ref{fig:created-benchmark-results}. TPC-H was used as most
of the involved pieces used it themselves \cite{tpch_analyzed}, and docker containers were
chosen to make deploying it easier. These benchmarks were
created by relying on the Mutable codebase as they had significant infrastructure
to support this, and is visible at
\url{https://github.com/zyros-dev/benchmarking-dockers}.

The benchmarks show that PostgreSQL is significantly slower than the rest, likely
because it is an on-disk database and most of the others are in-memory. With
PostgreSQL removed from the graph, HyPer and DuckDB are the fastest,
and with a single core DuckDB is the slowest.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.3]{figures/benchmarks.png}
	\caption{Benchmarking results.}
	\label{fig:created-benchmark-results}
\end{figure}

To identify how much potential gain there is in a major on-disk database,
\texttt{perf} was used on PostgreSQL during TPC-H queries 1, 3, 6, 12 and 14 in
figure~\ref{fig:pprof-psql} \cite{perf_man_page}. These queries were chosen because the Mutable
code infrastructure directly supported them. This shows that the CPU time
varied from between 34.87\% and 76.56\%, with an average of 49.32\%. These
metrics were identified
using the \texttt{prof} graph. With this much time in the CPU, it is
clear that the queries can become several times faster if optimised.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{figures/pperf_psql.png}
	\caption{PostgreSQL's time spent in the CPU, measured with prof.}
	\label{fig:pprof-psql}
\end{figure}

%-------------------------------------------------------------------------------- 
\section{Gaps in Literature}\label{sec:gaps}
%-------------------------------------------------------------------------------- 

A core gap is the extension system within existing database. HyPer and Umbra
managed to commercialise their systems, but the other databases are strictly
research systems and some do not support ACID, multithreading, or other core
requirements such as index scans. Michael Stonebraker, a Turing Award recipient and the founder of
PostgreSQL, writes that a fundamental issue in research is that they have
forgotten the use case of the systems and target the $0.01\%$ of users \cite{top-ten-fears}.
These commercial databases reaching high performance is a symptom of this.
Testing the wide variety of ACID requirements is a significant undertaking.

The other issue is writing these compiled query engines is a large undertaking,
and the core reason why vectorised execution has gained more popularity in
production systems. Debugging a compiled program within a database is challenging,
and while solutions have been offered, such as Umbra's debugger \cite{umbra-debugger}, it is still
challenging and questionable how transferable those tools are.

Relying on an extension system such that it's an optional feature means users
can install the optimisations, and tests can be done with production systems
without requesting pull requests into the system itself. Since these are large
source code changes, it adds political complexity to have the solution
added to the official system without production proof of it being used.
The result of this would be an useable compiler accelerator, that can easily
be installed into existing systems, and once used in many scenarios is
easier to add to the official system.

%================================================================================ 
\section{Aims}\label{sec:lit-rev}
%================================================================================ 
Tying this together, this piece aims to integrate a research compiler into a
battle-tested system by using an extension system. This addresses the gap
of these systems being difficult to use widely, and potential to integrate it
into the original system once stronger correctness and speed optimisations have
been shown. Accomplishing this shows there is a way to rely on previous
ACID-compliance and supporting code infrastructure. Users can install the
extension, have faster queries with rollbacks, and the implementation effort
is lowered since core systems and algorithms can be skipped.

A key output is showing that the system can operate within
the same order of magnitude as the target system. The purpose of this is to
ensure other optimisations can be applied to fit the surrounding database later,
but the expectation is not to be faster than it.

One concern is these databases are large systems while the research systems
are smaller. This increases the testing difficulty because a complete system
has more variables, such as genetic algorithms in the query optimiser that makes
performance non-deterministic. To counter this, benchmarks can
be executed multiple times, and a standard deviation can be calculated.
