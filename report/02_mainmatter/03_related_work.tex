\chapter{Related Work}\label{ch:related-work}

This chapter summarises relevant works in the compiled queries space and their
architectures. Originally, the industry began with compiled query engines \cite{system-r}, but
this was overtaken by volcano models as they simplified the implementation
details with little cost at the time. However, now analytical engines are
examining compilers again \cite{everything-vector}.

This begins with PostgreSQL and their extension system in
section~\ref{sec:postgresql-related}, in section~\ref{sec:system-r} system R
will be explored as the classical
example, followed by HyPer and Umbra in section~\ref{sec:hyper} and
section~\ref{sec:umbra} which re-introduced the concept. Mutable in
section~\ref{sec:mutable} and LingoDB in section~\ref{sec:lingodb} are research
databases. Lastly, PostgreSQL will be examined in section~\ref{sec:postgresql-related}
as it uses expression-based compilation and there has been an attempt to create
a compiled engine before.

%-------------------------------------------------------------------------------- 
\section{OLAP Systems}\label{sec:olap-systems}
%-------------------------------------------------------------------------------- 

Most of the benefit of a compiled query engine is in OLAP contexts since the
OLTP contexts are usually retrieval queries and not too complicated \cite{ddia}.
In large-scale contexts Apache HIVE is common, but this is a data warehouse
system, not a database, that contains data within Hadoop's distributed file store,
which is closer to a flat-storage~\cite{apache_hive}. The common OLAP
databases are MonetDB, SnowFlake, ClickHouse, RedShift and Vectorwise
\cite{htap_survey}. For our context, understanding ClickHouse, NoisePage,
DuckDB and extensions that turn PostgreSQL into an OLAP database are important.

ClickHouse and NoisePage are standalone systems, while DuckDB is embedded
and in-process, similar to SQLite. ClickHouse is a columnar, disk-oriented
database with a vectorised execution engine with optional LLVM compilation
for expressions (EXP) \cite{clickhouse}. NoisePage was created by the Carnegie
Mellon Database group, and is columnar and in-memory
with full query expression compilation (QEP), with a single node. They targeted
ML-driven self optimisation in their research, but they were archived in February 21, 2023 \cite{noisepage}. DuckDB is marketed
as the SQLite for analytical loads with in-memory disk spillage, or like a
more sophisticated Pandas DataFrame \cite{duckdb}. Their engine supports
vectorised execution because JIT would add too much overhead to their
lightweight philosophy.

%-------------------------------------------------------------------------------- 
\section{PostgreSQL and Extension Systems}\label{sec:postgresql-related}
%-------------------------------------------------------------------------------- 
PostgreSQL is a battle-tested system and is currently the most
popular database in the world with $51.9$ of developers in a Stack Overflow survey
saying they use it extensively in 2024 \cite{stackoverflow2024}. Within the context of compiled queries this means
the database itself cannot be treated as a research system. Changes directly to
it requires heavy-testing, but also, these changes will not be peer-reviewed
research. Instead, it is pull-requests online with more casual interaction.

Building extensions for PostgreSQL and making entire companies around these
extensions is not a new endeavour. Three such examples are Citus~\cite{citus},
TimescaleDB~\cite{timescaledb}, and Apache AGE~\cite{apache_age}. Citus
aims to add more horizontal scaling through sharding, TimescaleDB, now
rebranded as TigerData, transforms the engine into a time series database,
and Apache AGE turns it into a graph database. These all have thousands
of GitHub stars, but TimescaleDB especially had over 500 paying customers in
2022, and claimed their revenue climbed 20x by 2024 \cite{timescale_2024_update}
\cite{timescaledb_series_c}. This shows that the model of relying on the
extension system is a robust, travelled road.

There have also been several extensions that attempt to make PostgreSQL more
suited to OLAP workloads, with the most relevant one being pg\_duckdb
\cite{pg_duckdb}. This system replaces PostgreSQL's engine with DuckDB which
allows it to take advantage of the vectorised engine, and it is reasonably
popular with roughly 2700 stars on GitHub. Hydra is also worth mentioning,
but this is closer to TimescaleDB \cite{hydra_columnar} and makes the system
columnar with compression support. ParadeDB's pg\_analytics initially started
in the same way as pg\_duckdb, but pivoted into supporting search functionality
instead, similar to Elasticsearch.

There has been significant discussion about HyPer and JIT in regard to
PostgreSQL in 2017 \cite{postgres-pr-1,postgres-pr-2}. The general response expressed doubt that
someone would add support
for full query compilation, noting that rearchitecting such a core
component introduces significant risk.

However, in September 2017 Andres Freund started implementing JIT support for
expressions \cite{psql-jit-release-plan}. The reasoning was that most of the CPU time is in the expression
components, (such as y > 8 in SELECT * from table WHERE x > 8;). Furthermore,
there are significant benefits to tuple deformation as it interacts with the
cache and has poor branch prediction. PostgreSQL's JIT implementation is
documented in the official PostgreSQL documentation \cite{psql-jit-doc}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/pg-min-params.png}
	\caption{Peter Eisentraut asking whether the defaults are too low.}
	\label{fig:pg-params-too-low}
	\cite{postgres-pr-2}
\end{figure}

In the pull request Peter Eisentraut asked whether the default JIT
settings are too low, but in version 11 of PostgreSQL they went ahead with the
release but with the JIT disabled by default. This didn't get much usage,
and they decided that enabling it by default would give it exposure and testing \cite{psql-jit-discussion}.
However, when released, the United Kingdom's critical service for a COVID-19
dashboard automatically updated and spiked to a 70\% failure rate as
some of their queries ran 2,229x slower \cite{psql-jit-failure}. This incident reinforced
the view that JIT features should be disabled by default, and
has led to negative opinions about JIT and compiled queries.

Two cases where QEP query compilation with PostgreSQL was implemented were found.
The first is Vitesse DB, which made a series of public posts about getting
people to assist with testing it. They became generally available in 2015,
but their website is offline now and there is not much mention of them.
The second was at a PgCon presentation and achieved a 5.5x speed-up on TPC-H query
1, and has more documentation \cite{pgcon_jit_2017}. However, they did not publicize their implementation or show that it's easy for people to use.

This implementation of full JIT in PostgreSQL was preceded by
profiling work which showed that different TPC-H benchmarks place pressure on
different nodes, as shown in figure~\ref{fig:pgcon-profiling}. This is important
because they can make informed decisions about where to focus their efforts.
Their core method is generating a function that represents a node in the
plan tree, then inlining the function into the final LLVM IR \cite{pgcon_jit_2017}
in a push-based model. Another
interesting method that is used is pre-compiling the C code into LLVM,
then inlining that into the LLVM IR. This avoids runtime linking back to the C
code \cite{pgcon_jit_2017}, similar to approaches taken by HyPer \cite{hyper-compiler-1}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/pgcon_profiling.png}
	\caption{PGCon Dynamic Compilation of SQL Queries Profiling~\cite{pgcon_jit_2017}.}
	\label{fig:pgcon-profiling}
	\cite{postgres-pr-2}
\end{figure}

Other database systems also support extensions, and there are many systems that
rely on PostgreSQL's extension system. MySQL, ClickHouse, DuckDB, Oracle
Extensible Optimizer all support similar operations. This means more than
only PostgreSQL can be extended in this same manner rather than creating
databases from scratch.

%-------------------------------------------------------------------------------- 
\section{System R}\label{sec:system-r}
%-------------------------------------------------------------------------------- 
System R is a flagship paper in the databasing space that introduced SQL,
compiling engines, and ACID \cite{system-r}. Their vision described ACID requirements, but was
explained as seven dot points as it was not a concept yet. Their goal was to
run at a "level of performance comparable to existing lower-function database
systems." Reviewers commented that the compiler is the most important part of
the design.

Due to the implementation overhead of parsing, validity checking, and access
path selection, a compiler was appealing. These were not supported within the
running transaction by default, and they leveraged pre-compiled fragments of
Cobol for the reused functions to improve their compile times. This was
completely custom-made at the time because there were not many tools to support
writing compilers. System R shows the idea of compiled queries is as old
as databases, and over time the priorities of the systems changed.

%-------------------------------------------------------------------------------- 
\section{HyPer}\label{sec:hyper}
%-------------------------------------------------------------------------------- 
HyPer was a flagship system, and Umbra supersedes it. These are important
systems in the JIT-database space as they developed many of the core features.
Both were made by Thomas Neumann, and a core sign of its viability is that HyPer
was purchased by Tableau in 2016 to be used in production \cite{tableau-hyper}. This shows
that it is possible to use an in-memory JIT database at scale. The project
began in 2010, with their flagship paper releasing in 2011 for the compiler
component \cite{hyper-compiler-1}, and in 2018 they released another flagship paper about
adaptive compilation \cite{hyper-compiler-2}. However, the database being commercialised poses issues
for outside research because the source code is not accessible, but there is a
binary on their website that can be used for benchmarking.

Their 2011 paper on the compiler identifies that translating queries into C or
C++ introduced significant overhead compared to compiling into LLVM. As a result,
they suggested using pre-compiled C++ objects of common functions then
inlining them into the LLVM IR. This LLVM IR is executed by the LLVM's JIT
executor. By utilising LLVM IR, they can take advantage of overflow flags and
strong typing which prevent numerous bugs in their original C++ approach.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/hyper-compile-times.png}
	\caption{HyPer OLAP performance compared to other engines.}
	\label{fig:hyper-compile-times}
	\cite{hyper-compiler-1}
\end{figure}

HyPer shows they reduced their compile time by doing this in
figure~\ref{fig:hyper-compile-times} by many
multiples, and in figure~\ref{fig:hyper-profiler} they show they achieve many times fewer branches,
branch mispredictions, and other measurements compared to their baseline of MonetDB \cite{monetdb_x100}.
The cause of this is HyPer's output had less code in the compiled queries.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/hyper-profiler.png}
	\caption{HyPer branching and cache locality benchmarks.}
	\label{fig:hyper-profiler}
	\cite{hyper-compiler-1}
\end{figure}

Hyper continues on in 2018 where they separated the compiler into multiple rounds.
They introduced an interpreter on the byte code generated from LLVM IR, then they
can run unoptimised machine code, and on the final stage they can run optimised
machine code. Figure~\ref{fig:hyper-adaptive} visualises this with
the compile times of each stage.
However, they had to create the byte code interpreter themselves to enable this.

\begin{figure}[H]
	\centering
	\includegraphics[scale=1]{figures/hyper-adaptive.png}
	\caption{HyPer execution modes and compile times.}
	\label{fig:hyper-adaptive}
	\cite{hyper-compiler-2}
\end{figure}

The 2018 paper also improved their query optimisation by adding a dynamic
measurement for how long live queries are taking. This is because the
optimiser's cost model did not lead to accurate measurements for compilation
timing. Instead, they introduced an execution stage for workers, then
in a "work-stealing" stage they log how long the job took. With a combination
of the measurements and the query plan, they calculate estimates for jobs
and optimal levels to compile them to.

This was benchmarked with TPC-H Query 11 using 4 threads, and they found
the adaptive execution was faster than only using bytecode by $40\%$, unoptimised
compilation by $10\%$ and optimised compilation by $80\%$. The cause of this is that
the compilation stage is single threaded, while with multiple threads they can
compile in the background while execution is running.

Utilising additional stages of the LLVM compiler, improving the cost model, and
supporting multi threading the compilation and execution combined into a viable
JIT compiled-query application. The primary criticism is that they effectively
wrote the JIT compiler from the ground-up, which requires large amounts
of engineering time. Majority of the additions here are not unique to a database's
JIT compiler, and are mostly ways to target the compiler's latency.

%-------------------------------------------------------------------------------- 
\section{Umbra}\label{sec:umbra}
%-------------------------------------------------------------------------------- 
Umbra was created in 2020 by Thomas Neumann, the creator of HyPer, and the
main change is that they show it is possible to use the in-memory database
concepts from HyPer inside an on-disk database \cite{umbra}. The core reason for this is
the recent improvements of SSDs and buffer management advances. They take
concepts from LeanStore for the buffer management and latches, then multi-version
concurrency, compilation, and execution strategies from HyPer. This combination
led to an on-disk database that is scalable, flexible and faster than HyPer.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{figures/umbra-benchmark.png}
	\caption{Umbra benchmarks.}
	\label{fig:umbra-benchmark}
	\cite{umbra}
\end{figure}

% I skipped a paragraph about multi-level debugger here

A novel optimisation they introduced later was enabling the compiler to
change the query plan \cite{umbra-effec-compil}. That is, they can use the metrics collected during
execution to swap the order of joins, or the type of join being used.
This improved the runtime of the data-centric queries by a factor of two. Some other
databases introduce this concept by invoking the query optimiser multiple
times, but since their compiler is invoked multiple times during execution this
adds additional benefit.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{figures/umbra-effec-queries.png}
	\caption{Umbra benchmarks after adaptive query processing (AQP).}
	\label{fig:umbra-effec-queries}
	\cite{umbra-effec-compil}
\end{figure}

Umbra is currently ranked as the most effective database on ClickHouse's
benchmarking \cite{clickbench}. The main complaint of the compiler being too heavy is still there,
but it shows the advantage of having direct access to the JIT compiler with
its adaptive compilation to change optimisation choices. Additionally, Umbra supports
user-defined operators, enabling efficient integration of custom algorithms into the database \cite{umbra-user-operators}.

%-------------------------------------------------------------------------------- 
\section{Mutable}\label{sec:mutable}
%-------------------------------------------------------------------------------- 
In 2023, Mutable presented the concept of using a low-latency JIT
compiler (WebAssembly) rather than a heavy one in their initial paper \cite{mutable-1}.
Its primary purpose, however, is to serve as a framework for implementing other
concepts in database research so that they do not need to rewrite the framework
later \cite{mutable-2}. However, using WebAssembly meant they can omit most of the optimisations
that HyPer did while maintaining higher performance. Furthermore, they have
a minimal query optimiser and instead rely on the V8 engine.

The V8 engine contains a "Liftoff" component that adds an early-stage execution
step to lower the initial overhead of running the query \cite{wasm-liftoff}. The LiftOff component
produces machine code as fast as possible while skipping optimisations, then
"turbofan" is a second-stage compiler that runs in the background
while execution is running. However, HyPer has a direct bytecode interpreter
which can result in a lower time to execution.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/mutable-arch.png}
	\caption{Comparison of mutable to HyPer and Umbra.}
	\label{fig:mutable-arch}
	\cite{mutable-2}
\end{figure}

Mutable's benchmarks show they achieve similar compile and execution times to
HyPer, and outperform them in many cases \cite{mutable-2}. While pushing Mutable to the
same performance as HyPer or Umbra would require re-architecting, achieving this
performance within the implementation effort is a significant outcome.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{figures/mutable-benchmarks.png}
	\caption{Benchmarks produced by Mutable.}
	\label{fig:mutable-bench}
	\cite{mutable-2}
\end{figure}

%-------------------------------------------------------------------------------- 
\section{LingoDB}\label{sec:lingodb}
%-------------------------------------------------------------------------------- 
% I skipped the overview,
LingoDB piloted in 2022 and proposed using the MLIR framework to create the
optimisation layers \cite{lingo-db-1}. In most databases, the system parses the SQL into a
query tree, then relational algebra, then this is optimised using manual
implementations, and parse this into a plan tree for execution, or compile
this into a binary. With MLIR, this pipeline changes into parsing the plan
tree into a high-level dialect in MLIR, then doing optimisation passes on the
plan itself, and the LLVM compiler can be used directly to turn this into
LLVM, streamlining the process.

The LingoDB architecture can be seen in figure~\ref{fig:lingo-db-arch}, which begins
by parsing the
SQL into a relational algebra dialect. These dialects are defined using
MLIR's dialect system, and supported through code generation. Their compiler
is defined by a relational algebra dialect, a database dialect that represents
database-specific types and operations, a DSA dialect that represents data
structures and algorithms, a utility dialect that represents
utilities, and the final LLVM output. This splits the state of the intermediate
representation into three stages: relational algebra, a mixed dialect, and
finally the LLVM \cite{lingo-db-1}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/lingo-db-arch.png}
	\caption{LingoDB architecture~\cite{lingo-db-1}}
	\label{fig:lingo-db-arch}
	\cite{lingo-db-1}
\end{figure}

Their result is that they are less performant than HyPer, but do better than
DuckDB \cite{lingo-db-1}. This performance is not their key output, rather, it is that they can
implement the standard optimisation patterns within the compiler. Another feat
is that they are approximately 10,000 lines of code in the query execution model,
and Mutable is at 22,944 lines for their code despite skipping query
optimisation. Within LingoDB's paper they also compare this to being three times
less than DuckDB and five times less than NoisePage.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{figures/lingo-db-bench.png}
	\caption{LingoDB benchmarking.}
	\label{fig:lingo-db-bench}
	\cite{lingo-db-1}
\end{figure}

In later research, LingoDB also explores obscure operations such as GPU
acceleration, using the Torch-MLIR project's dialect, representing queries
as sub-operators for acceleration, non-relational systems, and more \cite{lingo-db-2}.
For our purposes, the appealing part of their architecture is that they use
\texttt{pg\_query} to parse the incoming SQL, which means their parser is the closest
to PostgreSQL's. This will be explored in the design in Section~\ref{sec:design}.
% idk why \texttt doesn't work here.

%-------------------------------------------------------------------------------- 
\section{Benchmarking}\label{sec:benchmarking}
%-------------------------------------------------------------------------------- 
% Maybe this section should change places? Meh.
These systems produced their own benchmarks and could selectively pick which
systems to involve, so a recreation of the benchmarks was done. DuckDB, HyPer,
Mutable, LingoDB and PostgreSQL were all compared to one another, and is
visible in Figure~\ref{fig:created-benchmark-results}. TPC-H was used as most
of the involved pieces used it themselves \cite{tpch_analyzed}, and docker containers were
chosen to make deploying it easier. These benchmarks were
created by relying on the Mutable codebase as they had significant infrastructure
to support this, and is visible at
\url{https://github.com/zyros-dev/benchmarking-dockers}.

The benchmarks show that PostgreSQL is significantly slower than the rest, likely
because it is an on-disk database and most of the others are in-memory. With
PostgreSQL removed from the graph, HyPer and DuckDB are the fastest,
and with a single core DuckDB is the slowest.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.3]{figures/benchmarks.png}
	\caption{Benchmarking results.}
	\label{fig:created-benchmark-results}
\end{figure}

To identify how much potential gain there is in a major on-disk database,
\texttt{perf} was used on PostgreSQL during TPC-H queries 1, 3, 6, 12 and 14 in
figure~\ref{fig:pprof-psql} \cite{perf_man_page}. These queries were chosen because the Mutable
code infrastructure directly supported them. This shows that the CPU time
varied from between 34.87\% and 76.56\%, with an average of 49.32\%. These
metrics were identified
using the \texttt{prof} graph. With this much time in the CPU, it is
clear that the queries can become several times faster if optimised.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{figures/pperf_psql.png}
	\caption{PostgreSQL's time spent in the CPU, measured with prof.}
	\label{fig:pprof-psql}
\end{figure}

%-------------------------------------------------------------------------------- 
\section{Gaps in Literature}\label{sec:gaps}
%-------------------------------------------------------------------------------- 

A core gap is the extension system within existing database. HyPer and Umbra
managed to commercialise their systems, but the other databases are strictly
research systems and some do not support ACID, multithreading, or other core
requirements such as index scans. Michael Stonebraker, a Turing Award recipient and the founder of
PostgreSQL, writes that a fundamental issue in research is that they have
forgotten the use case of the systems and target the $0.01\%$ of users \cite{top-ten-fears}.
These commercial databases reaching high performance is a symptom of this.
Testing the wide variety of ACID requirements is a significant undertaking.

The other issue is writing these compiled query engines is a large undertaking,
and the core reason why vectorised execution has gained more popularity in
production systems. Debugging a compiled program within a database is challenging,
and while solutions have been offered, such as Umbra's debugger \cite{umbra-debugger}, it is still
challenging and questionable how transferable those tools are.

Relying on an extension system such that it's an optional feature means users
can install the optimisations, and tests can be done with production systems
without requesting pull requests into the system itself. Since these are large
source code changes, it adds political complexity to have the solution
added to the official system without production proof of it being used.
The result of this would be an useable compiler accelerator, that can easily
be installed into existing systems, and once used in many scenarios is
easier to add to the official system.

%================================================================================ 
\section{Aims}\label{sec:lit-rev}
%================================================================================ 
Tying this together, this piece aims to integrate a research compiler into a
battle-tested system by using an extension system. This addresses the gap
of these systems being difficult to use widely, and potential to integrate it
into the original system once stronger correctness and speed optimisations have
been shown. Accomplishing this shows there is a way to rely on previous
ACID-compliance and supporting code infrastructure. Users can install the
extension, have faster queries with rollbacks, and the implementation effort
is lowered since core systems and algorithms can be skipped.

A key output is showing that the system can operate within
the same order of magnitude as the target system. The purpose of this is to
ensure other optimisations can be applied to fit the surrounding database later,
but the expectation is not to be faster than it.

One concern is these databases are large systems while the research systems
are smaller. This increases the testing difficulty because a complete system
has more variables, such as genetic algorithms in the query optimiser that makes
performance non-deterministic. To counter this, benchmarks can
be executed multiple times, and a standard deviation can be calculated.
